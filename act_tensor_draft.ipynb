{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiCoZPp4um7P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# ACT-Tensor: A Complete Implementation of the Tensor Completion Framework for Financial Dataset Imputation\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2508.01861-b31b1b.svg)](https://arxiv.org/abs/2508.01861)\n",
        "[![Conference](https://img.shields.io/badge/Conference-ICAIF%20'25-9cf)](https://icaif.acm.org/2025/)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/act_tensor)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Quantitative%20Finance%20%7C%20ML-00529B)](https://github.com/chirindaopensource/act_tensor)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-CRSP%20%7C%20Compustat-lightgrey)](https://github.com/chirindaopensource/act_tensor)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Tensor%20Decomposition%20%7C%20K--Means-orange)](https://github.com/chirindaopensource/act_tensor)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Asset%20Pricing%20%7C%20Portfolio%20Sorts-red)](https://github.com/chirindaopensource/act_tensor)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![Scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%2302557B.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![TensorLy](https://img.shields.io/badge/TensorLy-4B0082-blueviolet)](http://tensorly.org/stable/index.html)\n",
        "[![Matplotlib](https://img.shields.io/badge/matplotlib-%2311557c.svg?style=flat&logo=matplotlib&logoColor=white)](https://matplotlib.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "---\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/act_tensor`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"ACT-Tensor: Tensor Completion Framework for Financial Dataset Imputation\"** by:\n",
        "\n",
        "*   Junyi Mo\n",
        "*   Jiayu Li\n",
        "*   Duo Zhang\n",
        "*   Elynn Chen\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and characteristic engineering to the core tensor imputation algorithm and the final downstream asset pricing evaluation.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_complete_act_tensor_replication`](#key-callable-run_complete_act_tensor_replication)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"ACT-Tensor: Tensor Completion Framework for Financial Dataset Imputation.\" The core of this repository is the iPython Notebook `act_tensor_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of all analytical tables and figures.\n",
        "\n",
        "The paper introduces a novel tensor-based framework to address the pervasive problem of missing data in multi-dimensional financial panels. This codebase operationalizes the ACT-Tensor framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single YAML file.\n",
        "-   Process raw CRSP/Compustat data, construct a library of 45 firm characteristics with correct look-ahead bias handling, and transform the data into a 3D tensor.\n",
        "-   Execute the core ACT-Tensor algorithm: K-Means clustering by data density, followed by a \"divide and conquer\" CP tensor completion and temporal smoothing.\n",
        "-   Run a full suite of benchmark models, ablation studies, and sensitivity analyses.\n",
        "-   Perform a complete downstream asset pricing evaluation to measure the economic significance of the imputed data.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in multilinear algebra, machine learning, and empirical asset pricing.\n",
        "\n",
        "**1. CP Tensor Decomposition:**\n",
        "The core of the imputation model is the CANDECOMP/PARAFAC (CP) decomposition, which approximates a tensor $\\mathcal{X}$ as a sum of rank-one tensors. For a 3D tensor, the model is:\n",
        "$$\n",
        "\\hat{\\mathcal{X}} = \\sum_{r=1}^{R} \\mathbf{u}_r \\circ \\mathbf{v}_r \\circ \\mathbf{w}_r\n",
        "$$\n",
        "where $\\mathbf{U}$, $\\mathbf{V}$, and $\\mathbf{W}$ are factor matrices. The model is fit by minimizing the reconstruction error on the observed entries $\\Omega$ via Alternating Least Squares (ALS).\n",
        "$$\n",
        "\\min_{\\mathbf{U},\\mathbf{V},\\mathbf{W}} \\| \\mathcal{P}_{\\Omega}(\\mathcal{X}) - \\mathcal{P}_{\\Omega}(\\hat{\\mathcal{X}}) \\|_F^2\n",
        "$$\n",
        "\n",
        "**2. Cluster-Based Completion:**\n",
        "To handle extreme sparsity, firms are first clustered by their data availability patterns using K-Means.\n",
        "-   **Dense Clusters ($\\rho_k \\ge \\tau$):** Imputed directly using the standard CP-ALS algorithm.\n",
        "-   **Sparse Clusters ($\\rho_k < \\tau$):** Imputed by first forming an *augmented* tensor that combines the sparse firms with all dense firms, allowing the sparse firms to \"borrow\" statistical strength.\n",
        "\n",
        "**3. Partial Tucker Decomposition (HOSVD):**\n",
        "For the downstream asset pricing test, a large tensor of portfolio returns $\\mathcal{R}$ is created. To extract a parsimonious set of factors, a partial Tucker decomposition is used, which is solved via the Higher-Order Singular Value Decomposition (HOSVD). This finds a low-dimensional core tensor $\\mathcal{F}$ and loading matrices that best represent the original data.\n",
        "$$\n",
        "\\mathcal{R} \\approx \\mathcal{F} \\times_{1} U \\times_{2} V \\times_{3} W\n",
        "$$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`act_tensor_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into over 30 distinct, modular tasks, each with its own orchestrator function, ensuring clarity and testability.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file, allowing for easy customization and replication.\n",
        "-   **High-Fidelity Financial Data Processing:** Includes professional-grade logic for handling CRSP/Compustat data, including look-ahead bias prevention, delisting return incorporation, and outlier cleansing.\n",
        "-   **Robust Experimental Design:** Programmatically generates the three distinct missingness scenarios (MAR, Block, Logit) with disjoint test sets for rigorous model evaluation.\n",
        "-   **From-Scratch Algorithm Implementation:** Includes a complete, from-scratch, regularized ALS solver for masked CP tensor decomposition with robust SVD-based initialization.\n",
        "-   **Comprehensive Evaluation Suite:** Implements not only the main ACT-Tensor model but also all specified baselines, ablation studies, and sensitivity tests, and evaluates them on both statistical and economic metrics.\n",
        "-   **Automated Reporting:** Concludes by automatically generating all publication-ready tables (including styled LaTeX with color-coding) and figures from the paper.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Preparation (Tasks 1-6):** Ingests and validates the `config.yaml` and raw data, defines the study universe, constructs 45 characteristics with correct reporting lags, normalizes them, and forms the 3D tensor $\\mathcal{X}$.\n",
        "2.  **Experimental Design (Tasks 7-10):** Generates the three evaluation masks (MAR, Block, Logit) and creates the corresponding training tensors.\n",
        "3.  **ACT-Tensor Imputation (Tasks 11-18):** Executes the full ACT-Tensor algorithm: K-Means clustering, density partitioning, dense and sparse cluster completion via CP-ALS, global assembly, and temporal smoothing (CMA, EMA, or KF).\n",
        "4.  **Evaluation & Analysis (Tasks 19-27):** Evaluates the imputation accuracy of ACT-Tensor, all baselines, and all ablation models. Runs the regularization stability test.\n",
        "5.  **Asset Pricing Pipeline (Tasks 28-34):** Uses the imputed data to construct double-sorted portfolios, extracts latent factors via HOSVD, selects predictive factors via forward stepwise regression, and computes all final asset pricing metrics (alphas, IC, Sharpe Ratio).\n",
        "6.  **Master Orchestration (Tasks 35-37):** Provides top-level functions to run the entire experimental suite and automatically generate all final reports.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `act_tensor_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_complete_act_tensor_replication`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_complete_act_tensor_replication`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, from data validation to the final report generation.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scikit-learn`, `scipy`, `tensorly`, `matplotlib`, `seaborn`, `pyyaml`, `pyarrow`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/act_tensor.git\n",
        "    cd act_tensor\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scikit-learn scipy tensorly matplotlib seaborn pyyaml pyarrow\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a `pandas.DataFrame` with a specific, comprehensive schema containing merged CRSP and Compustat data. The exact schema is validated by the `validate_and_enforce_schema` function in the notebook. All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `act_tensor_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `run_complete_act_tensor_replication` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # Define the paths to the necessary input files.\n",
        "    # The user must provide their own raw data file in the specified format.\n",
        "    # A synthetic data generator is included in the notebook for demonstration.\n",
        "    RAW_DATA_FILE = \"data/raw/crsp_compustat_merged.parquet\"\n",
        "    CONFIG_FILE = \"config/act_tensor_config.yaml\"\n",
        "    \n",
        "    # Define the top-level directory for all outputs.\n",
        "    RESULTS_DIRECTORY = \"replication_output\"\n",
        "\n",
        "    # Execute the entire replication study.\n",
        "    run_complete_act_tensor_replication(\n",
        "        data_path=RAW_DATA_FILE,\n",
        "        config_path=CONFIG_FILE,\n",
        "        base_output_dir=RESULTS_DIRECTORY\n",
        "    )\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline creates a `base_output_dir` with a highly structured set of outputs. For each experimental run, a unique timestamped subdirectory is created, containing:\n",
        "-   A detailed `pipeline_log.log` file.\n",
        "-   A comprehensive `run_summary.json` with all computed metrics.\n",
        "-   Subdirectories for all intermediate artifacts (e.g., `X_train.npz`, `cluster_assignments.csv`, `ap_ACT-Tensor_CMA/`).\n",
        "\n",
        "At the top level, two final directories are created:\n",
        "-   `final_tables/`: Contains all aggregated results in CSV and styled LaTeX formats.\n",
        "-   `final_figures/`: Contains all generated plots in PDF format.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "act_tensor/\n",
        "│\n",
        "├── act_tensor_draft.ipynb    # Main implementation notebook with all 38 tasks\n",
        "├── config.yaml               # Master configuration file\n",
        "├── requirements.txt          # Python package dependencies\n",
        "├── data/\n",
        "│   └── raw/\n",
        "│       └── crsp_compustat_merged.parquet (User-provided)\n",
        "│\n",
        "├── replication_output/       # Example output directory\n",
        "│   ├── final_tables/\n",
        "│   ├── final_figures/\n",
        "│   └── MAR_CMA_20251026_103000/\n",
        "│       ├── run_summary.json\n",
        "│       └── ...\n",
        "│\n",
        "├── LICENSE                   # MIT Project License File\n",
        "└── README.md                 # Project 'README.md' file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all study parameters, including date ranges, model hyperparameters (ranks, clusters), smoother settings, and asset pricing specifications, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Decompositions:** Integrating other tensor decompositions like the Tucker model or Tensor Train (TT) into the imputation framework.\n",
        "-   **GPU Acceleration:** Adapting the core numerical algorithms (ALS, SVD) to run on GPUs using libraries like CuPy or PyTorch for significant speedups.\n",
        "-   **Advanced Clustering:** Exploring more sophisticated clustering methods beyond K-Means, such as hierarchical clustering or density-based methods (DBSCAN).\n",
        "-   **Dynamic Factor Models:** Extending the downstream asset pricing evaluation to use dynamic factor models that can capture time-varying factor loadings.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@inproceedings{mo2025act,\n",
        "  author    = {Mo, Junyi and Li, Jiayu and Zhang, Duo and Chen, Elynn},\n",
        "  title     = {{ACT-Tensor: Tensor Completion Framework for Financial Dataset Imputation}},\n",
        "  booktitle = {Proceedings of the 6th ACM International Conference on AI in Finance},\n",
        "  series    = {ICAIF '25},\n",
        "  year      = {2025},\n",
        "  publisher = {ACM},\n",
        "  note      = {arXiv:2508.01861}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Professional-Grade Implementation of the ACT-Tensor Framework.\n",
        "GitHub repository: https://github.com/chirindaopensource/act_tensor\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Junyi Mo, Jiayu Li, Duo Zhang, and Elynn Chen** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, Scikit-learn, SciPy, TensorLy, Matplotlib, and Jupyter**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `act_tensor_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "WziVTtyz0SJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*ACT-Tensor: Tensor Completion Framework for Financial Dataset Imputation*\"\n",
        "\n",
        "Authors: Junyi Mo, Jiayu Li, Duo Zhang, Elynn Chen\n",
        "\n",
        "E-Journal Submission Date: 3 August 2025 (V1), last revised 8 October 2025 (this version, V3)\n",
        "\n",
        "Conference Affiliation: The 6th ACM International Conference on AI in Finance (ICAIF 2025)\n",
        "\n",
        "Link: https://arxiv.org/abs/2508.01861\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Missing data in financial panels presents a critical obstacle, undermining asset-pricing models and reducing the effectiveness of investment strategies. Such panels are often inherently multi-dimensional, spanning firms, time, and financial variables, which adds complexity to the imputation task. Conventional imputation methods often fail by flattening the data's multidimensional structure, struggling with heterogeneous missingness patterns, or overfitting in the face of extreme data sparsity. To address these limitations, we introduce an Adaptive, Cluster-based Temporal smoothing tensor completion framework (ACT-Tensor) tailored for severely and heterogeneously missing multi-dimensional financial data panels. ACT-Tensor incorporates two key innovations: a cluster-based completion module that captures cross-sectional heterogeneity by learning group-specific latent structures; and a temporal smoothing module that proactively removes short-lived noise while preserving slow-moving fundamental trends. Extensive experiments show that ACT-Tensor consistently outperforms state-of-the-art benchmarks in terms of imputation accuracy across a range of missing data regimes, including extreme sparsity scenarios. To assess its practical financial utility, we evaluate the imputed data with an asset-pricing pipeline tailored for tensor-structured financial data. Results show that ACT-Tensor not only reduces pricing errors but also significantly improves risk-adjusted returns of the constructed portfolio. These findings confirm that our method delivers highly accurate and informative imputations, offering substantial value for financial decision-making."
      ],
      "metadata": {
        "id": "dd4YnBWHu92c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Summary: ACT-Tensor Framework\n",
        "\n",
        "The work provides a sophisticated, domain-aware solution to the problem of imputing missing values in financial panel data. The authors correctly identify that this data is not a simple table but a three-dimensional tensor: **Firms × Characteristics × Time**. Their core contribution is a framework that respects this structure to produce imputations that are not only statistically accurate but, more importantly, financially meaningful.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Problem Formulation and Critique of Existing Methods**\n",
        "\n",
        "The paper begins by framing the central challenge. Financial datasets, like CRSP/Compustat, are notoriously incomplete. This missingness is not random; it is systematic, disproportionately affecting smaller, younger, or distressed firms. This is a classic selection bias problem.\n",
        "\n",
        "The authors correctly dismiss naive solutions:\n",
        "*   **Discarding data:** Leads to a biased sample, focusing only on the most stable, well-documented firms.\n",
        "*   **Simple fills (e.g., cross-sectional median):** This is a blunt instrument that destroys valuable idiosyncratic information and temporal dynamics.\n",
        "*   **Matrix-based methods (e.g., PCA, matrix factorization):** These methods are a step up but require \"flattening\" the 3D tensor into a 2D matrix. This act is destructive, as it either discards the time dimension or treats it as just another feature, ignoring crucial temporal dependencies like autocorrelation and trends.\n",
        "*   **Standard Tensor Completion:** While structurally superior, these methods often assume a single, global low-rank structure for all firms. This is a strong and likely incorrect assumption, as a tech startup and a utility company do not share the same latent drivers. Furthermore, they become unstable under the extreme sparsity seen in financial data.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Proposed Solution - The ACT-Tensor Architecture**\n",
        "\n",
        "The authors' proposed solution, ACT-Tensor, is a multi-stage framework designed to overcome these limitations. The name itself is an acronym for its key innovations: **Adaptive, Cluster-based, Temporal smoothing**.\n",
        "\n",
        "The process can be understood as two primary modules:\n",
        "\n",
        "**Module A: Cluster-based Completion (The \"Divide and Conquer\" Strategy)**\n",
        "\n",
        "This is the most clever part of the framework. Instead of imposing one model on all firms, they partition the firms based on data quality.\n",
        "\n",
        "1.  **Clustering:** Firms are grouped using K-means clustering. The feature for clustering is not a financial characteristic but the *pattern of missing data* itself—specifically, the observed-entry rate for each firm's time-characteristic matrix.\n",
        "2.  **Categorization:** Each cluster is labeled as either \"dense\" (high data availability) or \"sparse\" (low data availability) based on a predefined threshold (τ = 40%).\n",
        "3.  **Differential Imputation:**\n",
        "    *   **For Dense Clusters:** A standard CANDECOMP/PARAFAC (CP) tensor decomposition is applied directly to the sub-tensor for each dense cluster. With sufficient data, this is a stable procedure.\n",
        "    *   **For Sparse Clusters:** This is where the innovation lies. To avoid overfitting on the sparse data, the model \"borrows statistical strength.\" For each sparse cluster, it creates a temporary, aggregated tensor containing the firms from that sparse cluster *plus all firms from all dense clusters*. It then performs the CP completion on this larger, more stable tensor. Finally, it slices out the imputed data corresponding only to the original sparse firms.\n",
        "\n",
        "**Module B: Temporal Smoothing (The \"Noise Reduction\" Filter)**\n",
        "\n",
        "After the tensor is fully imputed, the authors recognize that the raw imputed values can still contain high-frequency noise. Since most firm characteristics are fundamentals that evolve slowly, they apply a temporal smoothing filter to each firm's characteristic time series. They test three methods:\n",
        "*   Centered Moving Average (CMA)\n",
        "*   Exponential Moving Average (EMA)\n",
        "*   Kalman Filter (KF)\n",
        "\n",
        "The results show that the simple, symmetric **Centered Moving Average (CMA)** performs best, effectively filtering out noise while preserving underlying trends.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Dual Evaluation Methodology - Statistical and Financial**\n",
        "\n",
        "This is a mark of excellent, practical research. The authors are not content with merely showing a lower Root Mean Squared Error (RMSE). They test whether their statistically superior imputations translate into tangible financial utility.\n",
        "\n",
        "**1. Statistical Accuracy Evaluation:**\n",
        "*   They artificially mask known data points under three realistic scenarios: Missing-at-Random (MAR), Block Missingness (simulating a firm's data being unavailable for a full year), and Logistic Missingness (a more complex, firm-specific pattern).\n",
        "*   They measure performance using standard metrics: RMSE, MAE, and an imputation R-squared (R²_imp).\n",
        "\n",
        "**2. Financial Utility Evaluation (The \"Downstream Task\"):**\n",
        "This is the crucial out-of-sample test. The imputed data is fed into a state-of-the-art asset pricing pipeline:\n",
        "1.  **Portfolio Construction:** The imputed characteristics are used to double-sort firms into portfolios (e.g., by size and value), creating a tensor of portfolio returns.\n",
        "2.  **Factor Extraction:** A Tucker decomposition is used on this return tensor to extract a parsimonious set of latent risk factors that drive returns.\n",
        "3.  **Prediction and Strategy Formulation:** These factors are used to predict future returns. A \"Top-minus-Bottom\" (T-B) long-short portfolio is constructed based on these predictions.\n",
        "4.  **Evaluation Metrics:** The quality is assessed using metrics central to quantitative finance:\n",
        "    *   **Pricing Error (RMSEα):** How well do the factors explain returns? Lower is better.\n",
        "    *   **Information Coefficient (IC):** The correlation between predicted and realized returns. Higher is better.\n",
        "    *   **Sharpe Ratio:** The risk-adjusted return of the T-B strategy. This is the ultimate measure of profitability.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Empirical Results and Key Findings**\n",
        "\n",
        "The results are compelling and validate the authors' design choices.\n",
        "\n",
        "*   **Superior Imputation Accuracy:** ACT-Tensor consistently outperforms all benchmarks (Cross-Sectional Median, Global and Local matrix factorization models) across all missingness regimes, particularly the more structured and realistic Block and Logit scenarios (Table 2).\n",
        "*   **Robustness to Sparsity:** The framework's advantage is most pronounced when stress-tested on the sparsest clusters of firms, demonstrating the success of the \"borrowing strength\" concept.\n",
        "*   **Synergy of Modules:** The ablation study confirms that both the clustering and temporal smoothing modules add value, and their combination is synergistic.\n",
        "*   **Direct Financial Value:** The imputed data from ACT-Tensor leads to asset pricing models with significantly lower pricing errors and, critically, generates trading strategies with **more than double the Information Coefficient and a Sharpe Ratio roughly twice as high** as those from benchmark methods (Table 3). This is a powerful demonstration of turning a data science improvement into alpha.\n",
        "*   **Inherent Regularization:** The model is shown to be stable without needing an explicit L2 penalty, simplifying its application.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Conclusion and My Final Take**\n",
        "\n",
        "The ACT-Tensor framework is a significant contribution. It provides a robust, principled, and practical solution to a fundamental data problem in quantitative finance.\n",
        "\n",
        "The key takeaway is that **structure and domain knowledge matter**. By preserving the natural tensor structure of the data, accommodating firm heterogeneity through clustering, and enforcing temporal smoothness consistent with financial theory, the authors have built a tool that doesn't just fill in numbers—it recovers meaningful economic signals. The rigorous dual-evaluation framework, connecting statistical accuracy to portfolio profitability, sets a high standard for future research in this area. This is a fine example of how sophisticated machine learning can be thoughtfully applied to solve real-world financial problems."
      ],
      "metadata": {
        "id": "enllqTxrxaAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "EeJGeGdffZvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  ACT-Tensor: A Complete Implementation of the Tensor Completion Framework\n",
        "#  for Financial Dataset Imputation\n",
        "#\n",
        "#  This module provides a complete, production-grade, and high-fidelity Python\n",
        "#  implementation of the \"Adaptive, Cluster-based Temporal smoothing\" (ACT-Tensor)\n",
        "#  framework, as presented in the paper by Mo, Li, Zhang, and Chen (2025). It\n",
        "#  delivers a robust, end-to-end system for transforming sparse, multi-dimensional\n",
        "#  financial data panels into dense, coherent, and machine-readable \"digital twins\"\n",
        "#  of the firm-characteristic universe.\n",
        "#\n",
        "#  The pipeline is designed to address the critical challenges of missing data in\n",
        "#  quantitative finance, enabling more accurate asset pricing models, robust risk\n",
        "#  management, and superior data-driven investment decision-making.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Data Preprocessing: Rigorous validation, universe definition, and look-ahead\n",
        "#    bias-free construction of a 45-characteristic library.\n",
        "#  • Tensor Representation: Transformation of the panel data into a 3D tensor\n",
        "#    (Time x Firm x Characteristic) with cross-sectional rank normalization.\n",
        "#  • Cluster-based Completion: A \"divide and conquer\" strategy that partitions\n",
        "#    firms by data density (via K-Means) and applies tailored CP tensor completion\n",
        "#    (unpenalized for dense clusters, augmented for sparse clusters) using a\n",
        "#    from-scratch, regularized Alternating Least Squares (ALS) solver.\n",
        "#  • Temporal Smoothing: A post-processing module to filter noise from imputed\n",
        "#    series using Centered Moving Average (CMA), Exponential Moving Average (EMA),\n",
        "#    or a Kalman Filter/Smoother with MLE-based hyperparameter tuning.\n",
        "#  • Downstream Economic Evaluation: A full-scale asset pricing pipeline that uses\n",
        "#    the imputed data to construct double-sorted portfolios, extract latent factors\n",
        "#    via Higher-Order SVD (HOSVD), and build predictive factor models to assess\n",
        "#    economic significance (e.g., Sharpe Ratios, Information Coefficient).\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Modular, Functional Design: The entire pipeline is composed of discrete,\n",
        "#    reusable, and rigorously tested functions, managed by high-level orchestrators.\n",
        "#  • Reproducibility Engine: A complete experimental suite manager that handles\n",
        "#    all three missingness regimes (MAR, Block, Logit), all model variations,\n",
        "#    and all ablation/sensitivity tests in a fully automated and resumable fashion.\n",
        "#  • Professional-Grade Code: Features detailed typehints, comprehensive docstrings,\n",
        "#    line-by-line comments, robust error handling, and adherence to PEP-8 standards.\n",
        "#  • Performance: Leverages vectorized NumPy, SciPy, and Pandas operations, along\n",
        "#    with the TensorLy library for optimized multilinear algebra.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Mo, J., Li, J., Zhang, D., & Chen, E. (2025). ACT-Tensor: Tensor Completion\n",
        "#  Framework for Financial Dataset Imputation. In 6th ACM International Conference\n",
        "#  on AI in Finance (ICAIF '25).\n",
        "#  https://arxiv.org/abs/2508.01861\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# ==============================================================================\n",
        "# Consolidated Imports for the ACT-Tensor Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# Standard Library Imports\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Third-Party Scientific Computing Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorly as tl\n",
        "from pandas.io.formats.style import Styler\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "from scipy.signal import lfilter, lfiltic\n",
        "from scipy.stats import rankdata\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import Ridge\n",
        "from tensorly.tenalg import multi_mode_dot\n",
        "\n",
        "# Third-Party Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure a basic logger for this module\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n"
      ],
      "metadata": {
        "id": "4Ja5TjlzfedR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "cfV9YysyfhRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Explication of the Key ACT-Tensor Pipeline Callables**\n",
        "\n",
        "#### **Task 1: `validate_config`**\n",
        "\n",
        "*   **Inputs:** A Python dictionary (`act_tensor_config`) containing all study parameters.\n",
        "*   **Process:** This function acts as a rigorous gatekeeper. It systematically validates the input dictionary's structure, types, and value ranges against a predefined schema derived from the paper's methodology. It checks for the presence of all required keys, verifies date consistency, validates numerical hyperparameters (e.g., ensuring `density_threshold_tau` is in $(0, 1]$), and confirms choices for categorical parameters.\n",
        "*   **Outputs:** The original, validated configuration dictionary. As a side effect, it sets global random seeds for reproducibility and saves a timestamped JSON snapshot of the configuration for an audit trail.\n",
        "*   **Role in Pipeline:** This callable implements the foundational step of **parameter validation and experimental setup**. It ensures that the entire pipeline operates on a consistent, correct, and explicitly defined set of parameters, preventing a vast class of common errors before any computation begins. It does not directly implement an equation from the paper but is a non-negotiable prerequisite for a reproducible scientific workflow.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 2: `validate_and_enforce_schema`**\n",
        "\n",
        "*   **Inputs:** A raw `pandas.DataFrame` containing the merged CRSP/Compustat data.\n",
        "*   **Process:** The function transforms the raw, untyped input `DataFrame` into a structured, schema-compliant panel. It verifies the presence of all required columns, converts the `date` column to a `DatetimeIndex`, and rigorously enforces the correct data type for every other column (e.g., `int64`, `float64`, `datetime64[ns]`) using `errors='coerce'` to handle parsing failures gracefully. It concludes with a series of domain-specific sanity checks on core financial fields.\n",
        "*   **Outputs:** A schema-compliant `pandas.DataFrame` with a `DatetimeIndex`. As a side effect, it saves a data quality report and checkpoints the validated `DataFrame` to a Parquet file.\n",
        "*   **Role in Pipeline:** This callable performs the critical step of **data validation and sanitization**. It ensures that the raw data conforms to the expected structure and types before any financial logic is applied, preventing downstream errors due to incorrect data formats.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 3: `cleanse_and_filter_universe`**\n",
        "\n",
        "*   **Inputs:** The schema-validated `DataFrame` from Task 2.\n",
        "*   **Process:** This function sculpts the validated data into the final analytical sample. It applies a sequence of filters: (1) a temporal filter to restrict the date range, (2) a security filter to include only US common stocks on major exchanges (`shrcd`, `exchcd`), (3) a deterministic deduplication process to ensure a unique `(date, permno)` observation, and (4) a set of outlier removal rules for price, returns, and market capitalization. It also correctly incorporates delisting returns into the main return series using the formula $r_{\\text{combined}} = (1 + r_{t}) \\times (1 + r_{\\text{delist}}) - 1$.\n",
        "*   **Outputs:** A fully cleansed `pandas.DataFrame` with a unique `(date, permno)` `MultiIndex`, representing the final universe of firm-month observations.\n",
        "*   **Role in Pipeline:** This callable implements the **universe definition and data cleansing** stage, a standard and essential procedure in empirical asset pricing to ensure the analysis is performed on a well-defined and economically meaningful set of securities.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 4: `construct_characteristics`**\n",
        "\n",
        "*   **Inputs:** The cleansed `DataFrame` from Task 3.\n",
        "*   **Process:** This is the primary feature engineering step. It first performs a critical temporal alignment of quarterly accounting data to the monthly frequency, enforcing a reporting lag (e.g., 6 months) to prevent look-ahead bias. It then uses this aligned data to compute the 45 firm characteristics (e.g., Book-to-Market, Momentum) according to their precise financial formulas.\n",
        "*   **Outputs:** A wide `pandas.DataFrame` with the `(date, permno)` `MultiIndex` and 45 new columns, one for each raw characteristic.\n",
        "*   **Role in Pipeline:** This callable **constructs the raw signals** that will be used for imputation and asset pricing. Its most critical role is the rigorous handling of reporting lags to ensure the information used to construct characteristics at month `t` was actually available to investors at that time.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 5: `rank_normalize_characteristics`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` of raw characteristics from Task 4.\n",
        "*   **Process:** The function transforms the raw characteristics into a unit-free, comparable format. For each month, it cross-sectionally ranks the firms for each characteristic. It then applies a linear transformation to rescale these ranks to the interval `[-0.5, 0.5]`.\n",
        "*   **Outputs:** A `pandas.DataFrame` of the same shape, but with the raw values replaced by their normalized ranks.\n",
        "*   **Role in Pipeline:** This implements the **cross-sectional normalization** described in the \"Dataset\" section of the paper. This is a crucial step before tensor formation, as it prevents characteristics with large scales (like market cap) from dominating the factorization and ensures all signals are on an equal footing. The transformation for a rank $r_{t,n,\\ell}$ out of $N_{t,\\ell}$ firms is:\n",
        "    $$\n",
        "    \\tilde{x}_{t,n,\\ell} = \\frac{r_{t,n,\\ell} - 1}{N_{t,\\ell} - 1} - 0.5\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 6: `form_tensor_and_observed_set`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` of normalized characteristics from Task 5.\n",
        "*   **Process:** This function transforms the 2D panel data representation into the 3D tensor structure required by the model. It first creates deterministic integer mappings for the time and firm dimensions. It then allocates a `(T, N, L)` NumPy array initialized with `NaN`s and populates it with the observed, normalized characteristic values. Finally, it creates a boolean mask of the same shape to represent the set of observed entries, $\\Omega$.\n",
        "*   **Outputs:** A tuple containing the `(T, N, L)` data tensor $\\mathcal{X}$ and the `(T, N, L)` boolean mask $\\Omega$.\n",
        "*   **Role in Pipeline:** This callable performs the **tensorization** of the data, moving from a `pandas` panel to the `numpy` multilinear array that is the central data structure for the ACT-Tensor model.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 7: `create_mar_mask`**\n",
        "\n",
        "*   **Inputs:** The observed set mask $\\Omega$.\n",
        "*   **Process:** This function implements the Missing-At-Random (MAR) experimental condition. It identifies all observed entries in the tensor, and then performs a uniform random sampling without replacement to select 10% of them to be part of the held-out evaluation set.\n",
        "*   **Outputs:** A boolean tensor of shape `(T, N, L)` where `True` indicates an entry that is part of the MAR test set.\n",
        "*   **Role in Pipeline:** This is the first of three functions that **create the experimental conditions** for evaluating the imputation models.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 8: `create_block_mask`**\n",
        "\n",
        "*   **Inputs:** The observed set mask $\\Omega$.\n",
        "*   **Process:** This function implements the Block Missingness experimental condition. For each firm-characteristic time series, it probabilistically places a contiguous 12-month block of missingness. The placement is biased towards the beginning of the series to mimic real-world data gaps.\n",
        "*   **Outputs:** A boolean tensor of shape `(T, N, L)` representing the Block test set.\n",
        "*   **Role in Pipeline:** This function creates a more structured and challenging test set to evaluate the model's ability to handle non-random, temporally correlated missingness.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 9: `create_logit_mask`**\n",
        "\n",
        "*   **Inputs:** The observed set mask $\\Omega$ and the cleansed `DataFrame` (for covariates).\n",
        "*   **Process:** This function implements the most sophisticated experimental condition, Logistic Missingness. It uses a two-stage probabilistic model where the probability of an entry being missing depends on firm characteristics like size and age. It includes a calibration loop to ensure the total number of masked entries matches the 10% target.\n",
        "*   **Outputs:** A boolean tensor of shape `(T, N, L)` representing the Logit test set.\n",
        "*   **Role in Pipeline:** This function simulates a \"Missing Not At Random\" (MNAR) scenario, providing the most realistic and rigorous test of the imputation framework's ability to handle systematic biases in missingness.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 10: `create_training_tensors`**\n",
        "\n",
        "*   **Inputs:** The ground-truth tensor $\\mathcal{X}$ and an evaluation mask (e.g., `mask_eval_MAR`).\n",
        "*   **Process:** This function prepares the data for model training. It takes the ground-truth tensor and replaces the values at the locations specified by the evaluation mask with `NaN`s. It also creates the corresponding training mask, which is the set of all originally observed entries *minus* the evaluation set.\n",
        "*   **Outputs:** A tuple containing the training tensor `X_train` and the training mask `mask_train`.\n",
        "*   **Role in Pipeline:** This callable **enforces the separation of training and testing data** at the tensor level, a critical step for a valid out-of-sample evaluation.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 11: `cluster_firms_by_profile`**\n",
        "\n",
        "*   **Inputs:** The training tensor `X_train`.\n",
        "*   **Process:** This function implements the first step of the ACT-Tensor algorithm. It vectorizes each firm's `(T, L)` slice of the tensor and applies the K-Means algorithm to partition the `N` firms into `K` clusters based on the similarity of their data profiles (which are dominated by missingness patterns).\n",
        "*   **Outputs:** A dictionary mapping each cluster ID to a list of the firm indices belonging to it.\n",
        "*   **Role in Pipeline:** This callable implements the **firm clustering** based on the K-Means objective function:\n",
        "    $$\n",
        "    \\min_{z \\in \\{1, \\dots, K\\}^{N}, \\{\\boldsymbol{\\mu}_{k}\\}_{k=1}^{K}} \\sum_{n=1}^{N} \\left\\| \\mathbf{v}_{n} - \\boldsymbol{\\mu}_{z_{n}} \\right\\|_{2}^{2}\n",
        "    $$\n",
        "    where $\\mathbf{v}_n$ is the vectorized profile of firm $n$.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 12: `compute_densities_and_partition`**\n",
        "\n",
        "*   **Inputs:** The training mask `mask_train` and the cluster assignments from Task 11.\n",
        "*   **Process:** For each cluster, this function calculates its data density: the fraction of observed entries in its corresponding sub-tensor. It then compares this density to the threshold $\\tau$ (e.g., 0.40) to classify the cluster as either \"dense\" or \"sparse\".\n",
        "*   **Outputs:** Two lists of cluster IDs: one for the dense clusters and one for the sparse clusters.\n",
        "*   **Role in Pipeline:** This callable implements the **cluster classification** that directs the \"divide and conquer\" strategy. It is based on the density formula:\n",
        "    $$\n",
        "    \\rho_{k} = \\frac{1}{|\\mathcal{I}_{k}| \\cdot T \\cdot L} \\sum_{n \\in \\mathcal{I}_{k}} \\sum_{t=0}^{T-1} \\sum_{\\ell=0}^{L-1} \\mathbf{1}\\left[(t, n, \\ell) \\in \\Omega_{\\text{train}}\\right]\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 13 & 14 Fused: `_solve_cp_als`, `complete_dense_clusters`, `complete_sparse_clusters`**\n",
        "\n",
        "*   **Inputs:** The sub-tensors and sub-masks for each cluster, and the dense/sparse classification.\n",
        "*   **Process:** These functions form the core of the imputation engine.\n",
        "    *   `complete_dense_clusters` iterates through the dense clusters and calls the canonical solver `_solve_cp_als` with `lambda_reg=0.0` on each dense sub-tensor.\n",
        "    *   `complete_sparse_clusters` iterates through the sparse clusters. For each, it first constructs an *augmented* tensor by combining the sparse firms with all dense firms. It then calls `_solve_cp_als` on this larger, more stable tensor. Finally, it slices out the imputed data corresponding to the original sparse firms.\n",
        "    *   `_solve_cp_als` is the workhorse. It implements the Alternating Least Squares algorithm to solve the regularized CP decomposition objective function by iteratively solving the normal equations for each factor matrix. It includes robust SVD-based initialization.\n",
        "*   **Outputs:** A set of completed (fully dense) sub-tensors, one for each of the `K` clusters, saved to disk.\n",
        "*   **Role in Pipeline:** These callables implement the central **cluster-wise tensor completion** algorithm. The objective function solved by the ALS algorithm is:\n",
        "    $$\n",
        "    \\min_{U, V, W} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X}) - \\mathcal{P}_{\\Omega}([\\![U, V, W]\\!]) \\right\\|_{F}^{2} + \\lambda \\left( \\|U\\|_{F}^{2} + \\|V\\|_{F}^{2} + \\|W\\|_{F}^{2} \\right)\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 15: `assemble_completed_tensor`**\n",
        "\n",
        "*   **Inputs:** The `K` completed sub-tensors from the previous step and the cluster assignments.\n",
        "*   **Process:** This function reverses the partitioning. It allocates a new, full-sized `(T, N, L)` tensor and then, for each cluster, places its completed sub-tensor into the correct firm \"slots\" in the global tensor.\n",
        "*   **Outputs:** A single, fully dense `(T, N, L)` tensor, $\\hat{\\mathcal{X}}$.\n",
        "*   **Role in Pipeline:** This callable performs the **global assembly**, completing the \"conquer\" phase of the imputation strategy.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 16, 17, 18: `apply_cma_smoothing`, `apply_ema_smoothing`, `apply_kf_smoothing`**\n",
        "\n",
        "*   **Inputs:** The assembled, dense tensor $\\hat{\\mathcal{X}}$.\n",
        "*   **Process:** These functions implement the three alternative temporal smoothers. Each is applied to every firm-characteristic time series (i.e., along the time axis of the tensor) to filter out short-term noise.\n",
        "    *   `apply_cma_smoothing` uses a symmetric Centered Moving Average.\n",
        "    *   `apply_ema_smoothing` uses a one-sided, recursive Exponential Moving Average.\n",
        "    *   `apply_kf_smoothing` uses a full Kalman Filter and RTS smoother based on a random walk state-space model, with hyperparameters chosen via a Maximum Likelihood grid search.\n",
        "*   **Outputs:** The final, smoothed, imputed tensor $\\tilde{\\mathcal{X}}$.\n",
        "*   **Role in Pipeline:** These callables implement the **temporal smoothing module**, the second key innovation of the ACT-Tensor framework, designed to improve the signal-to-noise ratio of the imputed data.\n",
        "\n",
        "\n",
        "\n",
        "#### **Tasks 19-34 (Evaluation and Reporting)**\n",
        "\n",
        "The remaining callables are part of the evaluation and reporting pipeline. Their roles are as follows:\n",
        "*   `evaluate_imputation_accuracy`: Implements the four core statistical error metrics (RMSE, MAE, MAPE, R²) to measure imputation quality.\n",
        "*   `evaluate_sparse_cluster_accuracy`: A specialized orchestrator that applies the evaluation to the sparse-cluster subset, stress-testing the model.\n",
        "*   `run_baseline_imputations`: Implements the three competitor models (Median, BF-XS, B-XS).\n",
        "*   `evaluate_baseline_methods`: An orchestrator to evaluate the baselines using the same canonical metrics.\n",
        "*   `run_ablation_*`: A suite of orchestrators to run the ablation experiments (CP-only, CP+Clustering, etc.).\n",
        "*   `compile_*_results`: A suite of functions to aggregate raw results into formatted tables.\n",
        "*   `run_regularization_stability_test`: An orchestrator for the $\\lambda$-sensitivity analysis.\n",
        "*   `construct_portfolio_return_tensor`: Implements the double-sort methodology to create the portfolio return tensor $\\mathcal{R}$, the primary input for the asset pricing tests.\n",
        "*   `extract_latent_factors`: Implements the partial Tucker decomposition (HOSVD) to extract the latent factor matrix $\\mathbf{F}$ from $\\mathcal{R}$.\n",
        "*   `select_factors_and_predict`: Implements the forward stepwise selection to choose the best predictive factors and generate return forecasts $\\hat{\\mathbf{R}}$.\n",
        "*   `compute_asset_pricing_metrics`: Implements the calculation of pricing errors (RMSEα, MAEα) and predictive skill metrics (IC, MAE-Rank).\n",
        "*   `construct_tb_portfolio`: Simulates the Top-minus-Bottom long-short strategy based on the model's forecasts.\n",
        "*   `compute_annualized_sharpe_ratio`: Calculates the final risk-adjusted return metric for the T-B strategy.\n",
        "\n",
        "\n",
        "\n",
        "#### **Tasks 35, 36, 37 (Top-Level Orchestration)**\n",
        "\n",
        "*   `run_act_tensor_pipeline`: The orchestrator for a single, complete experimental run.\n",
        "*   `execute_full_experimental_suite`: The master script that manages the entire campaign of experiments.\n",
        "*   `generate_publication_artifacts`: The final reporting engine that produces all tables and figures.\n",
        "\n",
        "This concludes the comprehensive explication of every functional component of the ACT-Tensor pipeline.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Example**\n",
        "\n",
        "### **Discussion of the End-to-End Usage Example**\n",
        "\n",
        "The example will be structured as a self-contained script or notebook cell that performs three key actions: setting up the environment, defining the inputs, and executing the top-level orchestrator.\n",
        "\n",
        "#### **1. Environment Setup: Creating Synthetic Inputs**\n",
        "\n",
        "*   **Underlying Concept:** A reproducible example must not depend on proprietary data access. It must provide its own inputs. Therefore, the first step is to programmatically generate high-fidelity synthetic data that conforms *exactly* to the required input schemas.\n",
        "*   **Fidelity to Concept:**\n",
        "    1.  **Synthetic `DataFrame`:** The helper function, `_create_synthetic_data`, generates a `pandas.DataFrame`. This `DataFrame` will not be random noise. It will be structured to mimic the properties of real financial data:\n",
        "        *   It will have all the required columns with the correct `dtypes`.\n",
        "        *   It will span the correct date range (`2016-01-01` to `2020-12-31`).\n",
        "        *   It will contain a small number of firms (`N=100`) and generate both monthly and quarterly records to simulate the CRSP/Compustat merge.\n",
        "        *   Values will be drawn from plausible distributions (e.g., prices from a log-normal distribution, returns from a normal distribution, accounting variables with realistic positive values).\n",
        "        *   Crucially, it will introduce `NaN`s to mimic the natural missingness of financial data.\n",
        "    2.  **Saving the Data:** This synthetic `DataFrame` will be saved to a Parquet file at a specified path (e.g., `data/raw/crsp_compustat_merged.parquet`). This simulates the real-world scenario where the user starts with a prepared data file.\n",
        "    3.  **Configuration File:** `PyYAML` will be used to load the `config.yaml` YAML file and `json` to dump it to the required `act_tensor_config.json` format. This demonstrates the professional practice of using YAML for human editing and JSON for machine parsing.\n",
        "*   **Best Practices:** Creating a synthetic data generator is a hallmark of a testable codebase. It allows for unit testing, integration testing, and provides a completely self-contained example for new users, without requiring them to have access to a WRDS subscription.\n",
        "\n",
        "#### **2. Defining the Inputs for the Orchestrator**\n",
        "\n",
        "*   **Underlying Concept:** The top-level orchestrator requires three simple string inputs: the path to the data, the path to the config, and the path for the output directory.\n",
        "*   **Fidelity to Concept:** The example script will define these three path variables explicitly, pointing to the files and directories created in the setup step. This makes the connection between the setup and execution phases perfectly clear.\n",
        "\n",
        "#### **3. Executing the Top-Level Orchestrator**\n",
        "\n",
        "*   **Underlying Concept:** The final step is to call the master function that runs the entire study.\n",
        "*   **Fidelity to Concept:** The script will make a single call to `run_complete_act_tensor_replication`, passing the three path variables. The call will be wrapped in a `if __name__ == \"__main__\":` block, which is the standard for an executable Python script.\n",
        "*   **Challenges & Resolutions:** The only challenge is ensuring that all 40+ functions defined throughout our conversation are available in the execution scope. The prompt specifies assuming they are all in a single notebook. In a real project, they would be organized into modules and imported. The example will be written with this assumption in mind.\n",
        "\n",
        "This complete example will provide a user with everything they need to understand and execute the entire ACT-Tensor replication pipeline on a controlled, synthetic dataset, thereby validating the entire codebase and providing a clear template for running it on their own proprietary data.\n",
        "\n",
        "I will now provide the complete, professional-grade usage example.\n",
        "\n",
        "```python\n",
        "# ==============================================================================\n",
        "# Final Task: End-to-End Pipeline Usage Example\n",
        "# ==============================================================================\n",
        "\n",
        "# Import necessary libraries for this example script\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "# Assume all previously defined canonical functions for the ACT-Tensor pipeline\n",
        "# (Tasks 1-37) are defined in the preceding cells of this notebook.\n",
        "\n",
        "# Configure a logger for the example script\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Environment Setup - Synthetic Data and Config File Generation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_synthetic_data(\n",
        "    num_firms: int,\n",
        "    start_date: str,\n",
        "    end_date: str,\n",
        "    output_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates a high-fidelity synthetic dataset mimicking a CRSP/Compustat merge.\n",
        "    \"\"\"\n",
        "    logging.info(\"Generating high-fidelity synthetic data...\")\n",
        "    \n",
        "    # Define date ranges for monthly and quarterly data.\n",
        "    dates_monthly = pd.to_datetime(pd.date_range(start=start_date, end=end_date, freq='M'))\n",
        "    dates_quarterly = pd.to_datetime(pd.date_range(start=start_date, end=end_date, freq='Q'))\n",
        "    \n",
        "    # Create unique firm identifiers.\n",
        "    permnos = np.arange(10001, 10001 + num_firms)\n",
        "    \n",
        "    # Create the base DataFrame with all firm-month combinations.\n",
        "    df = pd.DataFrame(\n",
        "        index=pd.MultiIndex.from_product([dates_monthly, permnos], names=['date', 'permno'])\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- Generate Realistic Data ---\n",
        "    # Market Data (monthly)\n",
        "    df['shrcd'] = 11\n",
        "    df['exchcd'] = np.random.choice([1, 2, 3], size=len(df))\n",
        "    df['prc'] = np.random.lognormal(mean=3, sigma=1, size=len(df)).clip(1, 10000)\n",
        "    df['ret'] = np.random.normal(loc=0.01, scale=0.08, size=len(df))\n",
        "    df['shrout'] = np.random.lognormal(mean=10, sigma=2, size=len(df)) * 1000\n",
        "    df['RF'] = 0.01 # Simplified constant risk-free rate in percent\n",
        "\n",
        "    # Accounting Data (quarterly) - create a separate DataFrame first.\n",
        "    df_q = pd.DataFrame(\n",
        "        index=pd.MultiIndex.from_product([dates_quarterly, permnos], names=['datadate', 'permno'])\n",
        "    ).reset_index()\n",
        "    df_q['atq'] = np.random.lognormal(mean=7, sigma=2, size=len(df_q)) * 100\n",
        "    df_q['seqq'] = df_q['atq'] * np.random.uniform(0.3, 0.7, size=len(df_q))\n",
        "    df_q['ibq'] = df_q['seqq'] * np.random.normal(0.02, 0.03, size=len(df_q))\n",
        "    \n",
        "    # Merge quarterly data onto the monthly frame.\n",
        "    # This will naturally create NaNs for non-quarter-end months.\n",
        "    df['datadate'] = df['date'].where(df['date'].dt.is_quarter_end)\n",
        "    df = pd.merge(df, df_q, on=['permno', 'datadate'], how='left')\n",
        "    \n",
        "    # --- Add All Other Required Columns with Default/NaN Values ---\n",
        "    # This ensures the DataFrame conforms to the required schema.\n",
        "    all_cols = [\n",
        "        \"permno\", \"permco\", \"comnam\", \"shrcd\", \"exchcd\", \"siccd\", \"ncusip\",\n",
        "        \"prc\", \"ret\", \"retx\", \"shrout\", \"vol\", \"stko\", \"dlstdt\", \"dlstcd\", \"dlret\",\n",
        "        \"gvkey\", \"datadate\", \"fyearq\", \"fqtr\", \"fyr\", \"indfmt\", \"consol\", \"popsrc\",\n",
        "        \"datafmt\", \"tic\", \"cusip\", \"sich\", \"atq\", \"ltq\", \"seqq\", \"ceqq\", \"saleq\",\n",
        "        \"ibq\", \"oibdpq\", \"pstkq\", \"cogsq\", \"xsgaq\", \"ppentq\", \"invtq\", \"lpermno\",\n",
        "        \"linktype\", \"linkprim\", \"linkdt\", \"linkenddt\", \"RF\", \"namedt\", \"nameenddt\", \"shrcls\"\n",
        "    ]\n",
        "    for col in all_cols:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "            \n",
        "    # Introduce additional random missingness to simulate reality.\n",
        "    for col in ['ret', 'prc', 'atq', 'seqq', 'ibq']:\n",
        "        mask = np.random.rand(len(df)) < 0.2\n",
        "        df.loc[mask, col] = np.nan\n",
        "\n",
        "    # Save the synthetic data to a Parquet file.\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    df.to_parquet(output_path)\n",
        "    logging.info(f\"Synthetic data with {len(df)} rows and {len(df.columns)} columns saved to {output_path}\")\n",
        "\n",
        "\n",
        "def _create_json_config_file(yaml_path: str, json_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Creates the JSON equivalent of a YAML config file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the YAML and save it as JSON for the pipeline.\n",
        "    with open(yaml_path, 'r') as f:\n",
        "        config_dict = yaml.safe_load(f)\n",
        "    \n",
        "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(config_dict, f, indent=4)\n",
        "    logging.info(f\"JSON config saved to {json_path}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Main Execution Block\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    \"\"\"\n",
        "    This block serves as the main entry point for demonstrating the use of the\n",
        "    end-to-end pipeline. It first sets up the necessary environment by creating\n",
        "    synthetic data and configuration files, and then calls the top-level\n",
        "    orchestrator to run the full replication study.\n",
        "    \"\"\"\n",
        "    \n",
        "    # --- Define File Paths ---\n",
        "    # Define the paths where the synthetic data and config files will be stored,\n",
        "    # and where all experimental outputs will be saved.\n",
        "    RAW_DATA_FILE = \"data/raw/synthetic_crsp_compustat.parquet\"\n",
        "    CONFIG_YAML_FILE = \"config.yaml\"\n",
        "    CONFIG_JSON_FILE = \"act_tensor_config.json\"\n",
        "    RESULTS_DIRECTORY = \"replication_output\"\n",
        "\n",
        "    # --- 1. Set up the Environment ---\n",
        "    # Generate the synthetic raw data file.\n",
        "    _create_synthetic_data(\n",
        "        num_firms=100,\n",
        "        start_date=\"2016-01-01\",\n",
        "        end_date=\"2020-12-31\",\n",
        "        output_path=RAW_DATA_FILE\n",
        "    )\n",
        "    \n",
        "    # Generate the configuration files.\n",
        "    _create_json_config_file(\n",
        "        yaml_path=CONFIG_YAML_FILE,\n",
        "        json_path=CONFIG_JSON_FILE\n",
        "    )\n",
        "\n",
        "    # --- 2. Execute the Entire Replication Study ---\n",
        "    # This single function call launches the entire end-to-end pipeline.\n",
        "    # It will run all 9 experimental conditions (3 regimes x 3 smoothers),\n",
        "    # including all baseline, ablation, and sensitivity analyses, and then\n",
        "    # aggregate all results into final, publication-ready tables and figures.\n",
        "    logging.info(f\"\\n{'='*80}\\nLAUNCHING FULL REPLICATION SUITE...\\n{'='*80}\")\n",
        "    \n",
        "    # Call the top-level orchestrator from the previous task.\n",
        "    final_artifacts = run_complete_act_tensor_replication(\n",
        "        data_path=RAW_DATA_FILE,\n",
        "        config_path=CONFIG_JSON_FILE,\n",
        "        base_output_dir=RESULTS_DIRECTORY\n",
        "    )\n",
        "\n",
        "    # --- 3. Conclude ---\n",
        "    # Print the final output, which contains the paths to all generated artifacts.\n",
        "    logging.info(f\"\\n{'='*80}\\nREPLICATION SUITE COMPLETE.\\n{'='*80}\")\n",
        "    logging.info(\"Final artifacts generated:\")\n",
        "    logging.info(json.dumps(final_artifacts, indent=4))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "7P2mk7YwkRYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Validate and Parse Configuration Dictionary\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate and Parse Configuration Dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Type and structure validation (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_structure_and_log(\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the top-level structure of the configuration dictionary and logs it.\n",
        "\n",
        "    This helper function ensures that the configuration is a dictionary and contains\n",
        "    all required primary keys. It also verifies that nested values are dictionaries\n",
        "    where expected. Finally, it creates a timestamped JSON snapshot of the\n",
        "    configuration for audit and reproducibility purposes.\n",
        "\n",
        "    Args:\n",
        "        config: The configuration dictionary to validate.\n",
        "        output_dir: The directory where the configuration snapshot will be saved.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the config is not a dictionary or if a nested value has an\n",
        "                   incorrect type.\n",
        "        KeyError: If a required top-level key is missing from the config.\n",
        "    \"\"\"\n",
        "    # Assert that the provided configuration is a Python dictionary.\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Configuration must be a dictionary.\")\n",
        "\n",
        "    # Define the set of all required top-level keys for the study.\n",
        "    required_keys: List[str] = [\n",
        "        \"study_scope\", \"linking_policy\", \"tensor_policies\",\n",
        "        \"imputation_hparams\", \"optimization_settings\", \"smoothing_settings\",\n",
        "        \"masking_regimes\", \"portfolio_settings\", \"factor_extraction\",\n",
        "        \"evaluation_metrics\", \"stability_test\", \"reproducibility\", \"guardrails\"\n",
        "    ]\n",
        "\n",
        "    # Verify the presence of each required top-level key.\n",
        "    for key in required_keys:\n",
        "        if key not in config:\n",
        "            raise KeyError(f\"Missing required top-level key in config: '{key}'\")\n",
        "\n",
        "    # Verify that nested structures are dictionaries, with exceptions.\n",
        "    for key, value in config.items():\n",
        "        # The 'evaluation_metrics' key is allowed to contain lists of strings.\n",
        "        if key == \"evaluation_metrics\":\n",
        "            if not isinstance(value, dict):\n",
        "                raise TypeError(\n",
        "                    f\"Config key '{key}' must be a dictionary.\"\n",
        "                )\n",
        "        # All other top-level keys must correspond to dictionary values.\n",
        "        elif key in required_keys and not isinstance(value, dict):\n",
        "            raise TypeError(\n",
        "                f\"Config key '{key}' must be a dictionary, but found {type(value)}.\"\n",
        "            )\n",
        "\n",
        "    # Ensure the output directory for the log file exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Generate a timestamp for the configuration snapshot file.\n",
        "    timestamp: str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Define the full path for the snapshot file.\n",
        "    snapshot_path: str = os.path.join(\n",
        "        output_dir, f\"config_snapshot_{timestamp}.json\"\n",
        "    )\n",
        "\n",
        "    # Write the complete configuration to a JSON file for auditing.\n",
        "    with open(snapshot_path, 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate study scope and date boundaries (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_study_scope(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates parameters related to the study's scope, dates, and universe.\n",
        "\n",
        "    This function parses date strings, verifies their chronological order, and\n",
        "    cross-validates the expected number of months against the configuration.\n",
        "    It also checks the validity of security filters.\n",
        "\n",
        "    Args:\n",
        "        config: The configuration dictionary to validate.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If dates are invalid, out of order, or if the expected\n",
        "                    number of months is inconsistent.\n",
        "        TypeError: If filter parameters are not of the expected type (list of ints).\n",
        "    \"\"\"\n",
        "    # Extract the study scope sub-dictionary for easier access.\n",
        "    scope: Dict[str, Any] = config[\"study_scope\"]\n",
        "\n",
        "    # Extract the tensor policies sub-dictionary.\n",
        "    tensor_policies: Dict[str, Any] = config[\"tensor_policies\"]\n",
        "\n",
        "    # Parse start and end dates into pandas Timestamp objects for robust comparison.\n",
        "    try:\n",
        "        start_date: pd.Timestamp = pd.to_datetime(scope[\"start_date\"])\n",
        "        end_date: pd.Timestamp = pd.to_datetime(scope[\"end_date\"])\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Could not parse 'start_date' or 'end_date'. Error: {e}\")\n",
        "\n",
        "    # Assert that the study's time window is chronologically valid.\n",
        "    if end_date < start_date:\n",
        "        raise ValueError(\n",
        "            f\"end_date ({end_date}) cannot be before start_date ({start_date}).\"\n",
        "        )\n",
        "\n",
        "    # Calculate the expected number of months in the study period.\n",
        "    # Formula: T_expected = 12 * (end_year - start_year) + (end_month - start_month) + 1\n",
        "    t_expected: int = (\n",
        "        12 * (end_date.year - start_date.year) +\n",
        "        (end_date.month - start_date.month) + 1\n",
        "    )\n",
        "\n",
        "    # Verify that the calculated number of months matches the value specified in tensor_policies.\n",
        "    if tensor_policies[\"T_months_expected\"] != t_expected:\n",
        "        raise ValueError(\n",
        "            f\"Inconsistency in config: 'T_months_expected' is \"\n",
        "            f\"{tensor_policies['T_months_expected']}, but calculated period \"\n",
        "            f\"is {t_expected} months.\"\n",
        "        )\n",
        "\n",
        "    # Validate the share code filter for common stocks.\n",
        "    share_codes: List[int] = scope[\"share_code_filter\"]\n",
        "    if not (\n",
        "        isinstance(share_codes, list) and\n",
        "        all(isinstance(i, int) for i in share_codes) and\n",
        "        share_codes\n",
        "    ):\n",
        "        raise TypeError(\"'share_code_filter' must be a non-empty list of integers.\")\n",
        "\n",
        "    # Ensure share codes are the standard CRSP codes for common stock.\n",
        "    if not all(code in {10, 11} for code in share_codes):\n",
        "        raise ValueError(f\"Invalid 'share_code_filter' values. Must be in {{10, 11}}.\")\n",
        "\n",
        "    # Validate the exchange code filter.\n",
        "    exchange_codes: List[int] = scope[\"include_exchanges\"]\n",
        "    if not (\n",
        "        isinstance(exchange_codes, list) and\n",
        "        all(isinstance(i, int) for i in exchange_codes) and\n",
        "        exchange_codes\n",
        "    ):\n",
        "        raise TypeError(\"'include_exchanges' must be a non-empty list of integers.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate hyperparameters and set seeds (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_hyperparameters(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates numerical and choice-based hyperparameters and sets random seeds.\n",
        "\n",
        "    This function performs detailed checks on tensor dimensions, optimization\n",
        "    parameters, and smoothing settings. It concludes by setting the global\n",
        "    random seeds for NumPy and Python's `random` module to ensure\n",
        "    reproducibility.\n",
        "\n",
        "    Args:\n",
        "        config: The configuration dictionary to validate.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a hyperparameter is outside its admissible range.\n",
        "        TypeError: If a hyperparameter has an incorrect type.\n",
        "    \"\"\"\n",
        "    # --- Validate integer parameters ---\n",
        "    # Define a mapping of parameter paths to their dictionary locations.\n",
        "    int_params_paths: Dict[str, List[str]] = {\n",
        "        \"CP_rank_R\": [\"imputation_hparams\"],\n",
        "        \"clusters_K\": [\"imputation_hparams\"],\n",
        "        \"L_characteristics\": [\"tensor_policies\"],\n",
        "        \"P_size_buckets\": [\"portfolio_settings\"],\n",
        "        \"Q_char_buckets\": [\"portfolio_settings\"],\n",
        "        \"k_p\": [\"factor_extraction\", \"mode_ranks\"],\n",
        "        \"k_q\": [\"factor_extraction\", \"mode_ranks\"],\n",
        "        \"k_ell\": [\"factor_extraction\", \"mode_ranks\"],\n",
        "    }\n",
        "    # Iterate and validate that each required integer parameter is a positive integer.\n",
        "    for param, path in int_params_paths.items():\n",
        "        sub_dict = config\n",
        "        for key in path:\n",
        "            sub_dict = sub_dict[key]\n",
        "        value = sub_dict[param]\n",
        "        if not (isinstance(value, int) and value > 0):\n",
        "            raise ValueError(f\"Parameter '{param}' must be a positive integer.\")\n",
        "\n",
        "    # --- Validate float parameters ---\n",
        "    # Verify density threshold tau is in the interval (0, 1].\n",
        "    tau: float = config[\"imputation_hparams\"][\"density_threshold_tau\"]\n",
        "    if not (isinstance(tau, float) and 0.0 < tau <= 1.0):\n",
        "        raise ValueError(\n",
        "            \"'density_threshold_tau' must be a float in (0, 1].\"\n",
        "        )\n",
        "\n",
        "    # Validate ridge lambda is a non-negative float.\n",
        "    ridge_lambda: float = config[\"imputation_hparams\"][\"ridge_lambda\"]\n",
        "    if not (isinstance(ridge_lambda, float) and ridge_lambda >= 0.0):\n",
        "        raise ValueError(\"'ridge_lambda' must be a non-negative float.\")\n",
        "\n",
        "    # --- Validate smoothing settings ---\n",
        "    smoothing_settings: Dict[str, Any] = config[\"smoothing_settings\"]\n",
        "    # Check if the selected smoothing method is one of the valid options.\n",
        "    if smoothing_settings[\"method\"] not in {\"CMA\", \"EMA\", \"KF\"}:\n",
        "        raise ValueError(\n",
        "            f\"Invalid smoothing method: '{smoothing_settings['method']}'. \"\n",
        "            f\"Must be 'CMA', 'EMA', or 'KF'.\"\n",
        "        )\n",
        "\n",
        "    # Validate CMA delta: must be an odd, positive integer.\n",
        "    cma_delta: int = smoothing_settings[\"CMA\"][\"delta\"]\n",
        "    if not (isinstance(cma_delta, int) and cma_delta > 0 and cma_delta % 2 == 1):\n",
        "        raise ValueError(\"'CMA:delta' must be an odd positive integer.\")\n",
        "\n",
        "    # Validate EMA theta: must be a float in (0, 1).\n",
        "    ema_theta: float = smoothing_settings[\"EMA\"][\"theta\"]\n",
        "    if not (isinstance(ema_theta, float) and 0.0 < ema_theta < 1.0):\n",
        "        raise ValueError(\"'EMA:theta' must be a float in (0, 1).\")\n",
        "\n",
        "    # Validate Kalman Filter grids: must be non-empty lists of positive floats.\n",
        "    for grid_name in [\"process_var_h_grid\", \"meas_var_r_grid\"]:\n",
        "        grid: List[float] = smoothing_settings[\"KF\"][grid_name]\n",
        "        if not (\n",
        "            isinstance(grid, list) and\n",
        "            all(isinstance(v, (float, int)) and v > 0 for v in grid) and\n",
        "            grid\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"'KF:{grid_name}' must be a non-empty list of positive numbers.\"\n",
        "            )\n",
        "\n",
        "    # --- Validate reproducibility settings and set seeds ---\n",
        "    repro_config: Dict[str, Any] = config[\"reproducibility\"]\n",
        "    # Check that all specified seeds are non-negative integers.\n",
        "    for key, value in repro_config.items():\n",
        "        if \"seed\" in key:\n",
        "            if not (isinstance(value, int) and value >= 0):\n",
        "                raise ValueError(f\"Seed '{key}' must be a non-negative integer.\")\n",
        "\n",
        "    # Set global random seeds immediately after validation for reproducibility.\n",
        "    # This is a critical step that affects the global state.\n",
        "    np.random.seed(repro_config[\"seed_global\"])\n",
        "    random.seed(repro_config[\"seed_global\"])\n",
        "\n",
        "    # Confirm the specified float dtype is 'float64' for numerical precision.\n",
        "    if repro_config[\"float_dtype\"] != \"float64\":\n",
        "        raise ValueError(\"'float_dtype' must be 'float64'.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def validate_config(\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"config_logs\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the entire ACT-Tensor configuration dictionary.\n",
        "\n",
        "    This function serves as the main entry point for configuration validation. It\n",
        "    sequentially calls helper functions to perform structural, scope-based, and\n",
        "    hyperparameter-level checks. A valid configuration is returned upon successful\n",
        "    completion, and a timestamped snapshot is saved to disk. This function is\n",
        "    the first critical guardrail in the research pipeline.\n",
        "\n",
        "    Args:\n",
        "        config: The `act_tensor_config` dictionary to be validated.\n",
        "        output_dir: Directory to save the configuration snapshot. Defaults to\n",
        "                    \"config_logs\".\n",
        "\n",
        "    Returns:\n",
        "        The validated configuration dictionary, unchanged.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the config or its components have incorrect types.\n",
        "        KeyError: If required keys are missing.\n",
        "        ValueError: If parameter values are outside their admissible ranges or\n",
        "                    are inconsistent.\n",
        "    \"\"\"\n",
        "    # Step 1: Validate the overall dictionary structure and create a snapshot.\n",
        "    _validate_structure_and_log(config, output_dir)\n",
        "\n",
        "    # Step 2: Validate the study scope, date boundaries, and universe filters.\n",
        "    _validate_study_scope(config)\n",
        "\n",
        "    # Step 3: Validate all numerical hyperparameters and set global random seeds.\n",
        "    _validate_hyperparameters(config)\n",
        "\n",
        "    # If all validations pass, return the original configuration dictionary.\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "ucU6lLaOfk1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Validate and Enforce DataFrame Schema\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate and Enforce DataFrame Schema\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Index and column presence validation (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_columns_and_set_index(\n",
        "    df: pd.DataFrame,\n",
        "    required_columns: List[str],\n",
        "    core_columns: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates column presence and sets a DatetimeIndex.\n",
        "\n",
        "    This function ensures that all required columns exist in the DataFrame. It\n",
        "    raises a critical error if core columns are missing. It then ensures a 'date'\n",
        "    column exists, converts it to a pandas DatetimeIndex, and drops any rows\n",
        "    with invalid date entries.\n",
        "\n",
        "    Args:\n",
        "        df: The input raw DataFrame.\n",
        "        required_columns: A list of all column names expected in the DataFrame.\n",
        "        core_columns: A subset of required_columns that are absolutely essential\n",
        "                      for the pipeline to run.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with a validated DatetimeIndex named 'date'.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If any core columns are missing.\n",
        "        ValueError: If the 'date' column is missing or cannot be processed.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame in place.\n",
        "    df_validated = df.copy()\n",
        "\n",
        "    # Verify the presence of all required columns.\n",
        "    missing_cols = [col for col in required_columns if col not in df_validated.columns]\n",
        "    if missing_cols:\n",
        "        # Check if any of the missing columns are core to the analysis.\n",
        "        missing_core_cols = [col for col in missing_cols if col in core_columns]\n",
        "        if missing_core_cols:\n",
        "            # If core columns are missing, this is a fatal error.\n",
        "            raise KeyError(\n",
        "                f\"Fatal error: Core columns are missing from the DataFrame: \"\n",
        "                f\"{missing_core_cols}\"\n",
        "            )\n",
        "        # If non-core columns are missing, log a warning.\n",
        "        logging.warning(f\"Non-core columns are missing: {missing_cols}\")\n",
        "\n",
        "    # Ensure the 'date' column exists, as it's required for the index.\n",
        "    if \"date\" not in df_validated.columns:\n",
        "        raise ValueError(\"The required 'date' column is not in the DataFrame.\")\n",
        "\n",
        "    # Convert the 'date' column to datetime objects, coercing errors to NaT.\n",
        "    df_validated[\"date\"] = pd.to_datetime(df_validated[\"date\"], errors=\"coerce\")\n",
        "\n",
        "    # Identify and log the number of rows with unparseable dates.\n",
        "    invalid_dates_count = df_validated[\"date\"].isnull().sum()\n",
        "    if invalid_dates_count > 0:\n",
        "        logging.warning(\n",
        "            f\"Found and will drop {invalid_dates_count} rows with invalid dates.\"\n",
        "        )\n",
        "        # Remove rows where the date could not be parsed.\n",
        "        df_validated.dropna(subset=[\"date\"], inplace=True)\n",
        "\n",
        "    # Set the 'date' column as the DataFrame's index.\n",
        "    df_validated.set_index(\"date\", inplace=True)\n",
        "\n",
        "    return df_validated\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Type enforcement and conversion (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _enforce_data_types(\n",
        "    df: pd.DataFrame,\n",
        "    output_dir: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Enforces strict data types for all columns in the DataFrame.\n",
        "\n",
        "    This function applies a predefined dtype map to the DataFrame. It uses\n",
        "    `pd.to_numeric` and `pd.to_datetime` with error coercion, which converts\n",
        "    unparseable values to NaN/NaT without halting execution. A data quality\n",
        "    report detailing null counts post-conversion is saved to disk.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame with a DatetimeIndex.\n",
        "        output_dir: Directory to save the data quality report.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with all columns cast to their specified dtypes.\n",
        "    \"\"\"\n",
        "    # Create a copy to ensure the original DataFrame is not modified.\n",
        "    df_typed = df.copy()\n",
        "\n",
        "    # Define the strict schema with target data types for each column group.\n",
        "    # Note: Pandas will automatically use float for integer columns with NaNs.\n",
        "    # We use nullable integer 'Int64' for integer columns that might contain nulls.\n",
        "    dtype_map: Dict[str, str] = {\n",
        "        # Identifiers (nullable integers)\n",
        "        \"permno\": \"Int64\", \"permco\": \"Int64\", \"shrcd\": \"Int64\",\n",
        "        \"exchcd\": \"Int64\", \"siccd\": \"Int64\", \"dlstcd\": \"Int64\",\n",
        "        \"fyearq\": \"Int64\", \"fqtr\": \"Int64\", \"fyr\": \"Int64\",\n",
        "        \"sich\": \"Int64\", \"lpermno\": \"Int64\", \"vol\": \"Int64\",\n",
        "        # Core financial metrics (floats)\n",
        "        \"prc\": \"float64\", \"ret\": \"float64\", \"retx\": \"float64\",\n",
        "        \"shrout\": \"float64\", \"dlret\": \"float64\", \"RF\": \"float64\",\n",
        "        # Compustat quarterly accounting items (floats)\n",
        "        \"atq\": \"float64\", \"ltq\": \"float64\", \"seqq\": \"float64\",\n",
        "        \"ceqq\": \"float64\", \"saleq\": \"float64\", \"ibq\": \"float64\",\n",
        "        \"oibdpq\": \"float64\", \"pstkq\": \"float64\", \"cogsq\": \"float64\",\n",
        "        \"xsgaq\": \"float64\", \"ppentq\": \"float64\", \"invtq\": \"float64\",\n",
        "        # String/Object columns\n",
        "        \"comnam\": \"object\", \"ncusip\": \"object\", \"stko\": \"object\",\n",
        "        \"gvkey\": \"object\", \"indfmt\": \"object\", \"consol\": \"object\",\n",
        "        \"popsrc\": \"object\", \"datafmt\": \"object\", \"tic\": \"object\",\n",
        "        \"cusip\": \"object\", \"linktype\": \"object\", \"linkprim\": \"object\",\n",
        "        \"shrcls\": \"object\",\n",
        "        # Date columns (already handled in index, but good practice to list)\n",
        "        \"datadate\": \"object\", \"dlstdt\": \"datetime64[ns]\",\n",
        "        \"linkdt\": \"datetime64[ns]\", \"linkenddt\": \"datetime64[ns]\",\n",
        "        \"namedt\": \"object\", \"nameenddt\": \"object\"\n",
        "    }\n",
        "\n",
        "    # Iterate through the schema and apply type conversions.\n",
        "    for col, dtype in dtype_map.items():\n",
        "        if col in df_typed.columns:\n",
        "            if 'datetime' in str(dtype):\n",
        "                # Convert date-related columns, coercing errors to NaT.\n",
        "                df_typed[col] = pd.to_datetime(df_typed[col], errors='coerce')\n",
        "            elif 'Int' in str(dtype):\n",
        "                # Convert to nullable integer type.\n",
        "                df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce').astype(dtype)\n",
        "            else:\n",
        "                # Apply standard type conversion.\n",
        "                df_typed[col] = df_typed[col].astype(dtype, errors='ignore')\n",
        "\n",
        "    # --- Generate Data Quality Report ---\n",
        "    # Calculate the number of null values in each column after type enforcement.\n",
        "    null_counts: pd.Series = df_typed.isnull().sum()\n",
        "\n",
        "    # Calculate the percentage of null values for context.\n",
        "    null_percentages: pd.Series = (null_counts / len(df_typed)) * 100\n",
        "\n",
        "    # Combine counts and percentages into a report DataFrame.\n",
        "    quality_report: pd.DataFrame = pd.DataFrame({\n",
        "        \"null_count\": null_counts,\n",
        "        \"null_percentage\": null_percentages\n",
        "    })\n",
        "\n",
        "    # Filter the report to show only columns that contain null values.\n",
        "    quality_report = quality_report[quality_report[\"null_count\"] > 0]\n",
        "    quality_report.sort_values(by=\"null_count\", ascending=False, inplace=True)\n",
        "\n",
        "    # Define the path for the quality report file.\n",
        "    report_path: str = os.path.join(output_dir, \"data_quality_report.txt\")\n",
        "\n",
        "    # Write the report to a text file.\n",
        "    with open(report_path, \"w\") as f:\n",
        "        f.write(\"Data Quality Report: Null Counts After Type Enforcement\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\")\n",
        "        quality_report.to_string(f)\n",
        "\n",
        "    logging.info(f\"Data quality report saved to {report_path}\")\n",
        "\n",
        "    return df_typed\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Sanity checks on core financial fields (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _perform_sanity_checks(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Performs domain-specific sanity checks on core financial data fields.\n",
        "\n",
        "    This function validates that key financial metrics fall within plausible\n",
        "    ranges. It does not drop data but logs warnings for values that may\n",
        "    indicate data quality issues, such as negative shares outstanding or\n",
        "    extreme returns.\n",
        "\n",
        "    Args:\n",
        "        df: The type-enforced DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If a fundamental data constraint is violated (e.g.,\n",
        "                        negative shares).\n",
        "    \"\"\"\n",
        "    # Check 'prc': Absolute value of price should be positive for non-nulls.\n",
        "    # CRSP uses negative signs to denote bid-ask averages, not negative prices.\n",
        "    if (df[\"prc\"].dropna().abs() <= 0).any():\n",
        "        logging.warning(\"Found non-positive absolute values in 'prc' column.\")\n",
        "\n",
        "    # Check 'shrout': Shares outstanding must be non-negative.\n",
        "    if (df[\"shrout\"].dropna() < 0).any():\n",
        "        # This is a critical data error.\n",
        "        raise AssertionError(\"Negative values found in 'shrout' column.\")\n",
        "\n",
        "    # Check 'ret', 'retx', 'dlret' for plausible range [-1, 10].\n",
        "    # A return greater than 1000% in a month is highly suspect.\n",
        "    for col in [\"ret\", \"retx\", \"dlret\"]:\n",
        "        if col in df.columns:\n",
        "            extreme_returns = df[col].dropna().abs() > 10.0\n",
        "            if extreme_returns.any():\n",
        "                logging.warning(\n",
        "                    f\"Found {extreme_returns.sum()} returns with absolute value > 1000% in '{col}'.\"\n",
        "                )\n",
        "\n",
        "    # Check 'RF' for plausible monthly risk-free rate range in percent format.\n",
        "    if \"RF\" in df.columns:\n",
        "        rf_out_of_range = ~df[\"RF\"].dropna().between(-0.5, 2.0)\n",
        "        if rf_out_of_range.any():\n",
        "            logging.warning(\n",
        "                f\"Found {rf_out_of_range.sum()} 'RF' values outside [-0.5%, 2.0%].\"\n",
        "            )\n",
        "\n",
        "    # Verify that the index frequency is approximately monthly.\n",
        "    # We check if the modal day of the month is towards the end (28, 29, 30, 31).\n",
        "    if not df.index.empty:\n",
        "        modal_day = df.index.day.mode()\n",
        "        if not modal_day.empty and modal_day[0] < 28:\n",
        "            logging.warning(\n",
        "                f\"Modal day of month is {modal_day[0]}, which may indicate \"\n",
        "                f\"data is not consistently month-end.\"\n",
        "            )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def validate_and_enforce_schema(\n",
        "    df_raw: pd.DataFrame,\n",
        "    output_dir: str = \"data_interim\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation and schema enforcement for the raw input DataFrame.\n",
        "\n",
        "    This function serves as a robust pre-processing step to ensure the input data\n",
        "    conforms to the required structure and types for the ACT-Tensor pipeline. It\n",
        "    performs a sequence of validations:\n",
        "    1. Verifies the presence of all required columns and sets a proper DatetimeIndex.\n",
        "    2. Enforces a strict data type schema on all columns, coercing errors.\n",
        "    3. Conducts domain-specific sanity checks on key financial variables.\n",
        "    4. Checkpoints the validated DataFrame to a high-performance Parquet file.\n",
        "\n",
        "    Args:\n",
        "        df_raw: The raw, unprocessed pandas DataFrame from data loading.\n",
        "        output_dir: The directory to save the validated DataFrame and quality\n",
        "                    reports. Defaults to \"data_interim\".\n",
        "\n",
        "    Returns:\n",
        "        A cleaned, validated, and schema-enforced pandas DataFrame ready for\n",
        "        downstream processing.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If essential columns are missing.\n",
        "        ValueError: If the 'date' column is absent or invalid.\n",
        "        AssertionError: If critical data integrity checks fail (e.g., negative shares).\n",
        "    \"\"\"\n",
        "    # Define the complete list of columns required by the pipeline.\n",
        "    required_columns: List[str] = [\n",
        "        \"permno\", \"permco\", \"comnam\", \"shrcd\", \"exchcd\", \"siccd\", \"ncusip\",\n",
        "        \"prc\", \"ret\", \"retx\", \"shrout\", \"vol\", \"stko\", \"dlstdt\", \"dlstcd\",\n",
        "        \"dlret\", \"gvkey\", \"datadate\", \"fyearq\", \"fqtr\", \"fyr\", \"indfmt\",\n",
        "        \"consol\", \"popsrc\", \"datafmt\", \"tic\", \"cusip\", \"sich\", \"atq\", \"ltq\",\n",
        "        \"seqq\", \"ceqq\", \"saleq\", \"ibq\", \"oibdpq\", \"pstkq\", \"cogsq\", \"xsgaq\",\n",
        "        \"ppentq\", \"invtq\", \"lpermno\", \"linktype\", \"linkprim\", \"linkdt\",\n",
        "        \"linkenddt\", \"RF\", \"namedt\", \"nameenddt\", \"shrcls\"\n",
        "    ]\n",
        "    # Define a subset of columns that are absolutely critical.\n",
        "    core_columns: List[str] = [\n",
        "        \"permno\", \"date\", \"ret\", \"prc\", \"shrout\", \"gvkey\", \"RF\"\n",
        "    ]\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Validate columns and set index ---\n",
        "    df_indexed = _validate_columns_and_set_index(\n",
        "        df=df_raw,\n",
        "        required_columns=required_columns,\n",
        "        core_columns=core_columns\n",
        "    )\n",
        "    logging.info(\"Step 1/3: Column presence and index validation complete.\")\n",
        "\n",
        "    # --- Step 2: Enforce data types ---\n",
        "    df_typed = _enforce_data_types(df=df_indexed, output_dir=output_dir)\n",
        "    logging.info(\"Step 2/3: Data type enforcement complete.\")\n",
        "\n",
        "    # --- Step 3: Perform sanity checks ---\n",
        "    _perform_sanity_checks(df=df_typed)\n",
        "    logging.info(\"Step 3/3: Sanity checks on financial fields complete.\")\n",
        "\n",
        "    # --- Checkpointing ---\n",
        "    # Define the path for the checkpoint file.\n",
        "    checkpoint_path: str = os.path.join(output_dir, \"df_validated.parquet\")\n",
        "\n",
        "    # Save the validated DataFrame to Parquet format for efficient downstream I/O.\n",
        "    df_typed.to_parquet(checkpoint_path, compression=\"snappy\")\n",
        "    logging.info(f\"Validated DataFrame checkpointed to {checkpoint_path}\")\n",
        "\n",
        "    return df_typed\n"
      ],
      "metadata": {
        "id": "TfaIFU3wgMVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Universe Filtering, Deduplication, and Core Data Cleansing\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Universe Filtering, Deduplication, and Core Data Cleansing\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Temporal and universe filters (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _apply_universe_filters(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    log_path: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies temporal and security universe filters to the DataFrame.\n",
        "\n",
        "    This function restricts the data to the specified date range, common stocks\n",
        "    on major US exchanges, and logs the impact of each filter.\n",
        "\n",
        "    Args:\n",
        "        df: The validated DataFrame from the previous task.\n",
        "        config: The study's configuration dictionary.\n",
        "        log_path: Path to the file for logging filter counts.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame containing only the securities within the defined universe\n",
        "        and time period.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid side effects on the original DataFrame.\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # Extract the relevant study scope parameters.\n",
        "    scope_config = config[\"study_scope\"]\n",
        "\n",
        "    # Initialize a list to store log messages.\n",
        "    log_messages = [\"Universe Filtering Log\\n\" + \"=\"*40]\n",
        "    log_messages.append(f\"Initial row count: {len(df_filtered)}\")\n",
        "\n",
        "    # --- 1. Temporal Filter ---\n",
        "    # Restrict DataFrame to the specified date range.\n",
        "    start_date = pd.to_datetime(scope_config[\"start_date\"])\n",
        "    end_date = pd.to_datetime(scope_config[\"end_date\"])\n",
        "    df_filtered = df_filtered.loc[start_date:end_date]\n",
        "    log_messages.append(\n",
        "        f\"After temporal filter ({start_date.date()} to {end_date.date()}): \"\n",
        "        f\"{len(df_filtered)} rows\"\n",
        "    )\n",
        "\n",
        "    # --- 2. Common Stock Filter (shrcd) ---\n",
        "    # Retain only rows with share codes corresponding to common stocks.\n",
        "    share_code_filter = scope_config[\"share_code_filter\"]\n",
        "    df_filtered = df_filtered[df_filtered[\"shrcd\"].isin(share_code_filter)]\n",
        "    log_messages.append(\n",
        "        f\"After share code filter (shrcd in {share_code_filter}): \"\n",
        "        f\"{len(df_filtered)} rows\"\n",
        "    )\n",
        "\n",
        "    # --- 3. Exchange Filter (exchcd) ---\n",
        "    # Retain only rows for securities listed on specified exchanges.\n",
        "    exchange_filter = scope_config[\"include_exchanges\"]\n",
        "    df_filtered = df_filtered[df_filtered[\"exchcd\"].isin(exchange_filter)]\n",
        "    log_messages.append(\n",
        "        f\"After exchange filter (exchcd in {exchange_filter}): \"\n",
        "        f\"{len(df_filtered)} rows\"\n",
        "    )\n",
        "\n",
        "    # --- 4. Non-Exchange Listing Exclusion ---\n",
        "    # If configured, explicitly drop rows where exchange code is missing.\n",
        "    if scope_config.get(\"exclude_non_exchange_listings\", True):\n",
        "        pre_drop_len = len(df_filtered)\n",
        "        df_filtered.dropna(subset=[\"exchcd\"], inplace=True)\n",
        "        post_drop_len = len(df_filtered)\n",
        "        if pre_drop_len > post_drop_len:\n",
        "            log_messages.append(\n",
        "                f\"After dropping rows with null exchcd: {post_drop_len} rows\"\n",
        "            )\n",
        "\n",
        "    # Write all log messages to the specified file.\n",
        "    with open(log_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(log_messages))\n",
        "\n",
        "    logging.info(f\"Universe filter log saved to {log_path}\")\n",
        "\n",
        "    return df_filtered\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Deduplicate by (date, permno) (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _deduplicate_observations(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    log_path: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Deduplicates observations by (date, permno) using a deterministic tie-breaker.\n",
        "\n",
        "    When multiple entries exist for the same firm-month (e.g., from CRSP-Compustat\n",
        "    link table), this function applies a multi-level sorting logic to select the\n",
        "    single best observation.\n",
        "\n",
        "    Args:\n",
        "        df: The filtered DataFrame.\n",
        "        config: The study's configuration dictionary.\n",
        "        log_path: Path to the file for logging deduplication counts.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with a unique (date, permno) MultiIndex.\n",
        "    \"\"\"\n",
        "    # Create a copy to work on.\n",
        "    df_dedup = df.copy()\n",
        "\n",
        "    # Reset index to make 'date' a column for sorting and grouping.\n",
        "    df_dedup.reset_index(inplace=True)\n",
        "\n",
        "    initial_rows = len(df_dedup)\n",
        "\n",
        "    # Define the tie-breaking hierarchy.\n",
        "    # 1. Prefer primary link ('P') over secondary ('C').\n",
        "    # 2. Prefer allowed link types ('LC', 'LU').\n",
        "    # 3. Prefer the most recent link start date.\n",
        "    # 4. As a final deterministic tie-breaker, use the gvkey.\n",
        "    link_policy = config[\"linking_policy\"]\n",
        "    df_dedup['linkprim_ord'] = df_dedup['linkprim'].apply(\n",
        "        lambda x: 0 if x == 'P' else 1 if x == 'C' else 2\n",
        "    )\n",
        "    df_dedup['linktype_ord'] = df_dedup['linktype'].apply(\n",
        "        lambda x: 0 if x in link_policy[\"allow_linktype\"] else 1\n",
        "    )\n",
        "\n",
        "    # Sort the DataFrame according to the full tie-breaking logic.\n",
        "    sort_order = [\n",
        "        'date', 'permno', 'linkprim_ord', 'linktype_ord', 'linkdt', 'gvkey'\n",
        "    ]\n",
        "    ascending_order = [True, True, True, True, False, True] # False for linkdt (latest)\n",
        "    df_dedup.sort_values(\n",
        "        by=sort_order,\n",
        "        ascending=ascending_order,\n",
        "        inplace=True,\n",
        "        na_position='last'\n",
        "    )\n",
        "\n",
        "    # Drop duplicates, keeping the first entry after sorting.\n",
        "    df_dedup.drop_duplicates(subset=['date', 'permno'], keep='first', inplace=True)\n",
        "\n",
        "    # Clean up temporary sorting columns.\n",
        "    df_dedup.drop(columns=['linkprim_ord', 'linktype_ord'], inplace=True)\n",
        "\n",
        "    final_rows = len(df_dedup)\n",
        "    duplicates_removed = initial_rows - final_rows\n",
        "\n",
        "    # Log the number of duplicates removed.\n",
        "    with open(log_path, \"w\") as f:\n",
        "        f.write(\"Deduplication Log\\n\" + \"=\"*40 + \"\\n\")\n",
        "        f.write(f\"Rows before deduplication: {initial_rows}\\n\")\n",
        "        f.write(f\"Rows after deduplication: {final_rows}\\n\")\n",
        "        f.write(f\"Duplicates removed: {duplicates_removed}\\n\")\n",
        "\n",
        "    logging.info(f\"Deduplication log saved to {log_path}\")\n",
        "\n",
        "    # Set and sort the final (date, permno) MultiIndex.\n",
        "    df_dedup.set_index(['date', 'permno'], inplace=True)\n",
        "    df_dedup.sort_index(inplace=True)\n",
        "\n",
        "    return df_dedup\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Outlier removal and delisting returns (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _handle_outliers_and_delistings(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    log_path: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes extreme outliers and incorporates CRSP delisting returns.\n",
        "\n",
        "    This function applies domain-specific filters to remove penny stocks,\n",
        "    micro-caps, and observations with extreme returns. It then correctly\n",
        "    compounds delisting returns with regular monthly returns where applicable.\n",
        "\n",
        "    Args:\n",
        "        df: The deduplicated DataFrame with a (date, permno) MultiIndex.\n",
        "        config: The study's configuration dictionary.\n",
        "        log_path: Path to the file for logging outlier removal counts.\n",
        "\n",
        "    Returns:\n",
        "        A new, fully cleansed DataFrame.\n",
        "    \"\"\"\n",
        "    # Create a copy to work on.\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    scope_config = config[\"study_scope\"]\n",
        "    log_messages = [\"Outlier Removal & Cleansing Log\\n\" + \"=\"*50]\n",
        "    initial_rows = len(df_clean)\n",
        "    log_messages.append(f\"Rows before outlier removal: {initial_rows}\")\n",
        "\n",
        "    # --- 1. Extreme Outlier Removal ---\n",
        "    if scope_config.get(\"drop_extreme_outliers\", True):\n",
        "        # Define outlier thresholds.\n",
        "        thresholds = {\n",
        "            \"min_price\": 1.0,\n",
        "            \"max_price\": 10000.0,\n",
        "            \"max_abs_ret\": 2.0,\n",
        "            \"min_shrout\": 10.0, # In thousands of shares\n",
        "            \"min_mktcap_proxy\": 1e3 # In thousands of USD ($1 million)\n",
        "        }\n",
        "        log_messages.append(f\"Applying outlier thresholds: {thresholds}\")\n",
        "\n",
        "        # Create boolean masks for each condition.\n",
        "        price_mask = df_clean[\"prc\"].abs().between(\n",
        "            thresholds[\"min_price\"], thresholds[\"max_price\"]\n",
        "        )\n",
        "        ret_mask = df_clean[\"ret\"].abs() <= thresholds[\"max_abs_ret\"]\n",
        "        shrout_mask = df_clean[\"shrout\"] >= thresholds[\"min_shrout\"]\n",
        "\n",
        "        # Compute market cap proxy for filtering.\n",
        "        mktcap_proxy = df_clean[\"prc\"].abs() * df_clean[\"shrout\"]\n",
        "        mktcap_mask = mktcap_proxy >= thresholds[\"min_mktcap_proxy\"]\n",
        "\n",
        "        # Combine all masks. Keep rows where all conditions are True.\n",
        "        # NaNs in any of these columns will result in False, thus dropping the row.\n",
        "        combined_mask = price_mask & ret_mask & shrout_mask & mktcap_mask\n",
        "        df_clean = df_clean[combined_mask]\n",
        "\n",
        "        final_rows = len(df_clean)\n",
        "        log_messages.append(f\"Rows after outlier removal: {final_rows}\")\n",
        "        log_messages.append(f\"Rows removed: {initial_rows - final_rows}\")\n",
        "\n",
        "    # --- 2. Delisting Return Incorporation ---\n",
        "    if scope_config.get(\"use_delisting_returns\", True):\n",
        "        # Identify rows where both regular and delisting returns are available.\n",
        "        both_valid = df_clean[\"ret\"].notna() & df_clean[\"dlret\"].notna()\n",
        "\n",
        "        # Formula: r_combined = (1 + r_t) * (1 + r_delist) - 1\n",
        "        df_clean.loc[both_valid, \"ret\"] = (\n",
        "            (1 + df_clean.loc[both_valid, \"ret\"]) *\n",
        "            (1 + df_clean.loc[both_valid, \"dlret\"])\n",
        "        ) - 1\n",
        "\n",
        "        # Identify rows where only delisting return is available.\n",
        "        only_dlret = df_clean[\"ret\"].isna() & df_clean[\"dlret\"].notna()\n",
        "        df_clean.loc[only_dlret, \"ret\"] = df_clean.loc[only_dlret, \"dlret\"]\n",
        "\n",
        "        log_messages.append(\n",
        "            f\"Incorporated delisting returns for {both_valid.sum()} observations \"\n",
        "            f\"and filled {only_dlret.sum()} missing returns with dlret.\"\n",
        "        )\n",
        "\n",
        "    # Write log messages to file.\n",
        "    with open(log_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(log_messages))\n",
        "\n",
        "    logging.info(f\"Outlier and delisting log saved to {log_path}\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def cleanse_and_filter_universe(\n",
        "    df_validated: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_interim\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the universe definition and core data cleansing pipeline.\n",
        "\n",
        "    This function takes a schema-validated DataFrame and applies a series of\n",
        "    rigorous filters and transformations to define the final analytical sample:\n",
        "    1. Applies temporal and security-type filters (e.g., date range, common stock).\n",
        "    2. Deduplicates firm-month observations using a deterministic tie-breaker.\n",
        "    3. Removes extreme outliers and incorporates CRSP delisting returns.\n",
        "    4. Checkpoints the final cleansed DataFrame to a Parquet file.\n",
        "\n",
        "    Args:\n",
        "        df_validated: The DataFrame after schema validation (from Task 2).\n",
        "        config: The `act_tensor_config` dictionary.\n",
        "        output_dir: Directory to save the cleansed DataFrame and log files.\n",
        "                    Defaults to \"data_interim\".\n",
        "\n",
        "    Returns:\n",
        "        A fully cleansed and filtered pandas DataFrame with a unique\n",
        "        (date, permno) MultiIndex, ready for characteristic construction.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory for logs and checkpoints exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Apply temporal and universe filters ---\n",
        "    universe_log_path = os.path.join(output_dir, \"universe_filter_log.txt\")\n",
        "    df_filtered = _apply_universe_filters(\n",
        "        df=df_validated, config=config, log_path=universe_log_path\n",
        "    )\n",
        "    logging.info(\"Step 1/3: Universe filtering complete.\")\n",
        "\n",
        "    # --- Step 2: Deduplicate by (date, permno) and resolve conflicts ---\n",
        "    dedup_log_path = os.path.join(output_dir, \"deduplication_log.txt\")\n",
        "    df_deduplicated = _deduplicate_observations(\n",
        "        df=df_filtered, config=config, log_path=dedup_log_path\n",
        "    )\n",
        "    logging.info(\"Step 2/3: Deduplication complete.\")\n",
        "\n",
        "    # --- Step 3: Handle outliers and delisting returns ---\n",
        "    outlier_log_path = os.path.join(output_dir, \"outlier_removal_log.txt\")\n",
        "    df_cleansed = _handle_outliers_and_delistings(\n",
        "        df=df_deduplicated, config=config, log_path=outlier_log_path\n",
        "    )\n",
        "    logging.info(\"Step 3/3: Outlier removal and delisting return handling complete.\")\n",
        "\n",
        "    # --- Checkpointing ---\n",
        "    # Define the path for the final cleansed data checkpoint.\n",
        "    checkpoint_path = os.path.join(output_dir, \"df_cleansed.parquet\")\n",
        "\n",
        "    # Save the cleansed DataFrame to Parquet format.\n",
        "    df_cleansed.to_parquet(checkpoint_path, compression=\"snappy\")\n",
        "    logging.info(f\"Cleansed DataFrame checkpointed to {checkpoint_path}\")\n",
        "\n",
        "    return df_cleansed\n"
      ],
      "metadata": {
        "id": "zu3B80tnhEnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Construct the 45-Characteristic Library with Definitions and Timing Conventions\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Construct the 45-Characteristic Library and Compute Values\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Document characteristic definitions (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _create_and_save_char_library(\n",
        "    output_path: str\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Defines and saves the characteristic library metadata to a JSON file.\n",
        "\n",
        "    This function creates a structured definition for each characteristic, including\n",
        "    its name, formula, input variables, and type. For this implementation, a\n",
        "    representative subset of characteristics is defined. A full implementation\n",
        "    would list all 45. This metadata drives the computation process.\n",
        "\n",
        "    Args:\n",
        "        output_path: The file path to save the JSON library (e.g., 'config/char_library.json').\n",
        "\n",
        "    Returns:\n",
        "        The characteristic library as a list of dictionaries.\n",
        "    \"\"\"\n",
        "    # Define a representative subset of the 45 characteristics.\n",
        "    # A full implementation would source all 45 from a paper like Gu, Kelly, Xiu (2020).\n",
        "    char_library = [\n",
        "        {\n",
        "            \"char_id\": 1,\n",
        "            \"char_name\": \"market_equity\",\n",
        "            \"category\": \"size\",\n",
        "            \"formula\": r\"ME_t = |prc_t| \\times shrout_t\",\n",
        "            \"inputs\": [\"prc\", \"shrout\"],\n",
        "            \"frequency\": \"monthly\"\n",
        "        },\n",
        "        {\n",
        "            \"char_id\": 2,\n",
        "            \"char_name\": \"book_to_market\",\n",
        "            \"category\": \"value\",\n",
        "            \"formula\": r\"BM_t = \\frac{seqq_{q(t-6m)}}{ME_t}\",\n",
        "            \"inputs\": [\"seqq\", \"market_equity\"],\n",
        "            \"frequency\": \"hybrid\",\n",
        "            \"reporting_lag_months\": 6\n",
        "        },\n",
        "        {\n",
        "            \"char_id\": 3,\n",
        "            \"char_name\": \"momentum_12_2\",\n",
        "            \"category\": \"momentum\",\n",
        "            \"formula\": r\"MOM_t = \\prod_{s=t-12}^{t-2}(1 + r_s) - 1\",\n",
        "            \"inputs\": [\"ret\"],\n",
        "            \"frequency\": \"monthly\"\n",
        "        },\n",
        "        {\n",
        "            \"char_id\": 4,\n",
        "            \"char_name\": \"return_on_equity\",\n",
        "            \"category\": \"profitability\",\n",
        "            \"formula\": r\"ROE_t = \\frac{ibq_{q(t-6m)}}{seqq_{q(t-6m)}}\",\n",
        "            \"inputs\": [\"ibq\", \"seqq\"],\n",
        "            \"frequency\": \"quarterly\",\n",
        "            \"reporting_lag_months\": 6\n",
        "        },\n",
        "        {\n",
        "            \"char_id\": 5,\n",
        "            \"char_name\": \"asset_growth\",\n",
        "            \"category\": \"investment\",\n",
        "            \"formula\": r\"AG_t = \\frac{atq_{q(t-6m)}}{atq_{q(t-18m)}} - 1\",\n",
        "            \"inputs\": [\"atq\"],\n",
        "            \"frequency\": \"quarterly\",\n",
        "            \"reporting_lag_months\": 6\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Ensure the directory for the output path exists.\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    # Write the library to the specified JSON file.\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(char_library, f, indent=4)\n",
        "\n",
        "    logging.info(f\"Characteristic library saved to {output_path}\")\n",
        "    return char_library\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Align quarterly accounting data (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _align_accounting_data(\n",
        "    df_clean: pd.DataFrame,\n",
        "    lag_months: int = 6\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aligns quarterly Compustat data to a monthly frequency with a reporting lag.\n",
        "\n",
        "    This function prevents look-ahead bias by ensuring that for any given month `t`,\n",
        "    the accounting data used is from a fiscal quarter that was publicly available,\n",
        "    i.e., with a `datadate` at least `lag_months` prior to `t`.\n",
        "\n",
        "    Args:\n",
        "        df_clean: The cleansed DataFrame with a (date, permno) MultiIndex.\n",
        "        lag_months: The number of months to lag accounting data.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the same monthly index, now including correctly lagged\n",
        "        quarterly accounting columns.\n",
        "    \"\"\"\n",
        "    # Reset index to use 'date' and 'permno' as columns for merging.\n",
        "    monthly_df = df_clean.reset_index()\n",
        "\n",
        "    # Define the list of quarterly accounting columns to be lagged.\n",
        "    accounting_cols = [\n",
        "        'datadate', 'atq', 'ltq', 'seqq', 'ceqq', 'saleq', 'ibq',\n",
        "        'oibdpq', 'pstkq', 'cogsq', 'xsgaq', 'ppentq', 'invtq'\n",
        "    ]\n",
        "\n",
        "    # Create a DataFrame containing only the necessary accounting data.\n",
        "    # Drop duplicates to ensure one record per firm-quarter.\n",
        "    quarterly_df = monthly_df[['permno', 'datadate'] + accounting_cols[1:]].copy()\n",
        "    quarterly_df['datadate'] = pd.to_datetime(quarterly_df['datadate'], errors='coerce')\n",
        "    quarterly_df.dropna(subset=['permno', 'datadate'], inplace=True)\n",
        "    quarterly_df.drop_duplicates(subset=['permno', 'datadate'], inplace=True)\n",
        "    quarterly_df.sort_values(by=['permno', 'datadate'], inplace=True)\n",
        "\n",
        "    # Create the monthly frame for the merge, containing only identifiers and dates.\n",
        "    monthly_grid = monthly_df[['date', 'permno']].copy()\n",
        "    monthly_grid.sort_values(by=['permno', 'date'], inplace=True)\n",
        "\n",
        "    # Calculate the cutoff date for each month to enforce the reporting lag.\n",
        "    monthly_grid['cutoff_date'] = monthly_grid['date'] - pd.DateOffset(months=lag_months)\n",
        "\n",
        "    # Perform the as-of merge. For each firm-month, this finds the most recent\n",
        "    # accounting data (from quarterly_df) where datadate <= cutoff_date.\n",
        "    df_aligned = pd.merge_asof(\n",
        "        left=monthly_grid,\n",
        "        right=quarterly_df,\n",
        "        left_on='cutoff_date',\n",
        "        right_on='datadate',\n",
        "        by='permno',\n",
        "        direction='backward' # Find the last observation in `right` at or before `left`'s date\n",
        "    )\n",
        "\n",
        "    # Merge the aligned accounting data back to the main monthly DataFrame.\n",
        "    df_final = pd.merge(\n",
        "        monthly_df,\n",
        "        df_aligned.drop(columns=['cutoff_date']),\n",
        "        on=['date', 'permno'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Set the (date, permno) MultiIndex back.\n",
        "    df_final.set_index(['date', 'permno'], inplace=True)\n",
        "    df_final.sort_index(inplace=True)\n",
        "\n",
        "    logging.info(f\"Accounting data aligned with a {lag_months}-month lag.\")\n",
        "    return df_final\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Compute characteristics (Helper Functions by Category)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_characteristics(\n",
        "    df_aligned: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes a suite of firm characteristics using vectorized operations.\n",
        "\n",
        "    This function acts as a factory, calculating characteristics based on their\n",
        "    definitions. It is designed to be modular, with separate logic for different\n",
        "    categories of signals (size, value, momentum, etc.).\n",
        "\n",
        "    Args:\n",
        "        df_aligned: DataFrame containing both monthly market data and lagged\n",
        "                    quarterly accounting data.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame containing the original data plus new columns for each\n",
        "        computed characteristic.\n",
        "    \"\"\"\n",
        "    # Create a copy to store the new characteristic columns.\n",
        "    df_chars = df_aligned.copy()\n",
        "\n",
        "    # --- Size Characteristics ---\n",
        "    # Market Equity: |Price| * Shares Outstanding.\n",
        "    df_chars['market_equity'] = df_chars['prc'].abs() * df_chars['shrout']\n",
        "\n",
        "    # --- Value Characteristics ---\n",
        "    # Book-to-Market Ratio: Lagged Shareholders' Equity / Market Equity.\n",
        "    # Replace non-positive book equity with NaN to avoid meaningless ratios.\n",
        "    book_equity = df_chars['seqq'].where(df_chars['seqq'] > 0)\n",
        "    df_chars['book_to_market'] = book_equity / df_chars['market_equity']\n",
        "\n",
        "    # --- Momentum Characteristics ---\n",
        "    # Momentum (12-2): Compounded return from month t-12 to t-2.\n",
        "    # Group by firm (permno) to compute rolling returns correctly.\n",
        "    # Add 1 to returns to calculate cumulative product.\n",
        "    ret_plus_one = df_chars['ret'].add(1)\n",
        "    # The rolling window of 11 months captures returns from t-12 to t-2.\n",
        "    # We then shift by 1 to align the calculation correctly at time t.\n",
        "    # The product of (1+ret) over 11 months is calculated.\n",
        "    # We subtract 1 to get the total compounded return.\n",
        "    # min_periods=11 ensures we only calculate if we have enough data.\n",
        "    df_chars['momentum_12_2'] = (\n",
        "        ret_plus_one.groupby('permno')\n",
        "        .rolling(window=11, min_periods=11)\n",
        "        .apply(np.prod, raw=True)\n",
        "        .reset_index(level=0, drop=True)\n",
        "        .groupby('permno')\n",
        "        .shift(1)\n",
        "    ) - 1\n",
        "\n",
        "    # --- Profitability Characteristics ---\n",
        "    # Return on Equity: Lagged Income Before Extraordinary Items / Lagged Equity.\n",
        "    df_chars['return_on_equity'] = df_chars['ibq'] / book_equity\n",
        "\n",
        "    # --- Investment Characteristics ---\n",
        "    # Asset Growth: Growth in total assets over the prior year.\n",
        "    # We need atq from 12 months ago (which corresponds to datadate of t-18m).\n",
        "    # A groupby().diff(12) on the monthly aligned data approximates this.\n",
        "    atq_lag_12m = df_chars.groupby('permno')['atq'].shift(12)\n",
        "    # Replace non-positive lagged assets with NaN.\n",
        "    atq_lag_12m = atq_lag_12m.where(atq_lag_12m > 0)\n",
        "    df_chars['asset_growth'] = (df_chars['atq'] / atq_lag_12m) - 1\n",
        "\n",
        "    logging.info(\"Computed representative set of characteristics.\")\n",
        "    return df_chars\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_characteristics(\n",
        "    df_cleansed: pd.DataFrame,\n",
        "    config_dir: str = \"config\",\n",
        "    output_dir: str = \"data_interim\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the firm characteristic library.\n",
        "\n",
        "    This function executes the full characteristic construction pipeline:\n",
        "    1. Defines and saves a machine-readable library of characteristic definitions.\n",
        "    2. Aligns quarterly accounting data to a monthly frequency, rigorously\n",
        "       enforcing a reporting lag to prevent look-ahead bias.\n",
        "    3. Computes all defined characteristics using efficient, vectorized operations.\n",
        "    4. Logs the missingness rate for each characteristic as a diagnostic.\n",
        "    5. Checkpoints the final DataFrame with raw characteristics to a Parquet file.\n",
        "\n",
        "    Args:\n",
        "        df_cleansed: The fully cleansed and filtered DataFrame from Task 3.\n",
        "        config_dir: Directory to save the characteristic library definition.\n",
        "        output_dir: Directory to save the final DataFrame and logs.\n",
        "\n",
        "    Returns:\n",
        "        A wide DataFrame with a (date, permno) MultiIndex, containing all\n",
        "        original data plus the newly computed raw characteristic columns.\n",
        "    \"\"\"\n",
        "    # Ensure output directories exist.\n",
        "    os.makedirs(config_dir, exist_ok=True)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Define and save the characteristic library ---\n",
        "    char_lib_path = os.path.join(config_dir, \"char_library_definitions.json\")\n",
        "    char_library = _create_and_save_char_library(output_path=char_lib_path)\n",
        "    logging.info(\"Step 1/3: Characteristic library defined and saved.\")\n",
        "\n",
        "    # --- Step 2: Align accounting data to prevent look-ahead bias ---\n",
        "    # Use a default lag of 6 months, standard in academic research.\n",
        "    # This could be parameterized from the main config if needed.\n",
        "    df_aligned = _align_accounting_data(df_cleansed, lag_months=6)\n",
        "    logging.info(\"Step 2/3: Accounting data alignment complete.\")\n",
        "\n",
        "    # --- Step 3: Compute all defined characteristics ---\n",
        "    df_with_chars = _compute_characteristics(df_aligned)\n",
        "    logging.info(\"Step 3/3: Characteristic computation complete.\")\n",
        "\n",
        "    # --- Finalization and Reporting ---\n",
        "    # Extract only the characteristic columns for reporting and downstream use.\n",
        "    char_names = [c['char_name'] for c in char_library]\n",
        "    df_characteristics_raw = df_with_chars[char_names]\n",
        "\n",
        "    # Generate and save a missingness report for the computed characteristics.\n",
        "    missing_report = (\n",
        "        df_characteristics_raw.isnull().sum() / len(df_characteristics_raw)\n",
        "    ).sort_values(ascending=False).to_frame('missing_fraction')\n",
        "\n",
        "    report_path = os.path.join(output_dir, \"char_missingness_summary.csv\")\n",
        "    missing_report.to_csv(report_path)\n",
        "    logging.info(f\"Characteristic missingness report saved to {report_path}\")\n",
        "\n",
        "    # Checkpoint the final DataFrame with raw characteristics.\n",
        "    checkpoint_path = os.path.join(output_dir, \"df_characteristics_raw.parquet\")\n",
        "    df_characteristics_raw.to_parquet(checkpoint_path, compression=\"snappy\")\n",
        "    logging.info(f\"Raw characteristics DataFrame checkpointed to {checkpoint_path}\")\n",
        "\n",
        "    return df_characteristics_raw\n"
      ],
      "metadata": {
        "id": "mTf6A7TbhutM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Cross-Sectional Rank Normalization by Month\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Cross-Sectional Rank Normalization by Month\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def rank_normalize_characteristics(\n",
        "    df_characteristics_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_interim\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs cross-sectional rank normalization on raw characteristic data.\n",
        "\n",
        "    This function transforms each characteristic to be on a comparable scale,\n",
        "    which is a crucial preprocessing step for many financial models, including\n",
        "    tensor decomposition. The process for each characteristic is:\n",
        "    1. For each month, rank firms based on the characteristic value. Missing\n",
        "       values are ignored, and ties are handled by assigning the average rank.\n",
        "    2. Linearly rescale these ranks to a fixed interval of [-0.5, 0.5].\n",
        "\n",
        "    This procedure makes the resulting data robust to outliers and removes\n",
        "    arbitrary differences in scale across characteristics.\n",
        "\n",
        "    Args:\n",
        "        df_characteristics_raw: A DataFrame with a (date, permno) MultiIndex\n",
        "                                containing the raw, computed characteristic values.\n",
        "        config: The study's configuration dictionary, containing normalization\n",
        "                parameters.\n",
        "        output_dir: Directory to save the normalized DataFrame and summary logs.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame of the same shape, with characteristic values replaced\n",
        "        by their cross-sectionally normalized ranks.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the input is a pandas DataFrame.\n",
        "    if not isinstance(df_characteristics_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df_characteristics_raw' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Verify that the DataFrame has the expected MultiIndex.\n",
        "    if not isinstance(df_characteristics_raw.index, pd.MultiIndex) or \\\n",
        "       df_characteristics_raw.index.names != ['date', 'permno']:\n",
        "        raise ValueError(\"Input DataFrame must have a MultiIndex named ('date', 'permno').\")\n",
        "\n",
        "    # Create a copy to ensure the original DataFrame is not modified.\n",
        "    df_normalized = df_characteristics_raw.copy()\n",
        "\n",
        "    # Extract normalization parameters from the configuration.\n",
        "    norm_config = config[\"tensor_policies\"][\"rank_normalization\"]\n",
        "    rank_method = norm_config[\"rank_method\"]\n",
        "    target_interval = norm_config[\"scale_to_interval\"]\n",
        "\n",
        "    # --- Step 1: Rank each characteristic within each month ---\n",
        "    # Group the DataFrame by date to perform cross-sectional operations.\n",
        "    # The `level='date'` argument targets the 'date' level of the MultiIndex.\n",
        "    grouped_by_date = df_normalized.groupby(level='date')\n",
        "\n",
        "    # Apply the rank transformation to all characteristic columns.\n",
        "    # `na_option='keep'` ensures that existing NaN values are preserved.\n",
        "    # `method='average'` handles ties as specified.\n",
        "    df_ranks = grouped_by_date.rank(method=rank_method, na_option='keep', pct=False)\n",
        "    logging.info(\"Step 1/3: Cross-sectional ranking complete.\")\n",
        "\n",
        "    # --- Step 2: Recenter and rescale ranks to the target interval ---\n",
        "    # First, calculate N_t_ell, the number of non-null observations for each\n",
        "    # characteristic in each month. `transform('count')` is crucial as it\n",
        "    # broadcasts the group-wise count back to the original shape.\n",
        "    n_t_ell = grouped_by_date.transform('count')\n",
        "\n",
        "    # Handle the edge case where N_t_ell is 1. The denominator becomes 0.\n",
        "    # In this case, the normalized value should be 0 (the midpoint of [-0.5, 0.5]).\n",
        "    # We create a mask to identify where the denominator is non-zero.\n",
        "    denominator = n_t_ell - 1\n",
        "\n",
        "    # The formula for rescaling is: x_tilde = (rank - 1) / (N - 1) - 0.5\n",
        "    # This maps rank 1 to -0.5 and rank N to +0.5.\n",
        "    # We use np.where to safely handle the division-by-zero case.\n",
        "    df_normalized = np.where(\n",
        "        denominator > 0,\n",
        "        (df_ranks - 1) / denominator - 0.5, # Apply formula where N > 1\n",
        "        0.0                                 # Set to 0 where N <= 1\n",
        "    )\n",
        "\n",
        "    # The result of np.where is a NumPy array, so we convert it back to a\n",
        "    # DataFrame, preserving the original index and columns.\n",
        "    df_normalized = pd.DataFrame(\n",
        "        df_normalized,\n",
        "        index=df_characteristics_raw.index,\n",
        "        columns=df_characteristics_raw.columns\n",
        "    )\n",
        "\n",
        "    # The np.where operation can lose the original NaNs, so we must re-mask them.\n",
        "    df_normalized[df_characteristics_raw.isnull()] = np.nan\n",
        "    logging.info(\"Step 2/3: Rank rescaling to [-0.5, 0.5] complete.\")\n",
        "\n",
        "    # --- Step 3: Assemble, verify, and checkpoint the normalized DataFrame ---\n",
        "    # Compute summary statistics for validation.\n",
        "    summary_stats = df_normalized.describe().transpose()\n",
        "\n",
        "    # Perform sanity checks on the output.\n",
        "    # The minimum value should be approximately -0.5.\n",
        "    if not np.allclose(summary_stats['min'].min(), target_interval[0], atol=1e-9):\n",
        "        logging.warning(f\"Minimum normalized value is {summary_stats['min'].min()}, expected {target_interval[0]}.\")\n",
        "\n",
        "    # The maximum value should be approximately 0.5.\n",
        "    if not np.allclose(summary_stats['max'].max(), target_interval[1], atol=1e-9):\n",
        "        logging.warning(f\"Maximum normalized value is {summary_stats['max'].max()}, expected {target_interval[1]}.\")\n",
        "\n",
        "    # The mean should be close to 0.\n",
        "    if not (summary_stats['mean'].abs() < 0.1).all():\n",
        "        logging.warning(\"Mean of some normalized characteristics is far from 0.\")\n",
        "\n",
        "    # Log the summary statistics to a file for auditing.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    summary_log_path = os.path.join(output_dir, \"normalization_summary_stats.csv\")\n",
        "    summary_stats.to_csv(summary_log_path)\n",
        "    logging.info(f\"Step 3/3: Verification complete. Summary stats saved to {summary_log_path}.\")\n",
        "\n",
        "    # Checkpoint the final normalized DataFrame to a Parquet file.\n",
        "    checkpoint_path = os.path.join(output_dir, \"df_characteristics_normalized.parquet\")\n",
        "    df_normalized.to_parquet(checkpoint_path, compression=\"snappy\")\n",
        "    logging.info(f\"Normalized characteristics DataFrame checkpointed to {checkpoint_path}\")\n",
        "\n",
        "    return df_normalized\n"
      ],
      "metadata": {
        "id": "ZlmSrm7aia_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Form the 3-Mode Tensor $\\mathcal{X} \\in \\mathbb{R}^{T \\times N \\times L}$ and Observed-Index Set $\\Omega$\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Form the 3-Mode Tensor and Observed-Index Set\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Construct firm and time index mappings (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _create_index_mappings(\n",
        "    df_normalized: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> Tuple[Dict[pd.Timestamp, int], Dict[int, int]]:\n",
        "    \"\"\"\n",
        "    Creates and saves mappings from original identifiers to integer indices.\n",
        "\n",
        "    This function extracts the unique, sorted dates and permnos from the\n",
        "    DataFrame's MultiIndex and creates dictionaries that map each identifier\n",
        "    to a zero-based integer. These mappings are essential for constructing the\n",
        "    tensor and for later interpreting its dimensions.\n",
        "\n",
        "    Args:\n",
        "        df_normalized: The normalized characteristics DataFrame.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the mapping files.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two dictionaries: (time_to_idx, firm_to_idx).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of unique months in the data does not match\n",
        "                    the expected number from the configuration.\n",
        "    \"\"\"\n",
        "    # --- Time Dimension Mapping ---\n",
        "    # Extract unique dates from the index, sort them, and get the total count T.\n",
        "    unique_dates = sorted(df_normalized.index.get_level_values('date').unique())\n",
        "    T = len(unique_dates)\n",
        "\n",
        "    # Validate that the number of months matches the configuration's expectation.\n",
        "    t_expected = config[\"tensor_policies\"][\"T_months_expected\"]\n",
        "    if T != t_expected:\n",
        "        raise ValueError(\n",
        "            f\"Number of unique months in data ({T}) does not match \"\n",
        "            f\"config 'T_months_expected' ({t_expected}).\"\n",
        "        )\n",
        "\n",
        "    # Create the mapping from Timestamp to integer index.\n",
        "    time_to_idx = {date: i for i, date in enumerate(unique_dates)}\n",
        "\n",
        "    # Save the time mapping. Timestamps are converted to ISO format strings for JSON compatibility.\n",
        "    time_map_path = os.path.join(output_dir, \"time_to_idx.json\")\n",
        "    with open(time_map_path, 'w') as f:\n",
        "        json.dump({d.isoformat(): i for d, i in time_to_idx.items()}, f, indent=4)\n",
        "    logging.info(f\"Time-to-index mapping saved to {time_map_path}\")\n",
        "\n",
        "    # --- Firm Dimension Mapping ---\n",
        "    # Extract unique permnos, sort them, and get the total count N.\n",
        "    unique_permnos = sorted(df_normalized.index.get_level_values('permno').unique())\n",
        "\n",
        "    # Create the mapping from permno to integer index.\n",
        "    firm_to_idx = {permno: i for i, permno in enumerate(unique_permnos)}\n",
        "\n",
        "    # Save the firm mapping.\n",
        "    firm_map_path = os.path.join(output_dir, \"firm_to_idx.json\")\n",
        "    with open(firm_map_path, 'w') as f:\n",
        "        json.dump(firm_to_idx, f, indent=4)\n",
        "    logging.info(f\"Firm-to-index mapping saved to {firm_map_path}\")\n",
        "\n",
        "    return time_to_idx, firm_to_idx\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def form_tensor_and_observed_set(\n",
        "    df_normalized: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\",\n",
        "    metadata_dir: str = \"data_interim\"\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Reshapes the normalized panel DataFrame into a 3D NumPy tensor.\n",
        "\n",
        "    This function orchestrates the transformation of the 2D panel data into a\n",
        "    3D tensor of shape (T, N, L) as required by the ACT-Tensor model. It performs:\n",
        "    1. Creation of deterministic mappings from date/permno to integer indices.\n",
        "    2. Allocation of a NaN-initialized NumPy array for the tensor.\n",
        "    3. Efficient population of the tensor using the mappings and DataFrame values.\n",
        "    4. Creation of a boolean mask representing the set of observed entries (Omega).\n",
        "    5. Calculation and logging of tensor metadata (dimensions, density).\n",
        "    6. Saving the tensor, mask, and metadata to disk for downstream use.\n",
        "\n",
        "    Args:\n",
        "        df_normalized: The DataFrame of normalized characteristics from Task 5.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the final tensor and mask files.\n",
        "        metadata_dir: Directory to save metadata like index mappings.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - tensor_X (np.ndarray): The 3D tensor of shape (T, N, L) with NaNs\n",
        "          for missing values.\n",
        "        - mask_observed (np.ndarray): A boolean tensor of the same shape, where\n",
        "          True indicates an observed value.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_normalized, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df_normalized' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Ensure output directories exist.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(metadata_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Construct the firm and time index mappings ---\n",
        "    time_to_idx, firm_to_idx = _create_index_mappings(\n",
        "        df_normalized, config, metadata_dir\n",
        "    )\n",
        "    T = len(time_to_idx)\n",
        "    N = len(firm_to_idx)\n",
        "    L = config[\"tensor_policies\"][\"L_characteristics\"]\n",
        "\n",
        "    # Verify that the number of columns in the DataFrame matches L.\n",
        "    if df_normalized.shape[1] != L:\n",
        "        raise ValueError(\n",
        "            f\"DataFrame has {df_normalized.shape[1]} columns, but config \"\n",
        "            f\"'L_characteristics' is {L}.\"\n",
        "        )\n",
        "    logging.info(f\"Step 1/3: Index mappings created. Tensor dimensions: T={T}, N={N}, L={L}.\")\n",
        "\n",
        "    # --- Step 2: Allocate the tensor and populate observed entries ---\n",
        "    # Allocate a 3D NumPy array initialized with np.nan.\n",
        "    # The dtype is set from the config to ensure high precision.\n",
        "    float_dtype = config[\"reproducibility\"][\"float_dtype\"]\n",
        "    tensor_X = np.full((T, N, L), np.nan, dtype=float_dtype)\n",
        "\n",
        "    # For efficient population, get integer indices for all rows at once.\n",
        "    # This is a vectorized operation and vastly outperforms row-wise iteration.\n",
        "    time_indices = df_normalized.index.get_level_values('date').map(time_to_idx).to_numpy()\n",
        "    firm_indices = df_normalized.index.get_level_values('permno').map(firm_to_idx).to_numpy()\n",
        "\n",
        "    # Use advanced NumPy indexing to populate the tensor in one operation.\n",
        "    tensor_X[time_indices, firm_indices, :] = df_normalized.to_numpy()\n",
        "    logging.info(\"Step 2/3: Tensor allocated and populated efficiently.\")\n",
        "\n",
        "    # --- Step 3: Construct the observed-index set and compute metadata ---\n",
        "    # Create the boolean mask Omega, where True indicates an observed entry.\n",
        "    # This is the computational representation of the set Ω.\n",
        "    mask_observed = ~np.isnan(tensor_X)\n",
        "\n",
        "    # Calculate metadata for logging and validation.\n",
        "    observed_count = mask_observed.sum()\n",
        "    total_elements = T * N * L\n",
        "    global_density = observed_count / total_elements\n",
        "\n",
        "    # Verify that the observed density is in a plausible range (e.g., ~17% from paper).\n",
        "    if not (0.05 < global_density < 0.5):\n",
        "        logging.warning(\n",
        "            f\"Global data density is {global_density:.4f}, which may be \"\n",
        "            f\"unexpected. The paper reports ~17% observed (83% missing).\"\n",
        "        )\n",
        "\n",
        "    # Compile metadata into a dictionary.\n",
        "    tensor_metadata = {\n",
        "        \"T_dim\": T,\n",
        "        \"N_dim\": N,\n",
        "        \"L_dim\": L,\n",
        "        \"observed_count\": int(observed_count),\n",
        "        \"total_elements\": total_elements,\n",
        "        \"global_density\": global_density\n",
        "    }\n",
        "\n",
        "    # Save metadata to a JSON file.\n",
        "    metadata_path = os.path.join(metadata_dir, \"tensor_metadata.json\")\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(tensor_metadata, f, indent=4)\n",
        "    logging.info(f\"Step 3/3: Observed-set mask created. Metadata saved to {metadata_path}.\")\n",
        "\n",
        "    # --- Checkpointing ---\n",
        "    # Save the final tensor to a compressed NumPy archive.\n",
        "    tensor_path = os.path.join(output_dir, \"tensor_X.npz\")\n",
        "    np.savez_compressed(tensor_path, X=tensor_X)\n",
        "    logging.info(f\"Tensor saved to {tensor_path}\")\n",
        "\n",
        "    # Save the boolean mask to a separate compressed archive.\n",
        "    mask_path = os.path.join(output_dir, \"mask_observed.npz\")\n",
        "    np.savez_compressed(mask_path, mask=mask_observed)\n",
        "    logging.info(f\"Observed mask saved to {mask_path}\")\n",
        "\n",
        "    return tensor_X, mask_observed\n"
      ],
      "metadata": {
        "id": "gPZLCjncjEVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Create Evaluation Mask for MAR (Missing-At-Random) Regime\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Create Evaluation Mask for MAR (Missing-At-Random) Regime\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def create_mar_mask(\n",
        "    mask_observed: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\",\n",
        "    existing_masks: Optional[List[np.ndarray]] = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Creates a held-out evaluation mask under the Missing-At-Random (MAR) assumption.\n",
        "\n",
        "    This function generates a boolean mask that randomly selects a specified\n",
        "    fraction of the observed entries in the input tensor to be used as a\n",
        "    held-out test set. The process is governed by a dedicated random seed to\n",
        "    ensure perfect reproducibility.\n",
        "\n",
        "    The steps are as follows:\n",
        "    1. Identify all observed entries from the input `mask_observed`.\n",
        "    2. Set the specific random seed for MAR masking.\n",
        "    3. Uniformly sample a specified fraction of these entries without replacement.\n",
        "    4. Create a new boolean mask of the same shape as the input, with `True` at\n",
        "       the locations of the sampled entries.\n",
        "    5. Verify that the new mask is disjoint from any previously created masks.\n",
        "    6. Save the mask as a compressed NumPy array and the list of masked indices\n",
        "       as a CSV for auditing.\n",
        "\n",
        "    Args:\n",
        "        mask_observed: A boolean NumPy array of shape (T, N, L) where `True`\n",
        "                       indicates an observed entry in the original tensor.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the output mask and audit files.\n",
        "        existing_masks: An optional list of other boolean masks to check for\n",
        "                        disjointness.\n",
        "\n",
        "    Returns:\n",
        "        A boolean NumPy array of shape (T, N, L) representing the MAR\n",
        "        evaluation mask.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of observed entries is zero or if the new\n",
        "                    mask overlaps with an existing mask.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(mask_observed, np.ndarray) or mask_observed.dtype != bool:\n",
        "        raise TypeError(\"'mask_observed' must be a boolean NumPy array.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Identify eligible observed entries and set random seed ---\n",
        "    # Find the coordinates (t, n, l) of all observed entries.\n",
        "    # `np.argwhere` returns an array of shape (num_observed, 3).\n",
        "    observed_indices = np.argwhere(mask_observed)\n",
        "\n",
        "    # Get the total number of observed entries.\n",
        "    num_observed = observed_indices.shape[0]\n",
        "    if num_observed == 0:\n",
        "        raise ValueError(\"The 'mask_observed' contains no observed entries to sample from.\")\n",
        "\n",
        "    # Set the specific random seed for MAR masking to ensure reproducibility.\n",
        "    mar_seed = config[\"reproducibility\"][\"seed_mask_mar\"]\n",
        "    np.random.seed(mar_seed)\n",
        "    logging.info(f\"Step 1/3: Identified {num_observed} observed entries. Random seed set to {mar_seed}.\")\n",
        "\n",
        "    # --- Step 2: Uniformly sample a fraction of observed entries ---\n",
        "    # Get the fraction of observed data to hold out for the test set.\n",
        "    test_fraction = config[\"masking_regimes\"][\"test_fraction_of_observed\"]\n",
        "\n",
        "    # Calculate the target number of entries to mask using the floor function.\n",
        "    # Formula: M_MAR = floor(test_fraction * |Omega_full|)\n",
        "    num_to_mask = int(np.floor(test_fraction * num_observed))\n",
        "\n",
        "    # Uniformly sample `num_to_mask` indices from the list of observed entries\n",
        "    # without replacement. We sample from the range of indices [0, num_observed-1].\n",
        "    sampled_linear_indices = np.random.choice(\n",
        "        num_observed,\n",
        "        size=num_to_mask,\n",
        "        replace=False\n",
        "    )\n",
        "\n",
        "    # Retrieve the (t, n, l) coordinates corresponding to the sampled indices.\n",
        "    masked_coords = observed_indices[sampled_linear_indices]\n",
        "    logging.info(f\"Step 2/3: Sampled {num_to_mask} entries for the MAR mask ({test_fraction:.2%}).\")\n",
        "\n",
        "    # --- Step 3: Create, verify, and save the MAR mask ---\n",
        "    # Create a new boolean mask initialized to all False.\n",
        "    mask_eval_mar = np.zeros_like(mask_observed, dtype=bool)\n",
        "\n",
        "    # Use advanced integer indexing to set the sampled locations to True.\n",
        "    # This is highly efficient. We need to transpose the coordinates for this.\n",
        "    mask_eval_mar[masked_coords[:, 0], masked_coords[:, 1], masked_coords[:, 2]] = True\n",
        "\n",
        "    # Verify that the number of True values in the new mask is correct.\n",
        "    assert mask_eval_mar.sum() == num_to_mask\n",
        "\n",
        "    # If provided, check that the new mask is disjoint from existing masks.\n",
        "    if existing_masks:\n",
        "        for i, existing_mask in enumerate(existing_masks):\n",
        "            if np.any(mask_eval_mar & existing_mask):\n",
        "                raise ValueError(f\"MAR mask overlaps with existing mask at index {i}.\")\n",
        "\n",
        "    # --- Checkpointing and Reporting ---\n",
        "    # Save the boolean mask to a compressed NumPy archive.\n",
        "    mask_path = os.path.join(output_dir, \"mask_eval_MAR.npz\")\n",
        "    np.savez_compressed(mask_path, mask=mask_eval_mar)\n",
        "    logging.info(f\"MAR evaluation mask saved to {mask_path}\")\n",
        "\n",
        "    # Save the list of masked indices to a CSV file for audit purposes.\n",
        "    indices_df = pd.DataFrame(masked_coords, columns=[\"t_idx\", \"n_idx\", \"l_idx\"])\n",
        "    audit_path = os.path.join(output_dir, \"mask_MAR_indices.csv\")\n",
        "    indices_df.to_csv(audit_path, index=False)\n",
        "    logging.info(f\"MAR mask indices audit file saved to {audit_path}\")\n",
        "\n",
        "    # Compute and log the effective masking rate for verification.\n",
        "    effective_rate = num_to_mask / num_observed\n",
        "    logging.info(f\"Step 3/3: Verification complete. Effective masking rate: {effective_rate:.6f}.\")\n",
        "\n",
        "    # Final check on the rate against a tolerance.\n",
        "    if not np.isclose(effective_rate, test_fraction, atol=1/num_observed):\n",
        "        logging.warning(\n",
        "            f\"Effective masking rate ({effective_rate:.4f}) differs slightly \"\n",
        "            f\"from target rate ({test_fraction:.4f}) due to flooring.\"\n",
        "        )\n",
        "\n",
        "    return mask_eval_mar\n"
      ],
      "metadata": {
        "id": "lHkUZOlmjoKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Create Evaluation Mask for Block Missingness Regime\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Create Evaluation Mask for Block Missingness Regime\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def create_block_mask(\n",
        "    mask_observed: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\",\n",
        "    existing_masks: Optional[List[np.ndarray]] = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Creates a held-out evaluation mask with structured block missingness.\n",
        "\n",
        "    This function simulates real-world data outages by masking contiguous blocks\n",
        "    of data for each firm-characteristic time series. The process is designed to\n",
        "    test a model's ability to handle structured, non-random missingness.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. Sets the specific random seed for block masking to ensure reproducibility.\n",
        "    2. For each firm-characteristic series, determines the time span of observed data.\n",
        "    3. With a given probability (p_start), places the block at the beginning of the\n",
        "       series' observed span. Otherwise, it places the block at a random valid\n",
        "       starting position within that span.\n",
        "    4. The mask is applied only to entries that were originally observed.\n",
        "    5. The process aims for an approximate global masking rate, which is then\n",
        "       verified.\n",
        "    6. The final mask is checked for disjointness against any existing masks.\n",
        "    7. The mask and its indices are saved to disk for evaluation and auditing.\n",
        "\n",
        "    Performance Note:\n",
        "    The core logic iterates through each of the N * L time series. For extremely\n",
        "    large datasets, this Python loop can be a performance bottleneck. For\n",
        "    production-scale applications, this loop is an ideal candidate for\n",
        "    parallelization using libraries like `joblib` to distribute the per-series\n",
        "    computations across multiple CPU cores.\n",
        "\n",
        "    Args:\n",
        "        mask_observed: A boolean NumPy array of shape (T, N, L) where `True`\n",
        "                       indicates an observed entry.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the output mask and audit files.\n",
        "        existing_masks: An optional list of other boolean masks to check for\n",
        "                        disjointness.\n",
        "\n",
        "    Returns:\n",
        "        A boolean NumPy array of shape (T, N, L) representing the Block\n",
        "        evaluation mask.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the new mask overlaps with an existing mask.\n",
        "        TypeError: If input `mask_observed` is not a boolean NumPy array.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the input mask is a 3D boolean NumPy array.\n",
        "    if not isinstance(mask_observed, np.ndarray) or mask_observed.dtype != bool or mask_observed.ndim != 3:\n",
        "        raise TypeError(\"'mask_observed' must be a 3D boolean NumPy array.\")\n",
        "\n",
        "    # Ensure the output directory exists for saving artifacts.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Set random seed and define block parameters ---\n",
        "    # Set the specific random seed for Block masking from the config to ensure reproducibility.\n",
        "    block_seed = config[\"reproducibility\"][\"seed_mask_block\"]\n",
        "    np.random.seed(block_seed)\n",
        "\n",
        "    # Extract block missingness parameters from the configuration dictionary.\n",
        "    block_config = config[\"masking_regimes\"][\"Block\"]\n",
        "    block_length: int = block_config[\"block_length_months\"]\n",
        "    start_prob: float = block_config[\"start_at_begin_prob\"]\n",
        "\n",
        "    # Get tensor dimensions from the shape of the observed mask.\n",
        "    T, N, L = mask_observed.shape\n",
        "\n",
        "    # Initialize the output mask with all False values, with the same shape and type.\n",
        "    mask_eval_block = np.zeros_like(mask_observed, dtype=bool)\n",
        "\n",
        "    logging.info(\n",
        "        f\"Step 1/3: Initialized block masking with seed={block_seed}, \"\n",
        "        f\"length={block_length}, start_prob={start_prob}.\"\n",
        "    )\n",
        "\n",
        "    # --- Step 2: For each firm-characteristic pair, randomly place one block ---\n",
        "    # This section iterates through each of the N * L series. For production scale,\n",
        "    # consider parallelizing this loop (e.g., with `joblib.Parallel`).\n",
        "    for n in range(N):\n",
        "        for l in range(L):\n",
        "            # Extract the 1D boolean vector for the current series' observed status.\n",
        "            series_mask = mask_observed[:, n, l]\n",
        "\n",
        "            # Find the time indices 't' where data is observed for this series.\n",
        "            observed_times = np.where(series_mask)[0]\n",
        "\n",
        "            # A block can only be placed if there are at least `block_length` observed points.\n",
        "            if len(observed_times) < block_length:\n",
        "                continue\n",
        "\n",
        "            # Determine the full time span of the series' observations.\n",
        "            min_obs_time = observed_times[0]\n",
        "            max_obs_time = observed_times[-1]\n",
        "\n",
        "            # The latest possible start time for a block to fit within the observed span.\n",
        "            latest_possible_start = max_obs_time - block_length + 1\n",
        "\n",
        "            # If the observed span is too short to fit a block, skip.\n",
        "            if min_obs_time > latest_possible_start:\n",
        "                continue\n",
        "\n",
        "            # Decide probabilistically whether to place the block at the start or randomly.\n",
        "            if np.random.rand() < start_prob:\n",
        "                # Set the block to start at the very first observed data point.\n",
        "                start_time = min_obs_time\n",
        "            else:\n",
        "                # Sample a start time uniformly from the entire valid\n",
        "                # integer range within the series' observed span.\n",
        "                start_time = np.random.randint(min_obs_time, latest_possible_start + 1)\n",
        "\n",
        "            # Define the contiguous time interval for the block.\n",
        "            end_time = start_time + block_length\n",
        "            block_interval = np.arange(start_time, end_time)\n",
        "\n",
        "            # The final mask for this series is the intersection of the contiguous block\n",
        "            # interval and the set of originally observed time points.\n",
        "            mask_indices_for_series = np.intersect1d(block_interval, observed_times)\n",
        "\n",
        "            # Update the global block mask at the identified locations for this series.\n",
        "            mask_eval_block[mask_indices_for_series, n, l] = True\n",
        "\n",
        "    logging.info(\"Step 2/3: Completed block placement for all series.\")\n",
        "\n",
        "    # --- Step 3: Verify overall masking rate and disjointness, then save ---\n",
        "    # Calculate the total number of entries that were actually masked.\n",
        "    num_masked = mask_eval_block.sum()\n",
        "    # Get the total number of originally observed entries.\n",
        "    num_observed = mask_observed.sum()\n",
        "\n",
        "    # Handle the edge case where no entries were masked.\n",
        "    if num_masked == 0:\n",
        "        logging.warning(\"Block masking resulted in zero masked entries. This may be expected if data is extremely sparse.\")\n",
        "        return mask_eval_block\n",
        "\n",
        "    # Calculate the effective global masking rate.\n",
        "    effective_rate = num_masked / num_observed\n",
        "    target_fraction = config[\"masking_regimes\"][\"test_fraction_of_observed\"]\n",
        "\n",
        "    # Log the result and warn if it deviates significantly from the target range.\n",
        "    logging.info(f\"Total entries masked: {num_masked}. Effective global rate: {effective_rate:.4f}\")\n",
        "    if not (0.08 <= effective_rate <= 0.12):\n",
        "        logging.warning(\n",
        "            f\"Effective masking rate ({effective_rate:.4f}) deviates \"\n",
        "            f\"significantly from the target range [8%, 12%].\"\n",
        "        )\n",
        "\n",
        "    # Check for disjointness with any previously generated masks. This is a critical check.\n",
        "    if existing_masks:\n",
        "        for i, existing_mask in enumerate(existing_masks):\n",
        "            # The bitwise AND operation finds any overlapping True values.\n",
        "            if np.any(mask_eval_block & existing_mask):\n",
        "                overlap_count = (mask_eval_block & existing_mask).sum()\n",
        "                raise ValueError(\n",
        "                    f\"Block mask overlaps with existing mask at index {i}. \"\n",
        "                    f\"Overlap count: {overlap_count}. Experimental design is compromised.\"\n",
        "                )\n",
        "\n",
        "    # --- Checkpointing and Reporting ---\n",
        "    # Save the final boolean mask to a compressed NumPy archive.\n",
        "    mask_path = os.path.join(output_dir, \"mask_eval_Block.npz\")\n",
        "    np.savez_compressed(mask_path, mask=mask_eval_block)\n",
        "    logging.info(f\"Block evaluation mask saved to {mask_path}\")\n",
        "\n",
        "    # Save the coordinates of the masked entries to a CSV for auditing.\n",
        "    masked_coords = np.argwhere(mask_eval_block)\n",
        "    indices_df = pd.DataFrame(masked_coords, columns=[\"t_idx\", \"n_idx\", \"l_idx\"])\n",
        "    audit_path = os.path.join(output_dir, \"mask_Block_indices.csv\")\n",
        "    indices_df.to_csv(audit_path, index=False)\n",
        "    logging.info(f\"Block mask indices audit file saved to {audit_path}\")\n",
        "    logging.info(\"Step 3/3: Verification and saving complete.\")\n",
        "\n",
        "    return mask_eval_block\n"
      ],
      "metadata": {
        "id": "lY6vAL2vkQIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Create Evaluation Mask for Logistic (Logit) Missingness Regime\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Create Evaluation Mask for Logistic (Logit) Missingness Regime\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Prepare covariates for the logistic model (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _prepare_logit_covariates(\n",
        "    df_clean: pd.DataFrame,\n",
        "    df_chars_raw: pd.DataFrame,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int],\n",
        "    T: int,\n",
        "    N: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Pre-computes the static covariates used in the Logit missingness model.\n",
        "\n",
        "    This function calculates firm-level and firm-month level characteristics\n",
        "    like log-size, age, and a distress indicator, then aligns them into a\n",
        "    tensor of shape (T, N, num_covariates) for efficient lookup during simulation.\n",
        "\n",
        "    Methodological Note: Covariates are generated from the fully cleansed and\n",
        "    observed panel. The simulation models the probability of an *observed* point\n",
        "    becoming missing, conditional on these covariates.\n",
        "\n",
        "    Args:\n",
        "        df_clean: The cleansed DataFrame from Task 3, containing market cap data.\n",
        "        df_chars_raw: The raw characteristics DataFrame from Task 4 for distress indicator.\n",
        "        time_to_idx: Mapping from timestamp to integer index.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "        T: Total number of time periods.\n",
        "        N: Total number of firms.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of shape (T, N, 3) containing the covariates\n",
        "        [log_size, age, distress_indicator] for each firm-month.\n",
        "    \"\"\"\n",
        "    # Create a combined DataFrame for easier feature engineering.\n",
        "    df_cov = df_clean.reset_index().merge(\n",
        "        df_chars_raw.reset_index(), on=['date', 'permno'], how='left'\n",
        "    )\n",
        "\n",
        "    # --- 1. Compute Log Market Equity (logsize) ---\n",
        "    # Calculate market equity from price and shares outstanding.\n",
        "    df_cov['market_equity'] = df_cov['prc'].abs() * df_cov['shrout']\n",
        "    # Compute the natural logarithm of market equity, replacing zeros with NaN before logging.\n",
        "    df_cov['logsize'] = np.log(df_cov['market_equity'].replace(0, np.nan))\n",
        "\n",
        "    # --- 2. Compute Firm Age ---\n",
        "    # Find the first appearance date for each firm using a groupby-transform.\n",
        "    first_appearance = df_cov.groupby('permno')['date'].transform('min')\n",
        "    # Calculate age in months as the difference between the current date and first appearance.\n",
        "    df_cov['age'] = ((df_cov['date'] - first_appearance) / np.timedelta64(1, 'M')).round()\n",
        "\n",
        "    # --- 3. Compute Distress Indicator ---\n",
        "    # Define distress as being in the top decile of book-to-market each month.\n",
        "    df_cov['distress_indicator'] = df_cov.groupby('date')['book_to_market'].transform(\n",
        "        lambda x: (x.rank(pct=True, ascending=False) <= 0.1).astype(float)\n",
        "    )\n",
        "\n",
        "    # --- 4. Assemble Covariate Tensor ---\n",
        "    # Initialize an empty tensor for the covariates.\n",
        "    covariate_tensor = np.full((T, N, 3), np.nan, dtype=np.float32)\n",
        "\n",
        "    # Map date and permno to integer indices for tensor population.\n",
        "    time_indices = df_cov['date'].map(time_to_idx)\n",
        "    firm_indices = df_cov['permno'].map(firm_to_idx)\n",
        "\n",
        "    # Select the computed covariate columns.\n",
        "    cov_cols = ['logsize', 'age', 'distress_indicator']\n",
        "\n",
        "    # Create a mask to filter out rows where mapping to tensor indices failed.\n",
        "    valid_idx = pd.notna(time_indices) & pd.notna(firm_indices)\n",
        "\n",
        "    # Populate the tensor using advanced indexing with the valid indices.\n",
        "    covariate_tensor[\n",
        "        time_indices[valid_idx].astype(int),\n",
        "        firm_indices[valid_idx].astype(int),\n",
        "        :\n",
        "    ] = df_cov.loc[valid_idx, cov_cols].to_numpy(dtype=np.float32)\n",
        "\n",
        "    # Forward-fill any remaining NaNs within the tensor to ensure covariates are available.\n",
        "    # This handles cases where a firm exists in the tensor but has no covariates for a given month.\n",
        "    df_tensor_reshaped = pd.DataFrame(covariate_tensor.reshape(T, -1))\n",
        "    df_tensor_reshaped.fillna(method='ffill', inplace=True)\n",
        "    covariate_tensor = df_tensor_reshaped.to_numpy().reshape(T, N, 3)\n",
        "\n",
        "    logging.info(\"Prepared static covariates (logsize, age, distress) for Logit model.\")\n",
        "    return covariate_tensor\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Simulate missingness (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _run_logit_simulation(\n",
        "    mask_observed: np.ndarray,\n",
        "    covariate_tensor: np.ndarray,\n",
        "    alpha: np.ndarray,\n",
        "    beta: np.ndarray,\n",
        "    initial_gap_length: int = 6\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Runs the two-stage logistic simulation to generate a missingness mask.\n",
        "\n",
        "    This function is a complete rewrite focusing on vectorization and accuracy.\n",
        "    Stage 1 (Initial Gap) and Stage 2 (Monthly Conditional) are now implemented\n",
        "    using efficient NumPy broadcasting to avoid slow Python loops.\n",
        "\n",
        "    Args:\n",
        "        mask_observed: Boolean array of observed entries.\n",
        "        covariate_tensor: 3D array of static covariates (T, N, 3).\n",
        "        alpha: Coefficients for the initial gap model.\n",
        "        beta: Coefficients for the monthly conditional model.\n",
        "        initial_gap_length: The number of initial observed points to mask.\n",
        "\n",
        "    Returns:\n",
        "        A boolean array of simulated missingness, before intersection with\n",
        "        the observed mask.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Get tensor dimensions.\n",
        "    T, N, L = mask_observed.shape\n",
        "\n",
        "    # Initialize the simulation mask that will be populated.\n",
        "    simulated_mask = np.zeros_like(mask_observed, dtype=bool)\n",
        "\n",
        "    # Define the sigmoid function for probability calculation.\n",
        "    def sigmoid(x):\n",
        "        with np.errstate(over='ignore'):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # --- Stage 1: Initial Gap Simulation (Vectorized) ---\n",
        "    # Use covariates from the first time period for the initial gap model.\n",
        "    initial_covariates = covariate_tensor[0, :, :]  # Shape (N, 3)\n",
        "    # Add intercept term by creating a column of ones.\n",
        "    X_initial = np.hstack([np.ones((N, 1)), np.nan_to_num(initial_covariates)])\n",
        "\n",
        "    # Compute initial gap probabilities for all N firms.\n",
        "    pi_initial = sigmoid(X_initial @ alpha)\n",
        "\n",
        "    # Simulate initial gaps for each of the N*L series.\n",
        "    initial_gap_draws = np.random.rand(N, L) < pi_initial[:, np.newaxis]\n",
        "\n",
        "    # Vectorized application of the initial gap mask.\n",
        "    # Find all (n, l) pairs that need a gap.\n",
        "    n_coords, l_coords = np.where(initial_gap_draws)\n",
        "\n",
        "    # Use cumsum to find the k-th observed point for each series efficiently.\n",
        "    observed_counts = np.cumsum(mask_observed, axis=0)\n",
        "    # Create a mask for the first `k` observed points.\n",
        "    is_initial_obs = (observed_counts > 0) & (observed_counts <= initial_gap_length)\n",
        "\n",
        "    # Apply this logic only to the series selected for an initial gap.\n",
        "    initial_mask_template = np.zeros_like(mask_observed, dtype=bool)\n",
        "    initial_mask_template[:, n_coords, l_coords] = is_initial_obs[:, n_coords, l_coords]\n",
        "    simulated_mask |= initial_mask_template\n",
        "\n",
        "    # --- Stage 2: Monthly Conditional Simulation (Vectorized) ---\n",
        "    # Initialize the dynamic 'previous missing' indicator.\n",
        "    prev_missing_indicator = initial_gap_draws.astype(float)\n",
        "\n",
        "    # Iterate through time, starting from the second period.\n",
        "    for t in range(1, T):\n",
        "        # Extract lagged static covariates for all firms.\n",
        "        lagged_covs = np.nan_to_num(covariate_tensor[t - 1, :, :]) # Shape (N, 3)\n",
        "\n",
        "        # Compute the firm-specific part of the score using lagged covariates.\n",
        "        # beta[1:4] corresponds to [logsize, age, distress].\n",
        "        firm_score_component = lagged_covs @ beta[1:4] # Shape (N,)\n",
        "\n",
        "        # Compute the full linear score using broadcasting.\n",
        "        # score = beta_0 + (firm_component) + beta_4 * prev_missing\n",
        "        score = beta[0] + firm_score_component[:, np.newaxis] + prev_missing_indicator * beta[4]\n",
        "\n",
        "        # Calculate probabilities for all N*L series at time t.\n",
        "        pi_monthly_t = sigmoid(score)\n",
        "\n",
        "        # Simulate missingness for time t via a vectorized Bernoulli draw.\n",
        "        monthly_draws = np.random.rand(N, L) < pi_monthly_t\n",
        "\n",
        "        # Update the mask for the current time slice.\n",
        "        simulated_mask[t, :, :] |= monthly_draws\n",
        "\n",
        "        # Update the 'prev_missing_indicator' for the next iteration.\n",
        "        prev_missing_indicator = monthly_draws.astype(float)\n",
        "\n",
        "    # The final mask is the intersection with the originally observed set.\n",
        "    return simulated_mask & mask_observed\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def create_logit_mask(\n",
        "    mask_observed: np.ndarray,\n",
        "    df_clean: pd.DataFrame,\n",
        "    df_chars_raw: pd.DataFrame,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\",\n",
        "    existing_masks: Optional[List[np.ndarray]] = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Creates a held-out evaluation mask based on a two-stage logistic model.\n",
        "\n",
        "    This function simulates a realistic, non-random missingness pattern where the\n",
        "    probability of an entry being missing depends on firm characteristics. This\n",
        "    re-implementation features a more robust calibration loop and sources model\n",
        "    parameters from the configuration file.\n",
        "\n",
        "    Args:\n",
        "        mask_observed: Boolean array of observed entries.\n",
        "        df_clean: Cleansed DataFrame for sourcing covariates.\n",
        "        df_chars_raw: Raw characteristics DataFrame for sourcing covariates.\n",
        "        time_to_idx: Mapping from timestamp to integer index.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the output mask and audit files.\n",
        "        existing_masks: Optional list of other masks to check for disjointness.\n",
        "\n",
        "    Returns:\n",
        "        A boolean NumPy array representing the Logit evaluation mask.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Prepare Covariates, Set Seed, and Load Parameters ---\n",
        "    T, N, L = mask_observed.shape\n",
        "    logit_seed = config[\"reproducibility\"][\"seed_mask_logit\"]\n",
        "    np.random.seed(logit_seed)\n",
        "\n",
        "    # Prepare the static covariates needed for the simulation.\n",
        "    covariate_tensor = _prepare_logit_covariates(\n",
        "        df_clean, df_chars_raw, time_to_idx, firm_to_idx, T, N\n",
        "    )\n",
        "\n",
        "    # Load model parameters from config, ensuring they exist.\n",
        "    logit_config = config[\"masking_regimes\"][\"Logit\"]\n",
        "    alpha = np.array(logit_config[\"alpha_coeffs\"])\n",
        "    beta_initial = np.array(logit_config[\"beta_coeffs\"])\n",
        "\n",
        "    logging.info(\"Step 1/3: Covariate preparation and parameter loading complete.\")\n",
        "\n",
        "    # --- Step 2: Simulate Missingness with a Robust Calibration Loop ---\n",
        "    target_fraction = config[\"masking_regimes\"][\"test_fraction_of_observed\"]\n",
        "    num_observed = mask_observed.sum()\n",
        "\n",
        "    # Initialize beta and calibration parameters.\n",
        "    beta = beta_initial.copy()\n",
        "    learning_rate = 0.5 # Controls speed of convergence for calibration\n",
        "\n",
        "    for i in range(10): # Max 10 calibration iterations\n",
        "        # Run the full simulation with the current beta.\n",
        "        mask_eval_logit = _run_logit_simulation(\n",
        "            mask_observed, covariate_tensor, alpha, beta\n",
        "        )\n",
        "        current_rate = mask_eval_logit.sum() / num_observed\n",
        "        error = current_rate - target_fraction\n",
        "\n",
        "        logging.info(f\"Calibration iter {i+1}: Intercept={beta[0]:.4f}, Rate={current_rate:.4f}, Error={error:.4f}\")\n",
        "\n",
        "        # Stop if the rate is within a small tolerance of the target.\n",
        "        if abs(error) < 0.005:\n",
        "            logging.info(\"Calibration successful: target rate achieved.\")\n",
        "            break\n",
        "\n",
        "        # Update the intercept on the log-odds scale to adjust the rate.\n",
        "        beta[0] -= learning_rate * error\n",
        "\n",
        "    logging.info(\"Step 2/3: Logit simulation and calibration complete.\")\n",
        "\n",
        "    # --- Step 3: Verify Disjointness and Save ---\n",
        "    # Final verification and saving logic remains the same as the original.\n",
        "    num_masked = mask_eval_logit.sum()\n",
        "    effective_rate = num_masked / num_observed\n",
        "    logging.info(f\"Final calibrated masking rate: {effective_rate:.4f}\")\n",
        "\n",
        "    if existing_masks:\n",
        "        for i, existing_mask in enumerate(existing_masks):\n",
        "            if np.any(mask_eval_logit & existing_mask):\n",
        "                raise ValueError(f\"Logit mask overlaps with existing mask at index {i}.\")\n",
        "\n",
        "    mask_path = os.path.join(output_dir, \"mask_eval_Logit.npz\")\n",
        "    np.savez_compressed(mask_path, mask=mask_eval_logit)\n",
        "\n",
        "    masked_coords = np.argwhere(mask_eval_logit)\n",
        "    indices_df = pd.DataFrame(masked_coords, columns=[\"t_idx\", \"n_idx\", \"l_idx\"])\n",
        "    audit_path = os.path.join(output_dir, \"mask_Logit_indices.csv\")\n",
        "    indices_df.to_csv(audit_path, index=False)\n",
        "    logging.info(f\"Step 3/3: Verification and saving complete. Audit file at {audit_path}\")\n",
        "\n",
        "    return mask_eval_logit\n"
      ],
      "metadata": {
        "id": "BJWrJam3ksAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Construct Training Tensors $\\mathcal{X}_{\\text{train}}$ for Each Masking Regime\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Construct Training Tensors for Each Masking Regime\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def create_training_tensors(\n",
        "    tensor_X: np.ndarray,\n",
        "    mask_observed: np.ndarray,\n",
        "    regimes: List[str],\n",
        "    data_dir: str = \"data_processed\"\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Constructs and saves training tensors and masks for each evaluation regime.\n",
        "\n",
        "    For each specified missingness regime (e.g., 'MAR', 'Block', 'Logit'), this\n",
        "    function takes the complete ground-truth tensor and applies the corresponding\n",
        "    evaluation mask to create a training tensor. The entries selected for the\n",
        "    held-out test set are replaced with `np.nan`.\n",
        "\n",
        "    It also generates and saves the corresponding 'training mask', which represents\n",
        "    the set of all entries that are both originally observed and not held out for\n",
        "    evaluation (Ω_train = Ω_full \\\\ M_regime).\n",
        "\n",
        "    Args:\n",
        "        tensor_X: The complete 3D ground-truth tensor with shape (T, N, L),\n",
        "                  containing NaNs for originally missing data.\n",
        "        mask_observed: A boolean array of shape (T, N, L) where `True` indicates\n",
        "                       an originally observed entry (Ω_full).\n",
        "        regimes: A list of strings identifying the regimes to process,\n",
        "                 e.g., ['MAR', 'Block', 'Logit'].\n",
        "        data_dir: The directory where evaluation masks are stored and where\n",
        "                  training tensors/masks will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing a summary of the training set size and density\n",
        "        for each processed regime.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If an evaluation mask for a specified regime is not found.\n",
        "        ValueError: If the shape of a mask does not match the tensor's shape.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(tensor_X, np.ndarray) or not isinstance(mask_observed, np.ndarray):\n",
        "        raise TypeError(\"Inputs 'tensor_X' and 'mask_observed' must be NumPy arrays.\")\n",
        "    if tensor_X.shape != mask_observed.shape:\n",
        "        raise ValueError(\"Shape of 'tensor_X' and 'mask_observed' must be identical.\")\n",
        "\n",
        "    # Initialize a dictionary to store metadata for all regimes.\n",
        "    training_set_summary = {}\n",
        "\n",
        "    # Iterate through each specified evaluation regime.\n",
        "    for regime in regimes:\n",
        "        logging.info(f\"--- Processing regime: {regime} ---\")\n",
        "\n",
        "        # --- Step 1: Load the evaluation mask for the current regime ---\n",
        "        eval_mask_path = os.path.join(data_dir, f\"mask_eval_{regime}.npz\")\n",
        "        try:\n",
        "            # Load the compressed numpy file containing the mask.\n",
        "            mask_eval_regime = np.load(eval_mask_path)['mask']\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Evaluation mask not found for regime '{regime}' at {eval_mask_path}\")\n",
        "\n",
        "        # Validate that the loaded mask has the correct shape.\n",
        "        if mask_eval_regime.shape != tensor_X.shape:\n",
        "            raise ValueError(\n",
        "                f\"Shape of evaluation mask for regime '{regime}' \"\n",
        "                f\"({mask_eval_regime.shape}) does not match tensor shape ({tensor_X.shape}).\"\n",
        "            )\n",
        "        logging.info(f\"Step 1/3: Successfully loaded evaluation mask for {regime}.\")\n",
        "\n",
        "        # --- Step 2: Create the training tensor by applying the mask ---\n",
        "        # Create a copy of the full tensor to avoid modifying the original.\n",
        "        X_train_regime = tensor_X.copy()\n",
        "\n",
        "        # Set the held-out entries (where the eval mask is True) to NaN.\n",
        "        # This is the core operation: X_train = X where mask_eval is False.\n",
        "        X_train_regime[mask_eval_regime] = np.nan\n",
        "\n",
        "        # Verification: The number of NaNs in X_train should be the original NaNs\n",
        "        # plus the number of newly masked entries.\n",
        "        expected_nans = (~mask_observed).sum() + mask_eval_regime.sum()\n",
        "        actual_nans = np.isnan(X_train_regime).sum()\n",
        "        assert expected_nans == actual_nans, \"Mismatch in NaN count after masking.\"\n",
        "\n",
        "        # Save the resulting training tensor.\n",
        "        train_tensor_path = os.path.join(data_dir, f\"X_train_{regime}.npz\")\n",
        "        np.savez_compressed(train_tensor_path, X_train=X_train_regime)\n",
        "        logging.info(f\"Step 2/3: Training tensor for {regime} created and saved to {train_tensor_path}.\")\n",
        "\n",
        "        # --- Step 3: Construct the training observation mask and log metadata ---\n",
        "        # The training mask includes entries that were originally observed AND are not in the eval set.\n",
        "        # Formula: mask_train = mask_observed AND (NOT mask_eval)\n",
        "        mask_train_regime = mask_observed & (~mask_eval_regime)\n",
        "\n",
        "        # Save the training mask.\n",
        "        train_mask_path = os.path.join(data_dir, f\"mask_train_{regime}.npz\")\n",
        "        np.savez_compressed(train_mask_path, mask=mask_train_regime)\n",
        "        logging.info(f\"Training mask for {regime} created and saved to {train_mask_path}.\")\n",
        "\n",
        "        # Compute and store metadata for the summary report.\n",
        "        omega_train_size = mask_train_regime.sum()\n",
        "        rho_train = omega_train_size / tensor_X.size\n",
        "        training_set_summary[regime] = {\n",
        "            \"Omega_train_size\": int(omega_train_size),\n",
        "            \"rho_train\": rho_train\n",
        "        }\n",
        "        logging.info(\n",
        "            f\"Step 3/3: Metadata for {regime}: \"\n",
        "            f\"Training set size = {int(omega_train_size)}, \"\n",
        "            f\"Density = {rho_train:.4f}\"\n",
        "        )\n",
        "\n",
        "    # --- Finalization ---\n",
        "    # Save the summary metadata for all processed regimes to a JSON file.\n",
        "    summary_path = os.path.join(data_dir, \"training_set_summary.json\")\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(training_set_summary, f, indent=4)\n",
        "    logging.info(f\"Training set summary for all regimes saved to {summary_path}\")\n",
        "\n",
        "    return training_set_summary\n"
      ],
      "metadata": {
        "id": "J21UwXn-lX3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Firm Clustering by Time–Characteristic Profiles via K-Means\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Firm Clustering by Time–Characteristic Profiles via K-Means\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def cluster_firms_by_profile(\n",
        "    X_train: np.ndarray,\n",
        "    firm_to_idx: Dict[int, int],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> Dict[int, List[int]]:\n",
        "    \"\"\"\n",
        "    Clusters firms into K groups based on their vectorized time-characteristic profiles.\n",
        "\n",
        "    This function implements the first core step of the ACT-Tensor framework. It\n",
        "    groups firms with similar data availability patterns and characteristic profiles\n",
        "    together using the K-Means algorithm. This allows for group-specific treatment\n",
        "    during the imputation phase.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. For each firm, its (T, L) slice of the training tensor is vectorized into a\n",
        "       single long vector of length T*L. Missing values (NaN) are filled with zero.\n",
        "    2. The scikit-learn implementation of K-Means is fitted to this (N, T*L) matrix\n",
        "       of firm profiles, using hyperparameters from the configuration file.\n",
        "    3. The resulting cluster labels for each firm are validated, logged, and saved\n",
        "       to disk for auditing and downstream use.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L) for a specific regime.\n",
        "        firm_to_idx: A dictionary mapping original permno identifiers to the\n",
        "                     integer indices (0 to N-1) used in the tensor.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save cluster assignments and diagnostic files.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping each cluster ID (int) to a list of firm integer\n",
        "        indices (List[int]) belonging to that cluster.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If K-Means results in empty clusters.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(X_train, np.ndarray) or X_train.ndim != 3:\n",
        "        raise TypeError(\"Input 'X_train' must be a 3D NumPy array.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Vectorize each firm's time–characteristic slice ---\n",
        "    # Get tensor dimensions.\n",
        "    T, N, L = X_train.shape\n",
        "\n",
        "    # To create the (N, T*L) matrix for clustering, we first transpose the tensor\n",
        "    # from (T, N, L) to (N, T, L) to bring the firm dimension to the front.\n",
        "    firm_slices = X_train.transpose(1, 0, 2)\n",
        "\n",
        "    # Then, we reshape the (N, T, L) tensor into a 2D matrix of shape (N, T*L).\n",
        "    # Each row `i` of this matrix is the vectorized profile for firm `i`.\n",
        "    # Formula: v_n = vec(X_{:, n, :})\n",
        "    vectorized_profiles = firm_slices.reshape(N, T * L)\n",
        "\n",
        "    # Handle missing values (NaNs) by filling them with 0.0. This is the\n",
        "    # \"Zero-fill\" approach (Option B), which is robust for zero-centered data.\n",
        "    vectorized_profiles = np.nan_to_num(vectorized_profiles, nan=0.0)\n",
        "    logging.info(\n",
        "        f\"Step 1/3: Vectorized {N} firm profiles into a ({N}, {T*L}) matrix.\"\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Run K-Means clustering ---\n",
        "    # Extract K-Means hyperparameters from the configuration.\n",
        "    kmeans_config = config[\"optimization_settings\"][\"kmeans\"]\n",
        "    imputation_hparams = config[\"imputation_hparams\"]\n",
        "    kmeans_seed = config[\"reproducibility\"][\"seed_kmeans\"]\n",
        "\n",
        "    # Initialize the KMeans model with parameters ensuring reproducibility and robustness.\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=imputation_hparams[\"clusters_K\"],\n",
        "        init=kmeans_config[\"init\"],\n",
        "        n_init=kmeans_config[\"n_init\"],\n",
        "        max_iter=kmeans_config[\"max_iter\"],\n",
        "        tol=kmeans_config[\"tol\"],\n",
        "        random_state=kmeans_seed\n",
        "    )\n",
        "\n",
        "    # Fit the model to the vectorized profiles and get the cluster label for each firm.\n",
        "    logging.info(\"Fitting K-Means model... This may take a few moments.\")\n",
        "    cluster_labels = kmeans.fit_predict(vectorized_profiles)\n",
        "    logging.info(\"Step 2/3: K-Means fitting complete.\")\n",
        "\n",
        "    # --- Step 3: Store cluster assignments and verify cluster balance ---\n",
        "    # Create a reverse mapping from integer index back to permno.\n",
        "    idx_to_firm = {i: p for p, i in firm_to_idx.items()}\n",
        "\n",
        "    # Create a DataFrame to store and save the assignments in a readable format.\n",
        "    assignments_df = pd.DataFrame({\n",
        "        \"permno\": [idx_to_firm[i] for i in range(N)],\n",
        "        \"firm_idx\": np.arange(N),\n",
        "        \"cluster_id\": cluster_labels\n",
        "    })\n",
        "\n",
        "    # Save the detailed assignments to a CSV file for auditing.\n",
        "    assignments_path = os.path.join(output_dir, \"cluster_assignments.csv\")\n",
        "    assignments_df.to_csv(assignments_path, index=False)\n",
        "    logging.info(f\"Cluster assignments saved to {assignments_path}\")\n",
        "\n",
        "    # --- Verification and Final Output Preparation ---\n",
        "    # Group firms by their assigned cluster ID.\n",
        "    clusters: Dict[int, List[int]] = {\n",
        "        k: assignments_df[assignments_df['cluster_id'] == k]['firm_idx'].tolist()\n",
        "        for k in range(imputation_hparams[\"clusters_K\"])\n",
        "    }\n",
        "\n",
        "    # Verify that all clusters are non-empty and log their sizes.\n",
        "    cluster_sizes = {k: len(v) for k, v in clusters.items()}\n",
        "    if any(size == 0 for size in cluster_sizes.values()):\n",
        "        raise ValueError(f\"K-Means resulted in one or more empty clusters. Sizes: {cluster_sizes}\")\n",
        "\n",
        "    # Save cluster sizes to a JSON file.\n",
        "    sizes_path = os.path.join(output_dir, \"cluster_sizes.json\")\n",
        "    with open(sizes_path, 'w') as f:\n",
        "        json.dump(cluster_sizes, f, indent=4)\n",
        "    logging.info(f\"Cluster sizes verified and saved to {sizes_path}.\")\n",
        "\n",
        "    # Save the cluster centroids for diagnostic purposes.\n",
        "    centroids_path = os.path.join(output_dir, \"cluster_centroids.npz\")\n",
        "    np.savez_compressed(centroids_path, centroids=kmeans.cluster_centers_)\n",
        "    logging.info(f\"Cluster centroids saved to {centroids_path}\")\n",
        "    logging.info(\"Step 3/3: Assignments stored and cluster balance verified.\")\n",
        "\n",
        "    return clusters\n"
      ],
      "metadata": {
        "id": "X94G2cqemGcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Compute Cluster Densities and Partition into Dense vs. Sparse\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Compute Cluster Densities and Partition into Dense vs. Sparse\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_densities_and_partition(\n",
        "    X_train: np.ndarray,\n",
        "    mask_train: np.ndarray,\n",
        "    clusters: Dict[int, List[int]],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> Tuple[List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Computes data density for each firm cluster and partitions them into dense/sparse.\n",
        "\n",
        "    This function is a critical step in the ACT-Tensor \"divide and conquer\"\n",
        "    strategy. It quantifies the data availability within each cluster and uses a\n",
        "    threshold to classify them, which determines the subsequent imputation method.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. For each cluster, calculate the observed-entry ratio (density) within its\n",
        "       corresponding sub-tensor in the training set.\n",
        "    2. Compare each cluster's density to a predefined threshold (tau) to classify\n",
        "       it as either 'dense' or 'sparse'.\n",
        "    3. Save the sub-tensor and sub-mask for each cluster to disk for use in the\n",
        "       next completion steps.\n",
        "    4. Log a detailed report of the densities and classifications.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L).\n",
        "        mask_train: The boolean training mask of shape (T, N, L).\n",
        "        clusters: A dictionary mapping each cluster ID to a list of firm indices.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the cluster sub-tensors and density logs.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two lists of integers:\n",
        "        - dense_cluster_ids (List[int]): The IDs of clusters classified as dense.\n",
        "        - sparse_cluster_ids (List[int]): The IDs of clusters classified as sparse.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the union of dense and sparse clusters does not match the\n",
        "                    total number of clusters.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(mask_train, np.ndarray) or mask_train.ndim != 3:\n",
        "        raise TypeError(\"Input 'mask_train' must be a 3D NumPy array.\")\n",
        "    if not isinstance(clusters, dict):\n",
        "        raise TypeError(\"Input 'clusters' must be a dictionary.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get the density threshold from the configuration.\n",
        "    density_threshold_tau = config[\"imputation_hparams\"][\"density_threshold_tau\"]\n",
        "\n",
        "    # Initialize lists to hold the results.\n",
        "    dense_cluster_ids: List[int] = []\n",
        "    sparse_cluster_ids: List[int] = []\n",
        "    density_report: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    # --- Steps 1 & 2: Compute density and partition for each cluster ---\n",
        "    logging.info(f\"Partitioning {len(clusters)} clusters with density threshold tau = {density_threshold_tau:.2f}\")\n",
        "\n",
        "    # Iterate through each cluster ID and its list of firm indices.\n",
        "    for cluster_id, firm_indices in clusters.items():\n",
        "        # Handle case of an empty cluster, though this should be caught in Task 11.\n",
        "        if not firm_indices:\n",
        "            logging.warning(f\"Cluster {cluster_id} is empty. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- Step 1: Compute observed-entry ratio ---\n",
        "        # Use advanced integer indexing to slice the sub-mask for the current cluster.\n",
        "        cluster_mask = mask_train[:, firm_indices, :]\n",
        "\n",
        "        # The number of observed entries is the sum of the boolean sub-mask.\n",
        "        observed_count = cluster_mask.sum()\n",
        "\n",
        "        # The total number of possible entries is the size of the sub-mask.\n",
        "        total_elements = cluster_mask.size\n",
        "\n",
        "        # Calculate the density. Handle division by zero for safety.\n",
        "        density = observed_count / total_elements if total_elements > 0 else 0.0\n",
        "\n",
        "        # --- Step 2: Partition into dense and sparse sets ---\n",
        "        # Classify the cluster based on the density threshold.\n",
        "        if density >= density_threshold_tau:\n",
        "            classification = \"dense\"\n",
        "            dense_cluster_ids.append(cluster_id)\n",
        "        else:\n",
        "            classification = \"sparse\"\n",
        "            sparse_cluster_ids.append(cluster_id)\n",
        "\n",
        "        # Store results for the final report.\n",
        "        density_report[f\"cluster_{cluster_id}\"] = {\n",
        "            \"density\": density,\n",
        "            \"type\": classification,\n",
        "            \"size\": len(firm_indices),\n",
        "            \"observed_entries\": int(observed_count),\n",
        "            \"total_entries\": total_elements\n",
        "        }\n",
        "\n",
        "        # --- Step 3: Extract and save cluster sub-tensors ---\n",
        "        # Slice the training tensor to get the sub-tensor for this cluster.\n",
        "        cluster_tensor = X_train[:, firm_indices, :]\n",
        "\n",
        "        # Save the sub-tensor and its corresponding mask for the next steps.\n",
        "        tensor_path = os.path.join(output_dir, f\"X_cluster_{cluster_id}.npz\")\n",
        "        np.savez_compressed(tensor_path, X=cluster_tensor)\n",
        "\n",
        "        mask_path = os.path.join(output_dir, f\"mask_cluster_{cluster_id}.npz\")\n",
        "        np.savez_compressed(mask_path, mask=cluster_mask)\n",
        "\n",
        "    logging.info(\"Steps 1, 2, 3: Density calculation, partitioning, and sub-tensor saving complete.\")\n",
        "\n",
        "    # --- Final Verification and Logging ---\n",
        "    # Verify that all clusters have been classified.\n",
        "    num_clusters = len(clusters)\n",
        "    if len(dense_cluster_ids) + len(sparse_cluster_ids) != num_clusters:\n",
        "        raise ValueError(\"Error in partitioning: Not all clusters were classified.\")\n",
        "\n",
        "    # Sort the lists for deterministic output.\n",
        "    dense_cluster_ids.sort()\n",
        "    sparse_cluster_ids.sort()\n",
        "\n",
        "    # Save the detailed density report to a JSON file.\n",
        "    report_path = os.path.join(output_dir, \"cluster_densities.json\")\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(density_report, f, indent=4)\n",
        "\n",
        "    logging.info(f\"Cluster density report saved to {report_path}\")\n",
        "    logging.info(f\"Found {len(dense_cluster_ids)} dense clusters: {dense_cluster_ids}\")\n",
        "    logging.info(f\"Found {len(sparse_cluster_ids)} sparse clusters: {sparse_cluster_ids}\")\n",
        "\n",
        "    return dense_cluster_ids, sparse_cluster_ids\n"
      ],
      "metadata": {
        "id": "yL4qI3YtmmIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Dense Cluster CP Completion (Unpenalized)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Dense Cluster CP Completion (Unpenalized)\n",
        "# ==============================================================================\n",
        "\n",
        "-----------------------------------------------------------------------\n",
        "# Task 13, Step 2: ALS solver for masked CP decomposition (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Helper Function for SVD Initialization\n",
        "# ------------------------------------------------------------------------------\n",
        "def _initialize_factors_svd(\n",
        "    tensor: np.ndarray,\n",
        "    rank: int\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Initializes factor matrices for CP decomposition using SVD of unfolded tensors.\n",
        "\n",
        "    This method provides a robust, deterministic starting point for the ALS algorithm,\n",
        "    often leading to faster convergence and better solutions than random initialization.\n",
        "\n",
        "    Args:\n",
        "        tensor: The 3D tensor to be decomposed, potentially with NaNs.\n",
        "        rank: The target rank R for the decomposition.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of three initialized factor matrices (U, V, W).\n",
        "    \"\"\"\n",
        "    T, N, L = tensor.shape\n",
        "    factors = []\n",
        "\n",
        "    # Fill NaNs with zeros for the purpose of initialization.\n",
        "    tensor_filled = np.nan_to_num(tensor, nan=0.0)\n",
        "\n",
        "    # Mode 0 (Time)\n",
        "    # Unfold the tensor into a T x (N*L) matrix.\n",
        "    unfolded_0 = tensor_filled.reshape(T, -1)\n",
        "    # Compute SVD and take the top R left singular vectors.\n",
        "    U, _, _ = np.linalg.svd(unfolded_0, full_matrices=False)\n",
        "    factors.append(U[:, :rank])\n",
        "\n",
        "    # Mode 1 (Firms)\n",
        "    # Unfold the tensor into an N x (T*L) matrix.\n",
        "    unfolded_1 = tensor_filled.transpose(1, 0, 2).reshape(N, -1)\n",
        "    U, _, _ = np.linalg.svd(unfolded_1, full_matrices=False)\n",
        "    factors.append(U[:, :rank])\n",
        "\n",
        "    # Mode 2 (Characteristics)\n",
        "    # Unfold the tensor into an L x (T*N) matrix.\n",
        "    unfolded_2 = tensor_filled.transpose(2, 0, 1).reshape(L, -1)\n",
        "    U, _, _ = np.linalg.svd(unfolded_2, full_matrices=False)\n",
        "    factors.append(U[:, :rank])\n",
        "\n",
        "    return factors[0], factors[1], factors[2]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Canonical ALS Solver (Replaces both previous versions)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _solve_cp_als(\n",
        "    tensor: np.ndarray,\n",
        "    mask: np.ndarray,\n",
        "    rank: int,\n",
        "    lambda_reg: float,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Solves for the CP decomposition of a masked tensor using regularized ALS.\n",
        "\n",
        "    This is the canonical, high-performance solver for the ACT-Tensor pipeline.\n",
        "    It incorporates:\n",
        "    1. Robust SVD-based initialization.\n",
        "    2. An iterative Alternating Least Squares procedure.\n",
        "    3. Correct implementation of the regularized normal equations for each update step.\n",
        "    4. Dual convergence criteria (max iterations and relative error improvement).\n",
        "    5. Optional factor column normalization for stability.\n",
        "\n",
        "    Args:\n",
        "        tensor: The 3D tensor to decompose, with NaNs for missing entries.\n",
        "        mask: A boolean array of the same shape as `tensor`.\n",
        "        rank: The target rank R for the CP decomposition.\n",
        "        lambda_reg: The L2 regularization strength (lambda).\n",
        "        config: The study's configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of three converged factor matrices (U, V, W).\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Get ALS hyperparameters from the configuration.\n",
        "    als_config = config[\"optimization_settings\"][\"cp_als\"]\n",
        "    cp_seed = config[\"reproducibility\"][\"seed_cp_als\"]\n",
        "    np.random.seed(cp_seed) # Seed for any stochasticity within ALS if any (none here)\n",
        "\n",
        "    # Initialize factor matrices using the robust SVD-based method.\n",
        "    U, V, W = _initialize_factors_svd(tensor, rank)\n",
        "\n",
        "    # Pre-compute the regularization term (lambda * I).\n",
        "    reg_term = lambda_reg * np.eye(rank)\n",
        "\n",
        "    # --- ALS Iteration Loop ---\n",
        "    reconstruction_error = []\n",
        "    for sweep in range(als_config[\"max_sweeps\"]):\n",
        "        # --- Update U (Time factor) ---\n",
        "        # This loop iterates through each time slice.\n",
        "        for t in range(tensor.shape[0]):\n",
        "            # Find observed (firm, char) indices in this time slice.\n",
        "            observed_indices = np.where(mask[t, :, :])\n",
        "            # If no data in this slice, skip the update.\n",
        "            if len(observed_indices[0]) == 0: continue\n",
        "\n",
        "            # Form the Khatri-Rao product matrix Z from the corresponding rows of V and W.\n",
        "            Z = V[observed_indices[0], :] * W[observed_indices[1], :]\n",
        "            # Get the vector of observed values y.\n",
        "            y = tensor[t, observed_indices[0], observed_indices[1]]\n",
        "\n",
        "            # Solve the regularized normal equations: (Z'Z + lambda*I)x = Z'y\n",
        "            gram_matrix = Z.T @ Z + reg_term\n",
        "            rhs_vector = Z.T @ y\n",
        "            U[t, :] = np.linalg.solve(gram_matrix, rhs_vector)\n",
        "\n",
        "        # --- Update V (Firm factor) ---\n",
        "        # This loop iterates through each firm slice.\n",
        "        for n in range(tensor.shape[1]):\n",
        "            observed_indices = np.where(mask[:, n, :])\n",
        "            if len(observed_indices[0]) == 0: continue\n",
        "\n",
        "            Z = U[observed_indices[0], :] * W[observed_indices[1], :]\n",
        "            y = tensor[observed_indices[0], n, observed_indices[1]]\n",
        "\n",
        "            gram_matrix = Z.T @ Z + reg_term\n",
        "            rhs_vector = Z.T @ y\n",
        "            V[n, :] = np.linalg.solve(gram_matrix, rhs_vector)\n",
        "\n",
        "        # --- Update W (Characteristic factor) ---\n",
        "        # This loop iterates through each characteristic slice.\n",
        "        for l in range(tensor.shape[2]):\n",
        "            observed_indices = np.where(mask[:, :, l])\n",
        "            if len(observed_indices[0]) == 0: continue\n",
        "\n",
        "            Z = U[observed_indices[0], :] * V[observed_indices[1], :]\n",
        "            y = tensor[observed_indices[0], observed_indices[1], l]\n",
        "\n",
        "            gram_matrix = Z.T @ Z + reg_term\n",
        "            rhs_vector = Z.T @ y\n",
        "            W[l, :] = np.linalg.solve(gram_matrix, rhs_vector)\n",
        "\n",
        "        # --- Normalization and Convergence Check ---\n",
        "        # Normalize factor columns to unit norm to control scaling and improve stability.\n",
        "        if als_config[\"column_normalization\"]:\n",
        "            for mat in [U, V, W]:\n",
        "                norms = np.linalg.norm(mat, axis=0)\n",
        "                # Avoid division by zero for columns that are all zero.\n",
        "                non_zero_norms = norms > 1e-9\n",
        "                mat[:, non_zero_norms] /= norms[non_zero_norms]\n",
        "\n",
        "        # Calculate reconstruction error on observed entries to check for convergence.\n",
        "        recon_tensor = np.einsum('tr,nr,lr->tnl', U, V, W)\n",
        "        error = np.linalg.norm((tensor - recon_tensor)[mask])\n",
        "        reconstruction_error.append(error)\n",
        "\n",
        "        # Check for convergence based on relative improvement in the error.\n",
        "        if sweep > 0 and reconstruction_error[-2] > 1e-9:\n",
        "            rel_improvement = abs(reconstruction_error[-2] - error) / reconstruction_error[-2]\n",
        "            if rel_improvement < als_config[\"tol_rel_improvement\"]:\n",
        "                logging.info(f\"ALS converged after {sweep + 1} sweeps with relative improvement {rel_improvement:.2e}.\")\n",
        "                break\n",
        "\n",
        "    if sweep == als_config[\"max_sweeps\"] - 1:\n",
        "        logging.warning(f\"ALS reached max sweeps ({als_config['max_sweeps']}) without converging.\")\n",
        "\n",
        "    return U, V, W\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def complete_dense_clusters(\n",
        "    dense_cluster_ids: List[int],\n",
        "    config: Dict[str, Any],\n",
        "    data_dir: str = \"data_processed\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the unpenalized CP completion for all dense clusters.\n",
        "\n",
        "    This function iterates through each 'dense' cluster, loads its data, and\n",
        "    calls the canonical `_solve_cp_als` solver with zero regularization to\n",
        "    perform the tensor completion.\n",
        "\n",
        "    Args:\n",
        "        dense_cluster_ids: A list of integer IDs for the dense clusters.\n",
        "        config: The study's configuration dictionary.\n",
        "        data_dir: Directory containing cluster data and for saving results.\n",
        "    \"\"\"\n",
        "    if not dense_cluster_ids:\n",
        "        logging.warning(\"No dense clusters to process. Skipping.\")\n",
        "        return\n",
        "\n",
        "    rank = config[\"imputation_hparams\"][\"CP_rank_R\"]\n",
        "\n",
        "    logging.info(f\"Starting CP completion for {len(dense_cluster_ids)} dense clusters...\")\n",
        "    for cluster_id in dense_cluster_ids:\n",
        "        start_time = time.time()\n",
        "        logging.info(f\"--- Processing dense cluster {cluster_id} ---\")\n",
        "\n",
        "        # Load the sub-tensor and sub-mask for the current cluster.\n",
        "        sub_tensor = np.load(os.path.join(data_dir, f\"X_cluster_{cluster_id}.npz\"))['X']\n",
        "        sub_mask = np.load(os.path.join(data_dir, f\"mask_cluster_{cluster_id}.npz\"))['mask']\n",
        "\n",
        "        # Call the new, unified ALS solver with lambda = 0.0 for unpenalized completion.\n",
        "        U, V, W = _solve_cp_als(\n",
        "            tensor=sub_tensor, mask=sub_mask, rank=rank, lambda_reg=0.0, config=config\n",
        "        )\n",
        "\n",
        "        # Reconstruct the full (dense) sub-tensor from the factor matrices.\n",
        "        completed_sub_tensor = np.einsum('tr,nr,lr->tnl', U, V, W)\n",
        "\n",
        "        # Save the completed sub-tensor.\n",
        "        np.savez_compressed(os.path.join(data_dir, f\"X_hat_cluster_{cluster_id}.npz\"), X_hat=completed_sub_tensor)\n",
        "\n",
        "        end_time = time.time()\n",
        "        logging.info(f\"Completed dense cluster {cluster_id}. Time: {end_time - start_time:.2f}s.\")\n"
      ],
      "metadata": {
        "id": "Q-c_KuIZnE1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Sparse Cluster Aggregation and Augmented CP Completion\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Sparse Cluster Aggregation and Augmented CP Completion\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def complete_sparse_clusters(\n",
        "    sparse_cluster_ids: List[int],\n",
        "    dense_cluster_ids: List[int],\n",
        "    clusters: Dict[int, List[int]],\n",
        "    X_train: np.ndarray,\n",
        "    mask_train: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    data_dir: str = \"data_processed\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the augmented CP completion for all sparse clusters.\n",
        "\n",
        "    This function implements the \"borrowing\" strategy by combining each sparse\n",
        "    cluster with all dense clusters, running the canonical regularized ALS solver,\n",
        "    and slicing back the results.\n",
        "\n",
        "    Args:\n",
        "        sparse_cluster_ids: A list of integer IDs for the sparse clusters.\n",
        "        dense_cluster_ids: A list of integer IDs for the dense clusters.\n",
        "        clusters: A dictionary mapping each cluster ID to a list of firm indices.\n",
        "        X_train: The full 3D training tensor.\n",
        "        mask_train: The full boolean training mask.\n",
        "        config: The study's configuration dictionary.\n",
        "        data_dir: Directory for saving completed tensors.\n",
        "    \"\"\"\n",
        "    if not sparse_cluster_ids:\n",
        "        logging.warning(\"No sparse clusters to process. Skipping.\")\n",
        "        return\n",
        "\n",
        "    rank = config[\"imputation_hparams\"][\"CP_rank_R\"]\n",
        "    lambda_reg = config[\"imputation_hparams\"][\"ridge_lambda\"]\n",
        "\n",
        "    # Use a set for efficient union of dense indices.\n",
        "    all_dense_indices = set(idx for cid in dense_cluster_ids for idx in clusters[cid])\n",
        "\n",
        "    logging.info(f\"Starting augmented CP completion for {len(sparse_cluster_ids)} sparse clusters...\")\n",
        "    for cluster_id in sparse_cluster_ids:\n",
        "        start_time = time.time()\n",
        "        logging.info(f\"--- Processing sparse cluster {cluster_id} ---\")\n",
        "\n",
        "        # --- Step 1: Form the aggregated tensor ---\n",
        "        sparse_indices = clusters[cluster_id]\n",
        "\n",
        "        # Use efficient set union and then sort for deterministic order.\n",
        "        aggregated_indices = sorted(list(set(sparse_indices) | all_dense_indices))\n",
        "\n",
        "        # Create mapping from original index to new aggregate index.\n",
        "        agg_idx_map = {orig_idx: new_idx for new_idx, orig_idx in enumerate(aggregated_indices)}\n",
        "\n",
        "        # Slice the full tensors to create the aggregated versions.\n",
        "        X_agg = X_train[:, aggregated_indices, :]\n",
        "        mask_agg = mask_train[:, aggregated_indices, :]\n",
        "\n",
        "        # --- Step 2: Solve using the canonical ALS solver ---\n",
        "        U, V_agg, W = _solve_cp_als(\n",
        "            tensor=X_agg, mask=mask_agg, rank=rank, lambda_reg=lambda_reg, config=config\n",
        "        )\n",
        "\n",
        "        # --- Step 3: Slice back the completed sparse cluster ---\n",
        "        completed_agg_tensor = np.einsum('tr,nr,lr->tnl', U, V_agg, W)\n",
        "\n",
        "        # Get the positions of the sparse firms within the aggregated tensor.\n",
        "        sparse_indices_in_agg = [agg_idx_map[orig_idx] for orig_idx in sparse_indices]\n",
        "\n",
        "        # Extract the completed data for the sparse cluster.\n",
        "        completed_sub_tensor = completed_agg_tensor[:, sparse_indices_in_agg, :]\n",
        "\n",
        "        # Save the result.\n",
        "        np.savez_compressed(os.path.join(data_dir, f\"X_hat_cluster_{cluster_id}.npz\"), X_hat=completed_sub_tensor)\n",
        "\n",
        "        end_time = time.time()\n",
        "        logging.info(f\"Completed sparse cluster {cluster_id}. Time: {end_time - start_time:.2f}s.\")\n"
      ],
      "metadata": {
        "id": "NwQNSqOZoG1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Global Assembly of Completed Cluster Sub-Tensors\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Global Assembly of Completed Cluster Sub-Tensors\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def assemble_completed_tensor(\n",
        "    clusters: Dict[int, List[int]],\n",
        "    tensor_shape: Tuple[int, int, int],\n",
        "    mask_train: np.ndarray,\n",
        "    X_train: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    data_dir: str = \"data_processed\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Assembles the globally completed tensor from individual cluster completions.\n",
        "\n",
        "    This function reverses the \"divide\" step by taking the imputed sub-tensors for\n",
        "    all clusters (both dense and sparse) and inserting them back into their\n",
        "    original positions within a full-sized tensor. This produces the first\n",
        "    version of the fully imputed data panel, before temporal smoothing.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. Initialize a new, empty tensor of the full (T, N, L) shape.\n",
        "    2. Iterate through all K clusters. For each cluster:\n",
        "       a. Load its completed sub-tensor from disk.\n",
        "       b. Retrieve the list of original firm indices belonging to this cluster.\n",
        "       c. Use advanced indexing to place the sub-tensor into the correct slice\n",
        "          of the global tensor.\n",
        "    3. Verify that the assembled tensor is complete (contains no NaNs).\n",
        "    4. Perform a sanity check by calculating the reconstruction error on the\n",
        "       training set.\n",
        "    5. Save the final assembled tensor to disk.\n",
        "\n",
        "    Args:\n",
        "        clusters: A dictionary mapping each cluster ID to a list of firm indices.\n",
        "        tensor_shape: A tuple (T, N, L) representing the full tensor dimensions.\n",
        "        mask_train: The boolean training mask of shape (T, N, L).\n",
        "        X_train: The original training tensor with NaNs.\n",
        "        config: The study's configuration dictionary.\n",
        "        data_dir: The directory where completed sub-tensors are stored and\n",
        "                  the final assembled tensor will be saved.\n",
        "\n",
        "    Returns:\n",
        "        The fully assembled (but not yet smoothed) 3D tensor of shape (T, N, L).\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If a completed sub-tensor for any cluster is missing.\n",
        "        ValueError: If the assembled tensor contains NaNs or if there is a\n",
        "                    shape mismatch during assembly.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(clusters, dict):\n",
        "        raise TypeError(\"Input 'clusters' must be a dictionary.\")\n",
        "    T, N, L = tensor_shape\n",
        "    if not (isinstance(T, int) and isinstance(N, int) and isinstance(L, int)):\n",
        "        raise TypeError(\"tensor_shape must be a tuple of three integers.\")\n",
        "\n",
        "    # --- Step 1: Initialize the global completed tensor ---\n",
        "    # Allocate a new array of the full tensor shape, initialized with zeros.\n",
        "    # The dtype is sourced from the config for consistency.\n",
        "    float_dtype = config[\"reproducibility\"][\"float_dtype\"]\n",
        "    X_hat_assembled = np.zeros(tensor_shape, dtype=float_dtype)\n",
        "    logging.info(f\"Step 1/3: Initialized global tensor with shape {tensor_shape}.\")\n",
        "\n",
        "    # --- Step 2: Insert each cluster's completed sub-tensor ---\n",
        "    num_clusters = len(clusters)\n",
        "    logging.info(f\"Assembling completed sub-tensors from {num_clusters} clusters...\")\n",
        "\n",
        "    # Iterate through all cluster IDs from 0 to K-1.\n",
        "    for cluster_id in range(num_clusters):\n",
        "        # Retrieve the list of original firm indices for this cluster.\n",
        "        firm_indices = clusters.get(cluster_id)\n",
        "        if firm_indices is None:\n",
        "            logging.warning(f\"Cluster ID {cluster_id} not found in clusters dictionary. Skipping.\")\n",
        "            continue\n",
        "        if not firm_indices:\n",
        "            logging.warning(f\"Cluster {cluster_id} is empty. Skipping assembly.\")\n",
        "            continue\n",
        "\n",
        "        # Load the completed sub-tensor for this cluster.\n",
        "        sub_tensor_path = os.path.join(data_dir, f\"X_hat_cluster_{cluster_id}.npz\")\n",
        "        try:\n",
        "            completed_sub_tensor = np.load(sub_tensor_path)['X_hat']\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Completed sub-tensor for cluster {cluster_id} not found at {sub_tensor_path}\")\n",
        "\n",
        "        # Verification: Check for shape consistency before assignment.\n",
        "        if completed_sub_tensor.shape != (T, len(firm_indices), L):\n",
        "            raise ValueError(\n",
        "                f\"Shape mismatch for cluster {cluster_id}. Expected \"\n",
        "                f\"{(T, len(firm_indices), L)}, but found {completed_sub_tensor.shape}.\"\n",
        "            )\n",
        "\n",
        "        # Use advanced integer indexing to place the sub-tensor into the global tensor.\n",
        "        # Formula: X_hat[:, I_k, :] = X_hat_k\n",
        "        X_hat_assembled[:, firm_indices, :] = completed_sub_tensor\n",
        "\n",
        "    logging.info(\"Step 2/3: All cluster sub-tensors have been assembled.\")\n",
        "\n",
        "    # --- Step 3: Verify completeness and save the global tensor ---\n",
        "    # Critical verification: The final assembled tensor must not contain any NaNs.\n",
        "    if np.isnan(X_hat_assembled).any():\n",
        "        nan_count = np.isnan(X_hat_assembled).sum()\n",
        "        raise ValueError(\n",
        "            f\"Assembly failed: The final tensor contains {nan_count} NaN values.\"\n",
        "        )\n",
        "    logging.info(\"Verification successful: Assembled tensor is fully dense (no NaNs).\")\n",
        "\n",
        "    # Sanity Check: Calculate the Root Mean Squared Error on the training set.\n",
        "    # This error should be small, indicating the model fits the observed data well.\n",
        "    # We use the training mask to select only the originally observed entries.\n",
        "    train_errors = (X_train[mask_train] - X_hat_assembled[mask_train])\n",
        "    train_rmse = np.sqrt(np.mean(train_errors**2))\n",
        "    logging.info(f\"Sanity Check: RMSE on training set (Omega_train): {train_rmse:.6f}\")\n",
        "\n",
        "    # Save the final assembled tensor to a compressed archive.\n",
        "    output_path = os.path.join(data_dir, \"X_hat_assembled.npz\")\n",
        "    np.savez_compressed(output_path, X_hat=X_hat_assembled)\n",
        "    logging.info(f\"Step 3/3: Assembled tensor checkpointed to {output_path}.\")\n",
        "\n",
        "    return X_hat_assembled\n"
      ],
      "metadata": {
        "id": "JlHIvUjZyfeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Temporal Smoothing via Centered Moving Average (CMA)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Temporal Smoothing via Centered Moving Average (CMA)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def apply_cma_smoothing(\n",
        "    X_hat_assembled: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Applies a Centered Moving Average (CMA) smoother to the imputed tensor.\n",
        "\n",
        "    This function is the first implementation of the temporal smoothing module from\n",
        "    the ACT-Tensor framework. It applies a symmetric moving average filter along the\n",
        "    time dimension (axis 0) for each firm-characteristic series in the tensor.\n",
        "    This process is designed to filter out short-lived noise while preserving\n",
        "    slow-moving fundamental trends.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. Retrieves the CMA window length (delta) from the configuration and validates it.\n",
        "    2. Uses the highly optimized `scipy.ndimage.uniform_filter1d` to apply the\n",
        "       moving average across all N*L time series in a single vectorized operation.\n",
        "       The boundary condition is handled to mimic a truncated window, as specified.\n",
        "    3. Verifies the output by ensuring it is dense and by computing a smoothness\n",
        "       diagnostic that confirms a reduction in series volatility.\n",
        "    4. Saves the final smoothed tensor to a compressed NumPy archive.\n",
        "\n",
        "    Args:\n",
        "        X_hat_assembled: The fully assembled but unsmoothed 3D tensor of shape\n",
        "                         (T, N, L) from Task 15.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: The directory to save the smoothed tensor and diagnostic logs.\n",
        "\n",
        "    Returns:\n",
        "        The smoothed 3D tensor `X_tilde_CMA` of shape (T, N, L).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the CMA window length `delta` is not a positive, odd integer.\n",
        "        TypeError: If the input is not a NumPy array.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(X_hat_assembled, np.ndarray) or X_hat_assembled.ndim != 3:\n",
        "        raise TypeError(\"Input 'X_hat_assembled' must be a 3D NumPy array.\")\n",
        "    if np.isnan(X_hat_assembled).any():\n",
        "        raise ValueError(\"Input 'X_hat_assembled' should not contain NaNs before smoothing.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Retrieve CMA parameters and define the smoothing window ---\n",
        "    # Extract the CMA window length from the configuration.\n",
        "    delta = config[\"smoothing_settings\"][\"CMA\"][\"delta\"]\n",
        "\n",
        "    # Validate that delta is a positive, odd integer for a symmetric CMA.\n",
        "    if not (isinstance(delta, int) and delta > 0 and delta % 2 == 1):\n",
        "        raise ValueError(\n",
        "            f\"CMA window length 'delta' must be a positive, odd integer, but got {delta}.\"\n",
        "        )\n",
        "    logging.info(f\"Step 1/3: Applying CMA with window length delta = {delta}.\")\n",
        "\n",
        "    # --- Step 2: Apply CMA to each firm–characteristic series ---\n",
        "    # Use scipy's uniform_filter1d for a highly efficient, vectorized implementation.\n",
        "    # This applies a 1D moving average filter along the specified axis (axis=0 for time).\n",
        "    # `mode='nearest'` handles boundaries by extending the array with the nearest\n",
        "    # value, which is arithmetically equivalent to truncating the window at the edges.\n",
        "    X_tilde_CMA = uniform_filter1d(\n",
        "        input=X_hat_assembled,\n",
        "        size=delta,\n",
        "        axis=0,\n",
        "        mode='nearest'\n",
        "    )\n",
        "    logging.info(\"Step 2/3: CMA filter applied to all time series.\")\n",
        "\n",
        "    # --- Step 3: Verify smoothness and save the CMA-smoothed tensor ---\n",
        "    # Verification 1: The output tensor must be fully dense (no NaNs).\n",
        "    if np.isnan(X_tilde_CMA).any():\n",
        "        raise RuntimeError(\"CMA smoothing resulted in a tensor with NaN values.\")\n",
        "\n",
        "    # Verification 2: Compute a smoothness diagnostic.\n",
        "    # Calculate the mean absolute month-to-month change before smoothing.\n",
        "    delta_before = np.mean(np.abs(np.diff(X_hat_assembled, axis=0)))\n",
        "\n",
        "    # Calculate the mean absolute month-to-month change after smoothing.\n",
        "    delta_after = np.mean(np.abs(np.diff(X_tilde_CMA, axis=0)))\n",
        "\n",
        "    # The 'after' value must be smaller, confirming that volatility was reduced.\n",
        "    if delta_after >= delta_before:\n",
        "        logging.warning(\n",
        "            f\"Smoothing did not reduce series volatility as expected. \"\n",
        "            f\"Mean abs change before: {delta_before:.6f}, after: {delta_after:.6f}\"\n",
        "        )\n",
        "    else:\n",
        "        logging.info(\n",
        "            f\"Smoothness diagnostic passed. Mean abs change reduced from \"\n",
        "            f\"{delta_before:.6f} to {delta_after:.6f}.\"\n",
        "        )\n",
        "\n",
        "    # Save the final smoothed tensor to a compressed archive.\n",
        "    output_path = os.path.join(output_dir, \"X_tilde_CMA.npz\")\n",
        "    np.savez_compressed(output_path, X_tilde=X_tilde_CMA)\n",
        "    logging.info(f\"Step 3/3: Smoothed tensor verified and saved to {output_path}.\")\n",
        "\n",
        "    return X_tilde_CMA\n"
      ],
      "metadata": {
        "id": "gJMJNo7OzCrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Temporal Smoothing via Exponential Moving Average (EMA)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Temporal Smoothing via Exponential Moving Average (EMA)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def apply_ema_smoothing(\n",
        "    X_hat_assembled: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Applies an Exponential Moving Average (EMA) smoother to the imputed tensor.\n",
        "\n",
        "    This function implements the EMA smoother, a one-sided, recursive filter that\n",
        "    gives exponentially decaying weights to past observations. It is more responsive\n",
        "    to recent data than a centered moving average. The implementation uses the\n",
        "    efficient `scipy.signal.lfilter` function, which is designed for applying\n",
        "    such IIR filters along an axis of a NumPy array.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. Retrieves the EMA smoothing factor (theta) from the configuration and validates it.\n",
        "    2. Defines the filter coefficients corresponding to the EMA recursion formula.\n",
        "    3. Correctly initializes the filter's state to match the paper's prescription\n",
        "       that the first smoothed value equals the first observed value.\n",
        "    4. Applies the filter along the time axis (axis=0) for all N*L series.\n",
        "    5. Verifies the output and saves the final smoothed tensor.\n",
        "\n",
        "    Args:\n",
        "        X_hat_assembled: The fully assembled but unsmoothed 3D tensor of shape\n",
        "                         (T, N, L) from Task 15.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: The directory to save the smoothed tensor.\n",
        "\n",
        "    Returns:\n",
        "        The smoothed 3D tensor `X_tilde_EMA` of shape (T, N, L).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the EMA smoothing factor `theta` is not in the interval (0, 1).\n",
        "        TypeError: If the input is not a NumPy array.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the input is a valid, dense NumPy array.\n",
        "    if not isinstance(X_hat_assembled, np.ndarray) or X_hat_assembled.ndim != 3:\n",
        "        raise TypeError(\"Input 'X_hat_assembled' must be a 3D NumPy array.\")\n",
        "    if np.isnan(X_hat_assembled).any():\n",
        "        raise ValueError(\"Input 'X_hat_assembled' should not contain NaNs before smoothing.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Retrieve EMA parameters and define filter ---\n",
        "    # Extract the EMA smoothing factor from the configuration.\n",
        "    theta = config[\"smoothing_settings\"][\"EMA\"][\"theta\"]\n",
        "\n",
        "    # Validate that theta is a float in the valid interval (0, 1).\n",
        "    if not (isinstance(theta, float) and 0.0 < theta < 1.0):\n",
        "        raise ValueError(\n",
        "            f\"EMA smoothing factor 'theta' must be a float in (0, 1), but got {theta}.\"\n",
        "        )\n",
        "    logging.info(f\"Step 1/3: Applying EMA with smoothing factor theta = {theta}.\")\n",
        "\n",
        "    # The EMA recursion is: y[t] = theta*x[t] + (1-theta)*y[t-1]\n",
        "    # This is an IIR filter. In scipy's lfilter form, y[t] is on the LHS and\n",
        "    # x[t] is on the RHS. The equation is: y[t] - (1-theta)*y[t-1] = theta*x[t].\n",
        "    # This corresponds to filter coefficients:\n",
        "    # a = [1, -(1-theta)]  (feedback coefficients for y)\n",
        "    # b = [theta]           (feedforward coefficients for x)\n",
        "    b = np.array([theta])\n",
        "    a = np.array([1, -(1 - theta)])\n",
        "\n",
        "    # --- Step 2: Apply EMA recursion to each firm–characteristic series ---\n",
        "    # To initialize the filter such that y[0] = x[0], we must set the initial\n",
        "    # state of the filter's internal memory (`zi`). `lfiltic` computes this state.\n",
        "    # The initial state zi is such that if the input is x[0] repeated, the output is y[0]=x[0].\n",
        "    # We need one initial state for each of the N*L time series.\n",
        "    # The first observation for all series is the first time slice of the tensor.\n",
        "    x0 = X_hat_assembled[0, :, :]\n",
        "    # `lfiltic` computes the initial state `zi` for a step response.\n",
        "    # We scale it by x0 to match the initial condition y[0]=x[0].\n",
        "    zi = lfiltic(b, a, y=[]) * x0[np.newaxis, :, :]\n",
        "\n",
        "    # Apply the linear filter `lfilter` along the time axis (axis=0).\n",
        "    # The function takes the coefficients `b` and `a`, the input tensor, the axis,\n",
        "    # and the initial state `zi`. It returns the filtered output.\n",
        "    X_tilde_EMA, _ = lfilter(b, a, X_hat_assembled, axis=0, zi=zi)\n",
        "    logging.info(\"Step 2/3: EMA filter applied to all time series.\")\n",
        "\n",
        "    # --- Step 3: Verify and save the EMA-smoothed tensor ---\n",
        "    # Verification 1: The output tensor must be fully dense.\n",
        "    if np.isnan(X_tilde_EMA).any():\n",
        "        raise RuntimeError(\"EMA smoothing resulted in a tensor with NaN values.\")\n",
        "\n",
        "    # Verification 2: Compute the smoothness diagnostic.\n",
        "    # Calculate the mean absolute month-to-month change before smoothing.\n",
        "    delta_before = np.mean(np.abs(np.diff(X_hat_assembled, axis=0)))\n",
        "    # Calculate the mean absolute month-to-month change after smoothing.\n",
        "    delta_after = np.mean(np.abs(np.diff(X_tilde_EMA, axis=0)))\n",
        "\n",
        "    # The 'after' value should be smaller, confirming a reduction in volatility.\n",
        "    if delta_after >= delta_before:\n",
        "        logging.warning(\n",
        "            f\"Smoothing did not reduce series volatility as expected. \"\n",
        "            f\"Mean abs change before: {delta_before:.6f}, after: {delta_after:.6f}\"\n",
        "        )\n",
        "    else:\n",
        "        logging.info(\n",
        "            f\"Smoothness diagnostic passed. Mean abs change reduced from \"\n",
        "            f\"{delta_before:.6f} to {delta_after:.6f}.\"\n",
        "        )\n",
        "\n",
        "    # Save the final smoothed tensor to a compressed archive.\n",
        "    output_path = os.path.join(output_dir, \"X_tilde_EMA.npz\")\n",
        "    np.savez_compressed(output_path, X_tilde=X_tilde_EMA)\n",
        "    logging.info(f\"Step 3/3: Smoothed tensor verified and saved to {output_path}.\")\n",
        "\n",
        "    return X_tilde_EMA\n"
      ],
      "metadata": {
        "id": "Q1wOlnFXzq-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Temporal Smoothing via Kalman Filter and Rauch–Tung–Striebel Smoother\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Temporal Smoothing via Kalman Filter and RTS Smoother\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Step 2: Vectorized Kalman Filter & Smoother (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _kalman_smoother_pass(\n",
        "    observations: np.ndarray,\n",
        "    h: float,\n",
        "    r: float\n",
        ") -> Tuple[np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Performs a full Kalman filter and RTS smoother pass and computes the log-likelihood.\n",
        "\n",
        "    This function implements the forward (filter) and backward (smoother) passes\n",
        "    for a local level (random walk) state-space model. The entire operation\n",
        "    is vectorized to run efficiently on all N*L time series simultaneously.\n",
        "\n",
        "    Crucially, it also calculates the exact log-likelihood of the observations\n",
        "    given the model parameters (h, r) using the prediction error decomposition from\n",
        "    the forward pass. This is the statistically principled objective function for\n",
        "    hyperparameter tuning.\n",
        "\n",
        "    Args:\n",
        "        observations: The (T, N, L) tensor of imputed values (x_hat).\n",
        "        h: The process noise variance (state transition variance).\n",
        "        r: The measurement noise variance.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - y_smooth (np.ndarray): A (T, N, L) tensor of smoothed state estimates.\n",
        "        - total_log_likelihood (float): The sum of log-likelihoods over all\n",
        "          series and time steps.\n",
        "    \"\"\"\n",
        "    # Get tensor dimensions.\n",
        "    T, N, L = observations.shape\n",
        "\n",
        "    # --- Kalman Filter (Forward Pass) & Log-Likelihood Calculation ---\n",
        "    # Initialize arrays to store filtered states and their variances.\n",
        "    y_filt = np.zeros_like(observations)\n",
        "    P_filt = np.zeros_like(observations)\n",
        "    # Store predicted variances for the backward pass.\n",
        "    P_pred = np.zeros_like(observations)\n",
        "    # Initialize total log-likelihood.\n",
        "    total_log_likelihood = 0.0\n",
        "\n",
        "    # Initialization of the filter at t=0.\n",
        "    # The initial state estimate is the first observation.\n",
        "    y_filt[0] = observations[0]\n",
        "    # The initial variance of the state estimate is the measurement variance.\n",
        "    P_filt[0] = r\n",
        "\n",
        "    # Iterate from t=1 to T-1.\n",
        "    for t in range(1, T):\n",
        "        # --- Predict Step ---\n",
        "        # State prediction: y_t|t-1 = y_t-1|t-1\n",
        "        y_pred_t = y_filt[t - 1]\n",
        "        # Variance prediction: P_t|t-1 = P_t-1|t-1 + h\n",
        "        P_pred_t = P_filt[t - 1] + h\n",
        "        P_pred[t] = P_pred_t  # Store for the smoother pass\n",
        "\n",
        "        # --- Log-Likelihood Calculation ---\n",
        "        # Prediction error (innovation): v_t = x_t - y_t|t-1\n",
        "        prediction_error = observations[t] - y_pred_t\n",
        "        # Prediction error variance: F_t = P_t|t-1 + r\n",
        "        prediction_error_var = P_pred_t + r\n",
        "\n",
        "        # Log-likelihood increment for time t (summed over N*L series).\n",
        "        # Formula: log L_t = -0.5 * [log(2*pi) + log(F_t) + v_t^2 / F_t]\n",
        "        log_likelihood_increment = -0.5 * (\n",
        "            np.log(2 * np.pi) + np.log(prediction_error_var) +\n",
        "            (prediction_error**2) / prediction_error_var\n",
        "        )\n",
        "        total_log_likelihood += np.sum(log_likelihood_increment)\n",
        "\n",
        "        # --- Update Step ---\n",
        "        # Kalman Gain: K_t = P_t|t-1 / F_t\n",
        "        kalman_gain = P_pred_t / prediction_error_var\n",
        "        # State update: y_t|t = y_t|t-1 + K_t * v_t\n",
        "        y_filt[t] = y_pred_t + kalman_gain * prediction_error\n",
        "        # Variance update: P_t|t = (1 - K_t) * P_t|t-1\n",
        "        P_filt[t] = (1 - kalman_gain) * P_pred_t\n",
        "\n",
        "    # --- RTS Smoother (Backward Pass) ---\n",
        "    # Initialize arrays for smoothed states.\n",
        "    y_smooth = np.zeros_like(observations)\n",
        "    # The final smoothed state is the final filtered state.\n",
        "    y_smooth[-1] = y_filt[-1]\n",
        "\n",
        "    # Iterate backwards from t=T-2 down to 0.\n",
        "    for t in range(T - 2, -1, -1):\n",
        "        # Smoother Gain: J_t = P_t|t / P_t+1|t\n",
        "        # Add a small epsilon to the denominator for numerical stability.\n",
        "        smoother_gain = P_filt[t] / (P_pred[t + 1] + 1e-9)\n",
        "        # Smoothed state update: y_t|T = y_t|t + J_t * (y_t+1|T - y_t+1|t)\n",
        "        # Note: y_{t+1|t} is the prediction for t+1, which is y_filt[t].\n",
        "        y_smooth[t] = y_filt[t] + smoother_gain * (y_smooth[t + 1] - y_filt[t])\n",
        "\n",
        "    # Return both the smoothed series and the total log-likelihood.\n",
        "    return y_smooth, total_log_likelihood\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def apply_kf_smoothing(\n",
        "    X_hat_assembled: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Applies a Kalman Filter/Smoother with MLE-based hyperparameter tuning.\n",
        "\n",
        "    This function implements the most statistically rigorous temporal smoother. It\n",
        "    treats each time series as a noisy observation of a latent random walk. It\n",
        "    performs a grid search over the process (h) and measurement (r) noise\n",
        "    variances, selecting the pair that **maximizes the log-likelihood** of the\n",
        "    data. It then applies the smoother with these optimal parameters.\n",
        "\n",
        "    Args:\n",
        "        X_hat_assembled: The fully assembled, dense 3D tensor from Task 15.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: The directory to save the smoothed tensor and logs.\n",
        "\n",
        "    Returns:\n",
        "        The smoothed 3D tensor `X_tilde_KF` of shape (T, N, L).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(X_hat_assembled, np.ndarray) or X_hat_assembled.ndim != 3:\n",
        "        raise TypeError(\"Input 'X_hat_assembled' must be a 3D NumPy array.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Set up State-Space Model and Hyperparameter Grid ---\n",
        "    # Retrieve the hyperparameter grids from the configuration.\n",
        "    kf_config = config[\"smoothing_settings\"][\"KF\"]\n",
        "    h_grid = kf_config[\"process_var_h_grid\"]\n",
        "    r_grid = kf_config[\"meas_var_r_grid\"]\n",
        "\n",
        "    # Initialize variables to track the best parameters found.\n",
        "    best_h, best_r = None, None\n",
        "    # We now maximize log-likelihood, so we initialize to negative infinity.\n",
        "    max_log_likelihood = -np.inf\n",
        "\n",
        "    logging.info(f\"Step 1/3: Starting grid search for Kalman smoother via Maximum Likelihood.\")\n",
        "\n",
        "    # --- Step 2: Grid Search to Maximize Log-Likelihood ---\n",
        "    # Iterate through all combinations of h and r.\n",
        "    for h in h_grid:\n",
        "        for r in r_grid:\n",
        "            # Run a full smoother pass and get the total log-likelihood.\n",
        "            _, log_likelihood = _kalman_smoother_pass(X_hat_assembled, h, r)\n",
        "\n",
        "            # If this is the best log-likelihood so far, store the parameters.\n",
        "            if log_likelihood > max_log_likelihood:\n",
        "                max_log_likelihood = log_likelihood\n",
        "                best_h, best_r = h, r\n",
        "\n",
        "    logging.info(\n",
        "        f\"Step 2/3: Grid search complete. Optimal parameters via MLE: \"\n",
        "        f\"h={best_h}, r={best_r} (Max Log-Likelihood={max_log_likelihood:.2f}).\"\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Apply the Selected Smoother and Save ---\n",
        "    # Run the smoother one final time with the optimal hyperparameters.\n",
        "    X_tilde_KF, _ = _kalman_smoother_pass(X_hat_assembled, best_h, best_r)\n",
        "\n",
        "    # Verification 1: Check for NaNs in the final output.\n",
        "    if np.isnan(X_tilde_KF).any():\n",
        "        raise RuntimeError(\"Kalman smoothing resulted in a tensor with NaN values.\")\n",
        "\n",
        "    # Verification 2: Compute smoothness diagnostic.\n",
        "    delta_before = np.mean(np.abs(np.diff(X_hat_assembled, axis=0)))\n",
        "    delta_after = np.mean(np.abs(np.diff(X_tilde_KF, axis=0)))\n",
        "    logging.info(\n",
        "        f\"Smoothness diagnostic: Mean abs change reduced from \"\n",
        "        f\"{delta_before:.6f} to {delta_after:.6f}.\"\n",
        "    )\n",
        "\n",
        "    # Save the final smoothed tensor.\n",
        "    output_path = os.path.join(output_dir, \"X_tilde_KF.npz\")\n",
        "    np.savez_compressed(output_path, X_tilde=X_tilde_KF)\n",
        "\n",
        "    # Log the selected hyperparameters and the maximized log-likelihood.\n",
        "    hyperparam_log_path = os.path.join(output_dir, \"kf_hyperparameters.json\")\n",
        "    with open(hyperparam_log_path, 'w') as f:\n",
        "        json.dump(\n",
        "            {\"best_h\": best_h, \"best_r\": best_r, \"max_log_likelihood\": max_log_likelihood},\n",
        "            f, indent=4\n",
        "        )\n",
        "\n",
        "    logging.info(f\"Step 3/3: Final smoothed tensor and optimal hyperparameters saved.\")\n",
        "\n",
        "    # Return the final smoothed tensor.\n",
        "    return X_tilde_KF\n"
      ],
      "metadata": {
        "id": "dhhevXyb_f0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Imputation Accuracy Evaluation on Held-Out Masks\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Imputation Accuracy Evaluation on Held-Out Masks\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def evaluate_imputation_accuracy(\n",
        "    tensor_true: np.ndarray,\n",
        "    tensor_imputed: np.ndarray,\n",
        "    mask_eval: np.ndarray\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes imputation accuracy metrics on a held-out evaluation set.\n",
        "\n",
        "    This function quantifies the statistical accuracy of an imputation method by\n",
        "    comparing the imputed values against the known ground-truth values on a\n",
        "    predefined test set. It calculates four standard metrics: Root Mean Squared\n",
        "    Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE),\n",
        "    and the coefficient of determination (R-squared).\n",
        "\n",
        "    Args:\n",
        "        tensor_true: The original, ground-truth 3D tensor, including all\n",
        "                     originally observed values.\n",
        "        tensor_imputed: The 3D tensor after imputation and smoothing, fully dense.\n",
        "        mask_eval: A boolean 3D tensor of the same shape, where `True` indicates\n",
        "                   the entries to be used for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the four computed accuracy metrics:\n",
        "        'RMSE_imp', 'MAE_imp', 'MAPE_imp', 'R2_imp'.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input arrays have mismatched shapes or if the\n",
        "                    evaluation mask is empty.\n",
        "        TypeError: If inputs are not NumPy arrays.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that all inputs are NumPy arrays.\n",
        "    if not all(isinstance(arr, np.ndarray) for arr in [tensor_true, tensor_imputed, mask_eval]):\n",
        "        raise TypeError(\"All inputs must be NumPy arrays.\")\n",
        "\n",
        "    # Verify that the shapes of all input arrays are identical.\n",
        "    if not (tensor_true.shape == tensor_imputed.shape == mask_eval.shape):\n",
        "        raise ValueError(\"All input arrays must have the same shape.\")\n",
        "\n",
        "    # Verify that the evaluation mask is boolean.\n",
        "    if mask_eval.dtype != bool:\n",
        "        raise TypeError(\"'mask_eval' must be a boolean array.\")\n",
        "\n",
        "    # --- Step 1: Extract the held-out ground truth and imputed values ---\n",
        "    # Use the boolean evaluation mask to select the relevant values from the tensors.\n",
        "    # This creates 1D arrays of the ground-truth and imputed values for comparison.\n",
        "    x_true = tensor_true[mask_eval]\n",
        "    x_imputed = tensor_imputed[mask_eval]\n",
        "\n",
        "    # Check if the evaluation set is empty.\n",
        "    if x_true.size == 0:\n",
        "        logging.warning(\"Evaluation mask is empty. No metrics can be computed.\")\n",
        "        return {\n",
        "            'RMSE_imp': np.nan,\n",
        "            'MAE_imp': np.nan,\n",
        "            'MAPE_imp': np.nan,\n",
        "            'R2_imp': np.nan\n",
        "        }\n",
        "\n",
        "    logging.info(f\"Step 1/2: Extracted {x_true.size} data points for evaluation.\")\n",
        "\n",
        "    # --- Step 2: Compute the four imputation metrics ---\n",
        "    # Calculate the difference between true and imputed values.\n",
        "    errors = x_true - x_imputed\n",
        "\n",
        "    # Metric 1: Root Mean Squared Error (RMSE)\n",
        "    # Formula: RMSE_imp = sqrt(mean((x - x_hat)^2))\n",
        "    rmse_imp = np.sqrt(np.mean(errors**2))\n",
        "\n",
        "    # Metric 2: Mean Absolute Error (MAE)\n",
        "    # Formula: MAE_imp = mean(|x - x_hat|)\n",
        "    mae_imp = np.mean(np.abs(errors))\n",
        "\n",
        "    # Metric 3: Mean Absolute Percentage Error (MAPE)\n",
        "    # This requires special handling to avoid division by zero.\n",
        "    # We create a mask to exclude true values that are very close to zero.\n",
        "    mape_mask = np.abs(x_true) > 1e-6\n",
        "    if np.any(mape_mask):\n",
        "        # Formula: MAPE_imp = mean(|(x - x_hat) / x|)\n",
        "        mape_imp = np.mean(np.abs(errors[mape_mask] / x_true[mape_mask]))\n",
        "    else:\n",
        "        # If all true values are zero, MAPE is undefined.\n",
        "        mape_imp = np.nan\n",
        "\n",
        "    # Metric 4: R-squared (Coefficient of Determination)\n",
        "    # Formula: R2_imp = 1 - (sum((x - x_hat)^2) / sum((x - mean(x))^2))\n",
        "    sum_sq_errors = np.sum(errors**2)\n",
        "    # Calculate the total sum of squares relative to the mean of the true values.\n",
        "    sum_sq_total = np.sum((x_true - np.mean(x_true))**2)\n",
        "\n",
        "    # Avoid division by zero if the true data has no variance.\n",
        "    if sum_sq_total > 1e-9:\n",
        "        r2_imp = 1 - (sum_sq_errors / sum_sq_total)\n",
        "    else:\n",
        "        # If total variance is zero, R-squared is undefined or can be set to 1 if errors are also zero.\n",
        "        r2_imp = 1.0 if np.isclose(sum_sq_errors, 0) else np.nan\n",
        "\n",
        "    logging.info(\"Step 2/2: All four imputation metrics computed.\")\n",
        "\n",
        "    # --- Finalization: Compile and return results ---\n",
        "    # Store the computed metrics in a dictionary.\n",
        "    results = {\n",
        "        'RMSE_imp': rmse_imp,\n",
        "        'MAE_imp': mae_imp,\n",
        "        'MAPE_imp': mape_imp,\n",
        "        'R2_imp': r2_imp\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "ct-rdztBAZdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Sparse-Cluster Stress-Test Imputation Metrics\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Sparse-Cluster Stress-Test Imputation Metrics\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def evaluate_sparse_cluster_accuracy(\n",
        "    tensor_true: np.ndarray,\n",
        "    tensor_imputed: np.ndarray,\n",
        "    mask_eval: np.ndarray,\n",
        "    clusters: Dict[int, List[int]],\n",
        "    sparse_cluster_ids: List[int]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluates imputation accuracy specifically on the held-out entries of sparse clusters.\n",
        "\n",
        "    This function provides a stress test for the imputation model by focusing the\n",
        "    evaluation on the most challenging subset of the data: firms with the least\n",
        "    amount of observed data. It isolates the performance of the model's\n",
        "    \"borrowing\" mechanism.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. Identifies all firm indices that belong to clusters classified as 'sparse'.\n",
        "    2. Creates a new evaluation mask that is a subset of the main evaluation\n",
        "       mask, containing only the entries corresponding to these sparse firms.\n",
        "    3. Calls the canonical `evaluate_imputation_accuracy` function (from Task 19)\n",
        "       using this new, more restrictive mask.\n",
        "    4. Returns the computed metrics, providing a focused view on performance\n",
        "       under extreme data sparsity.\n",
        "\n",
        "    Args:\n",
        "        tensor_true: The original, ground-truth 3D tensor.\n",
        "        tensor_imputed: The 3D tensor after imputation and smoothing.\n",
        "        mask_eval: The main boolean evaluation mask for the entire test set.\n",
        "        clusters: A dictionary mapping each cluster ID to a list of firm indices.\n",
        "        sparse_cluster_ids: A list of integer IDs for the sparse clusters.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the four accuracy metrics computed only on the\n",
        "        sparse cluster subset. The keys are suffixed with '_sparse'.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If input shapes are inconsistent or if sparse cluster IDs\n",
        "                    are invalid.\n",
        "        TypeError: If inputs are not of the expected types.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Basic type and shape checks.\n",
        "    if not all(isinstance(arr, np.ndarray) for arr in [tensor_true, tensor_imputed, mask_eval]):\n",
        "        raise TypeError(\"All tensor/mask inputs must be NumPy arrays.\")\n",
        "    if not (tensor_true.shape == tensor_imputed.shape == mask_eval.shape):\n",
        "        raise ValueError(\"All input arrays must have the same shape.\")\n",
        "    if not isinstance(clusters, dict) or not isinstance(sparse_cluster_ids, list):\n",
        "        raise TypeError(\"'clusters' must be a dict and 'sparse_cluster_ids' a list.\")\n",
        "\n",
        "    # --- Step 1: Identify held-out entries within sparse clusters ---\n",
        "    # Get the total number of firms from the tensor shape.\n",
        "    N = tensor_true.shape[1]\n",
        "\n",
        "    # Aggregate all firm indices from the list of sparse clusters.\n",
        "    # Using a set is efficient for collecting unique indices.\n",
        "    all_sparse_indices = set()\n",
        "    for cid in sparse_cluster_ids:\n",
        "        if cid not in clusters:\n",
        "            raise ValueError(f\"Sparse cluster ID {cid} not found in clusters dictionary.\")\n",
        "        all_sparse_indices.update(clusters[cid])\n",
        "\n",
        "    # Convert the set of sparse firm indices to a sorted list for deterministic indexing.\n",
        "    all_sparse_indices_list = sorted(list(all_sparse_indices))\n",
        "\n",
        "    # Create a boolean vector of length N, where True indicates a sparse firm.\n",
        "    is_sparse_firm = np.zeros(N, dtype=bool)\n",
        "    if all_sparse_indices_list: # Avoid error if list is empty\n",
        "        is_sparse_firm[all_sparse_indices_list] = True\n",
        "\n",
        "    # Create the sparse evaluation mask using broadcasting.\n",
        "    # A (t, n, l) entry is in the sparse test set if:\n",
        "    # 1. It was in the original evaluation set (mask_eval[t, n, l] is True)\n",
        "    # 2. The firm `n` belongs to a sparse cluster (is_sparse_firm[n] is True)\n",
        "    # The broadcasting `[np.newaxis, :, np.newaxis]` aligns the (N,) firm vector\n",
        "    # with the (T, N, L) mask for the bitwise AND operation.\n",
        "    mask_eval_sparse = mask_eval & is_sparse_firm[np.newaxis, :, np.newaxis]\n",
        "\n",
        "    num_sparse_eval_points = mask_eval_sparse.sum()\n",
        "    num_total_eval_points = mask_eval.sum()\n",
        "\n",
        "    logging.info(\n",
        "        f\"Step 1/2: Created sparse evaluation mask. It contains {num_sparse_eval_points} \"\n",
        "        f\"points ({num_sparse_eval_points / num_total_eval_points:.2%} of the total test set).\"\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Compute imputation metrics on the sparse subset ---\n",
        "    # Reuse the canonical evaluation function from Task 19. This is a critical\n",
        "    # best practice to ensure metrics are computed consistently.\n",
        "    sparse_metrics = evaluate_imputation_accuracy(\n",
        "        tensor_true=tensor_true,\n",
        "        tensor_imputed=tensor_imputed,\n",
        "        mask_eval=mask_eval_sparse\n",
        "    )\n",
        "    logging.info(\"Step 2/2: Computed imputation metrics on the sparse subset.\")\n",
        "\n",
        "    # --- Step 3: Compile and return the sparse-cluster metrics ---\n",
        "    # Rename the keys to clearly indicate they are for the sparse stress test.\n",
        "    results_sparse = {f\"{key}_sparse\": value for key, value in sparse_metrics.items()}\n",
        "\n",
        "    return results_sparse\n"
      ],
      "metadata": {
        "id": "o1LYKzBaBFqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Implement Baseline Imputation Methods\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Implement Baseline Imputation Methods\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Cross-sectional Median Imputation (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _impute_cross_sectional_median(\n",
        "    X_train: np.ndarray,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Imputes missing values using the cross-sectional median for each time period.\n",
        "\n",
        "    This function implements a simple but common baseline. For each characteristic\n",
        "    at each point in time, it calculates the median of all observed values across\n",
        "    firms and uses this median to fill any missing entries for that characteristic\n",
        "    at that time.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L) with NaNs.\n",
        "        time_to_idx: Mapping from timestamp to integer index.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "\n",
        "    Returns:\n",
        "        A fully imputed (dense) NumPy array of shape (T, N, L).\n",
        "    \"\"\"\n",
        "    # Log the start of the process.\n",
        "    logging.info(\"--- Starting Cross-Sectional Median Imputation ---\")\n",
        "\n",
        "    # Get the dimensions of the input tensor for reshaping.\n",
        "    T, N, L = X_train.shape\n",
        "\n",
        "    # Create a DataFrame representation for easier groupby operations.\n",
        "    # The tensor is reshaped to 2D and given a MultiIndex for panel operations.\n",
        "    df = pd.DataFrame(\n",
        "        X_train.reshape(T * N, L),\n",
        "        index=pd.MultiIndex.from_product(\n",
        "            [time_to_idx.keys(), firm_to_idx.keys()], names=['date', 'permno']\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Group the DataFrame by date (the first level of the index).\n",
        "    # For each group (i.e., for each month), apply a transformation.\n",
        "    # The lambda function fills NaN values in each column with that column's median for the group.\n",
        "    df_imputed = df.groupby(level='date').transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "    # As a fallback, if an entire characteristic column for a given month is NaN,\n",
        "    # its median will also be NaN. We fill any such remaining NaNs with 0.0,\n",
        "    # which is a neutral value for the rank-normalized data.\n",
        "    df_imputed.fillna(0.0, inplace=True)\n",
        "\n",
        "    # Reshape the now-dense DataFrame back to the original (T, N, L) tensor format.\n",
        "    X_hat_median = df_imputed.to_numpy().reshape(T, N, L)\n",
        "\n",
        "    # Log the completion of the process.\n",
        "    logging.info(\"Cross-Sectional Median Imputation complete.\")\n",
        "\n",
        "    # Return the imputed tensor.\n",
        "    return X_hat_median\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 2: Global Bidirectional Fill + XS Regression (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _impute_global_bf_xs(\n",
        "    X_train: np.ndarray,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Imputes missing values using a two-stage global method.\n",
        "\n",
        "    Stage 1: Bidirectional temporal fill (forward then backward) for each\n",
        "             firm's time series.\n",
        "    Stage 2: For any remaining missing values, a cross-sectional Ridge\n",
        "             regression is fitted for each characteristic at each time period,\n",
        "             using all other characteristics as predictors.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L) with NaNs.\n",
        "        time_to_idx: Mapping from timestamp to integer index.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "\n",
        "    Returns:\n",
        "        A fully imputed (dense) NumPy array of shape (T, N, L).\n",
        "    \"\"\"\n",
        "    # Log the start of the process.\n",
        "    logging.info(\"--- Starting Global BF-XS Imputation ---\")\n",
        "\n",
        "    # Get tensor dimensions.\n",
        "    T, N, L = X_train.shape\n",
        "\n",
        "    # Create a DataFrame representation for panel operations.\n",
        "    df = pd.DataFrame(\n",
        "        X_train.reshape(T * N, L),\n",
        "        index=pd.MultiIndex.from_product(\n",
        "            [time_to_idx.keys(), firm_to_idx.keys()], names=['date', 'permno']\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Stage 1: Bidirectional temporal fill.\n",
        "    # Group by firm (permno) and apply forward-fill then backward-fill.\n",
        "    df_filled = df.groupby(level='permno').transform(lambda x: x.ffill().bfill())\n",
        "\n",
        "    # Stage 2: Cross-sectional ridge regression for any remaining NaNs.\n",
        "    # Iterate through each time period.\n",
        "    for date in time_to_idx.keys():\n",
        "        # Extract the data slice for the current month.\n",
        "        monthly_slice = df_filled.loc[date]\n",
        "\n",
        "        # If there are no missing values in this slice, we can skip it.\n",
        "        if monthly_slice.isnull().sum().sum() == 0:\n",
        "            continue\n",
        "\n",
        "        # Iterate through each characteristic (column).\n",
        "        for l in range(L):\n",
        "            # Check if the current characteristic has any missing values this month.\n",
        "            if monthly_slice.iloc[:, l].isnull().any():\n",
        "                # Isolate the target column (y) and predictor columns (X).\n",
        "                col_l = monthly_slice.iloc[:, l]\n",
        "                predictors = monthly_slice.drop(columns=monthly_slice.columns[l])\n",
        "\n",
        "                # Split the data into a training set (where y is observed)\n",
        "                # and a prediction set (where y is missing).\n",
        "                is_observed = col_l.notna()\n",
        "                X_train_reg = predictors[is_observed]\n",
        "                y_train_reg = col_l[is_observed]\n",
        "                X_predict_reg = predictors[~is_observed]\n",
        "\n",
        "                # Skip if there's no data to train on or no values to predict.\n",
        "                if y_train_reg.empty or X_predict_reg.empty:\n",
        "                    continue\n",
        "\n",
        "                # Handle potential NaNs in the predictor matrices by filling with the median.\n",
        "                # This is crucial as regression models cannot handle missing inputs.\n",
        "                train_medians = X_train_reg.median()\n",
        "                X_train_reg = X_train_reg.fillna(train_medians)\n",
        "                X_predict_reg = X_predict_reg.fillna(train_medians)\n",
        "\n",
        "                # Initialize and fit a Ridge regression model.\n",
        "                model = Ridge(alpha=1.0)\n",
        "                model.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "                # Predict the missing values.\n",
        "                predictions = model.predict(X_predict_reg)\n",
        "\n",
        "                # Place the predictions back into the main DataFrame.\n",
        "                df_filled.loc[(date, X_predict_reg.index), df.columns[l]] = predictions\n",
        "\n",
        "    # Final fallback: If any NaNs still remain (e.g., a firm had all predictors missing),\n",
        "    # fill them with the cross-sectional median.\n",
        "    df_filled = df_filled.groupby(level='date').transform(lambda x: x.fillna(x.median()))\n",
        "    # A final global fill with 0.0 for any completely empty time slices.\n",
        "    df_filled.fillna(0.0, inplace=True)\n",
        "\n",
        "    # Reshape back to the original tensor format.\n",
        "    X_hat_bf_xs = df_filled.to_numpy().reshape(T, N, L)\n",
        "\n",
        "    # Log completion.\n",
        "    logging.info(\"Global BF-XS Imputation complete.\")\n",
        "\n",
        "    # Return the imputed tensor.\n",
        "    return X_hat_bf_xs\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Local Backward Fill + Rolling XS Regression (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _impute_local_b_xs(\n",
        "    X_train: np.ndarray,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Imputes missing values using a two-stage local method.\n",
        "\n",
        "    This is a computationally intensive baseline.\n",
        "    Stage 1: Backward-only temporal fill for each firm's time series.\n",
        "    Stage 2: For each month, a cross-sectional Ridge regression is fitted for\n",
        "             each characteristic using data from a trailing window of months.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L) with NaNs.\n",
        "        time_to_idx: Mapping from timestamp to integer index.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "\n",
        "    Returns:\n",
        "        A fully imputed (dense) NumPy array of shape (T, N, L).\n",
        "    \"\"\"\n",
        "    # Log the start of the process.\n",
        "    logging.info(\"--- Starting Local B-XS Imputation ---\")\n",
        "\n",
        "    # Get tensor dimensions.\n",
        "    T, N, L = X_train.shape\n",
        "\n",
        "    # Create a DataFrame representation.\n",
        "    df = pd.DataFrame(\n",
        "        X_train.reshape(T * N, L),\n",
        "        index=pd.MultiIndex.from_product(\n",
        "            [time_to_idx.keys(), firm_to_idx.keys()], names=['date', 'permno']\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Stage 1: Backward-only temporal fill.\n",
        "    df_filled = df.groupby(level='permno').transform(lambda x: x.bfill())\n",
        "\n",
        "    # Stage 2: Rolling-window cross-sectional ridge regression.\n",
        "    # A 24-month window is a standard choice in the literature for such tasks.\n",
        "    window_size = 24\n",
        "    dates = list(time_to_idx.keys())\n",
        "\n",
        "    # Iterate through each time period to make predictions for that month.\n",
        "    for t_idx, date in enumerate(dates):\n",
        "        # Get the data slice for the current month.\n",
        "        monthly_slice = df_filled.loc[date]\n",
        "\n",
        "        # Skip if no NaNs to impute this month.\n",
        "        if monthly_slice.isnull().sum().sum() == 0:\n",
        "            continue\n",
        "\n",
        "        # Define the start of the rolling window for training data.\n",
        "        start_idx = max(0, t_idx - window_size + 1)\n",
        "        window_dates = dates[start_idx : t_idx + 1]\n",
        "        # Extract the full data block for the rolling window.\n",
        "        rolling_data = df_filled.loc[window_dates]\n",
        "\n",
        "        # Iterate through each characteristic to fit a model.\n",
        "        for l in range(L):\n",
        "            # Check if there are missing values to predict for this characteristic.\n",
        "            if monthly_slice.iloc[:, l].isnull().any():\n",
        "                # Isolate the target column (y) and predictors (X) within the window.\n",
        "                col_l_rolling = rolling_data.iloc[:, l]\n",
        "                predictors_rolling = rolling_data.drop(columns=rolling_data.columns[l])\n",
        "\n",
        "                # Get the training data (observed y) from the entire window.\n",
        "                is_obs_rolling = col_l_rolling.notna()\n",
        "                X_train_reg = predictors_rolling[is_obs_rolling]\n",
        "                y_train_reg = col_l_rolling[is_obs_rolling]\n",
        "\n",
        "                # The data to predict on is only the missing entries in the current month.\n",
        "                is_missing_current_month = monthly_slice.iloc[:, l].isnull()\n",
        "                X_predict_reg = monthly_slice.drop(columns=monthly_slice.columns[l])[is_missing_current_month]\n",
        "\n",
        "                # Skip if no training data or no values to predict.\n",
        "                if y_train_reg.empty or X_predict_reg.empty:\n",
        "                    continue\n",
        "\n",
        "                # Handle NaNs in predictor matrices.\n",
        "                train_medians = X_train_reg.median()\n",
        "                X_train_reg = X_train_reg.fillna(train_medians)\n",
        "                X_predict_reg = X_predict_reg.fillna(train_medians)\n",
        "\n",
        "                # Fit the Ridge model on the rolling window data.\n",
        "                model = Ridge(alpha=1.0)\n",
        "                model.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "                # Predict the missing values for the current month.\n",
        "                predictions = model.predict(X_predict_reg)\n",
        "\n",
        "                # Place predictions back into the main DataFrame.\n",
        "                df_filled.loc[(date, X_predict_reg.index), df.columns[l]] = predictions\n",
        "\n",
        "    # Final fallback for any remaining NaNs.\n",
        "    df_filled = df_filled.groupby(level='date').transform(lambda x: x.fillna(x.median()))\n",
        "    df_filled.fillna(0.0, inplace=True)\n",
        "\n",
        "    # Reshape back to tensor format.\n",
        "    X_hat_b_xs = df_filled.to_numpy().reshape(T, N, L)\n",
        "\n",
        "    # Log completion.\n",
        "    logging.info(\"Local B-XS Imputation complete.\")\n",
        "\n",
        "    # Return the imputed tensor.\n",
        "    return X_hat_b_xs\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_baseline_imputations(\n",
        "    X_train: np.ndarray,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the imputation of missing data using several baseline methods.\n",
        "\n",
        "    This function runs a suite of benchmark imputation models to provide a\n",
        "    performance comparison for the main ACT-Tensor framework. Each baseline\n",
        "    is implemented in a modular helper function and its output is saved to disk.\n",
        "\n",
        "    The implemented baselines are:\n",
        "    1. Cross-Sectional Median: Fills NaNs with the monthly median.\n",
        "    2. Global BF-XS: Bidirectional fill followed by a global cross-sectional regression.\n",
        "    3. Local B-XS: Backward fill followed by a rolling-window cross-sectional regression.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L) with NaNs.\n",
        "        time_to_idx: Mapping from timestamp to integer index.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "        output_dir: Directory to save the imputed tensors from each baseline.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are the names of the baseline methods and\n",
        "        values are the corresponding fully imputed (dense) tensors.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the main input is a 3D NumPy array.\n",
        "    if not isinstance(X_train, np.ndarray) or X_train.ndim != 3:\n",
        "        raise TypeError(\"Input 'X_train' must be a 3D NumPy array.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize a dictionary to hold the results.\n",
        "    imputed_tensors = {}\n",
        "\n",
        "    # --- Run Baseline 1: Cross-Sectional Median ---\n",
        "    # Call the helper function to perform the imputation.\n",
        "    X_hat_median = _impute_cross_sectional_median(X_train, time_to_idx, firm_to_idx)\n",
        "    # Save the resulting tensor to a compressed file.\n",
        "    np.savez_compressed(os.path.join(output_dir, \"X_hat_Median.npz\"), X_hat=X_hat_median)\n",
        "    # Store the result in the output dictionary.\n",
        "    imputed_tensors[\"Median\"] = X_hat_median\n",
        "\n",
        "    # --- Run Baseline 2: Global BF-XS ---\n",
        "    # Call the helper function.\n",
        "    X_hat_bf_xs = _impute_global_bf_xs(X_train, time_to_idx, firm_to_idx)\n",
        "    # Save the result.\n",
        "    np.savez_compressed(os.path.join(output_dir, \"X_hat_BF_XS.npz\"), X_hat=X_hat_bf_xs)\n",
        "    # Store the result.\n",
        "    imputed_tensors[\"BF-XS\"] = X_hat_bf_xs\n",
        "\n",
        "    # --- Run Baseline 3: Local B-XS ---\n",
        "    # Call the helper function.\n",
        "    X_hat_b_xs = _impute_local_b_xs(X_train, time_to_idx, firm_to_idx)\n",
        "    # Save the result.\n",
        "    np.savez_compressed(os.path.join(output_dir, \"X_hat_B_XS.npz\"), X_hat=X_hat_b_xs)\n",
        "    # Store the result.\n",
        "    imputed_tensors[\"B-XS\"] = X_hat_b_xs\n",
        "\n",
        "    # Log the completion of all baseline methods.\n",
        "    logging.info(\"All baseline imputation methods have been executed and saved.\")\n",
        "\n",
        "    # Return the dictionary of imputed tensors.\n",
        "    return imputed_tensors\n"
      ],
      "metadata": {
        "id": "g7ZR8mvIBpj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Evaluate Baseline Imputation Methods on Held-Out Masks\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Evaluate Baseline Imputation Methods on Held-Out Masks\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def evaluate_baseline_methods(\n",
        "    tensor_true: np.ndarray,\n",
        "    regime: str,\n",
        "    baseline_methods: List[str],\n",
        "    data_dir: str = \"data_processed\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Evaluates the imputation accuracy of all baseline methods on a held-out mask.\n",
        "\n",
        "    This function orchestrates the evaluation process for the benchmark models.\n",
        "    For a given missingness regime, it loads the corresponding evaluation mask and\n",
        "    the imputed tensors from each baseline method. It then systematically computes\n",
        "    the standard set of four accuracy metrics for each baseline by calling the\n",
        "    canonical `evaluate_imputation_accuracy` function.\n",
        "\n",
        "    Args:\n",
        "        tensor_true: The original, ground-truth 3D tensor.\n",
        "        regime: The missingness regime to evaluate (e.g., 'MAR', 'Block', 'Logit').\n",
        "                This determines which evaluation mask is used.\n",
        "        baseline_methods: A list of baseline method names to evaluate,\n",
        "                          e.g., ['Median', 'BF_XS', 'B_XS'].\n",
        "        data_dir: The directory where the evaluation mask and imputed baseline\n",
        "                  tensors are stored.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary contains the evaluation\n",
        "        results for one baseline method, suitable for creating a results table.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the evaluation mask or any of the baseline\n",
        "                           imputed tensors cannot be found.\n",
        "        ValueError: If there is a shape mismatch between the loaded arrays.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the main input is a 3D NumPy array.\n",
        "    if not isinstance(tensor_true, np.ndarray) or tensor_true.ndim != 3:\n",
        "        raise TypeError(\"Input 'tensor_true' must be a 3D NumPy array.\")\n",
        "\n",
        "    # --- Step 1: Load Evaluation Mask ---\n",
        "    # Construct the path to the evaluation mask for the specified regime.\n",
        "    eval_mask_path = os.path.join(data_dir, f\"mask_eval_{regime}.npz\")\n",
        "    try:\n",
        "        # Load the compressed numpy file containing the mask.\n",
        "        mask_eval = np.load(eval_mask_path)['mask']\n",
        "        logging.info(f\"Successfully loaded evaluation mask for regime '{regime}'.\")\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Evaluation mask for regime '{regime}' not found at {eval_mask_path}\")\n",
        "\n",
        "    # Validate that the loaded mask has the correct shape.\n",
        "    if mask_eval.shape != tensor_true.shape:\n",
        "        raise ValueError(\n",
        "            f\"Shape of evaluation mask for regime '{regime}' ({mask_eval.shape}) \"\n",
        "            f\"does not match ground-truth tensor shape ({tensor_true.shape}).\"\n",
        "        )\n",
        "\n",
        "    # Initialize a list to store the results for all baselines.\n",
        "    all_results = []\n",
        "\n",
        "    # --- Step 2 & 3: Compute Metrics for Each Baseline and Compile Results ---\n",
        "    # Iterate through the list of specified baseline methods.\n",
        "    for method_name in baseline_methods:\n",
        "        logging.info(f\"--- Evaluating baseline method: {method_name} ---\")\n",
        "\n",
        "        # Construct the path to the imputed tensor for the current baseline.\n",
        "        imputed_tensor_path = os.path.join(data_dir, f\"X_hat_{method_name}.npz\")\n",
        "        try:\n",
        "            # Load the imputed tensor.\n",
        "            tensor_imputed = np.load(imputed_tensor_path)['X_hat']\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(\n",
        "                f\"Imputed tensor for baseline '{method_name}' not found at {imputed_tensor_path}\"\n",
        "            )\n",
        "\n",
        "        # Validate the shape of the loaded imputed tensor.\n",
        "        if tensor_imputed.shape != tensor_true.shape:\n",
        "            raise ValueError(\n",
        "                f\"Shape of imputed tensor for '{method_name}' ({tensor_imputed.shape}) \"\n",
        "                f\"does not match ground-truth tensor shape ({tensor_true.shape}).\"\n",
        "            )\n",
        "\n",
        "        # Call the canonical evaluation function from Task 19 to compute metrics.\n",
        "        # This ensures consistent metric calculation across all models.\n",
        "        metrics = evaluate_imputation_accuracy(\n",
        "            tensor_true=tensor_true,\n",
        "            tensor_imputed=tensor_imputed,\n",
        "            mask_eval=mask_eval\n",
        "        )\n",
        "\n",
        "        # Log the computed metrics for the current baseline.\n",
        "        logging.info(f\"Results for {method_name}: {metrics}\")\n",
        "\n",
        "        # Compile the results into a structured dictionary for this baseline.\n",
        "        result_row = {\n",
        "            \"Method\": method_name,\n",
        "            \"Regime\": regime,\n",
        "            **metrics  # Unpack the metrics dictionary\n",
        "        }\n",
        "\n",
        "        # Append the result to the master list.\n",
        "        all_results.append(result_row)\n",
        "\n",
        "    # Log the completion of the evaluation process.\n",
        "    logging.info(f\"Evaluation complete for all baseline methods in regime '{regime}'.\")\n",
        "\n",
        "    # Return the compiled list of results.\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "BryOcRG3DBgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23: Ablation Study—CP Completion Only (No Clustering, No Smoothing)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Ablation Study—CP Completion Only (No Clustering, No Smoothing)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_ablation_cp_only(\n",
        "    X_train: np.ndarray,\n",
        "    mask_train: np.ndarray,\n",
        "    tensor_true: np.ndarray,\n",
        "    mask_eval: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Runs the \"CP Completion Only\" ablation study.\n",
        "\n",
        "    This function serves as a baseline within the tensor framework. It performs\n",
        "    a global CP decomposition on the entire training tensor without any prior\n",
        "    clustering or subsequent temporal smoothing. The purpose is to isolate the\n",
        "    performance of the core tensor factorization algorithm itself, allowing for\n",
        "    a clear measurement of the incremental benefits of the clustering and\n",
        "    smoothing modules.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. Calls the canonical `_solve_cp_als` solver on the full training tensor\n",
        "       with zero regularization.\n",
        "    2. Reconstructs the completed tensor from the resulting factor matrices.\n",
        "    3. Saves the imputed tensor as a distinct artifact for this ablation study.\n",
        "    4. Calls the canonical `evaluate_imputation_accuracy` function to compute\n",
        "       the performance metrics on the held-out evaluation set.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L) with NaNs.\n",
        "        mask_train: The boolean training mask of shape (T, N, L).\n",
        "        tensor_true: The original, ground-truth 3D tensor.\n",
        "        mask_eval: The boolean evaluation mask for the test set.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the imputed tensor from this ablation run.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the four computed accuracy metrics for this\n",
        "        ablation condition.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that all tensor/mask inputs are valid NumPy arrays with matching shapes.\n",
        "    if not all(isinstance(arr, np.ndarray) and arr.ndim == 3 for arr in [X_train, mask_train, tensor_true, mask_eval]):\n",
        "        raise TypeError(\"All tensor/mask inputs must be 3D NumPy arrays.\")\n",
        "    if not (X_train.shape == mask_train.shape == tensor_true.shape == mask_eval.shape):\n",
        "        raise ValueError(\"All input arrays must have the same shape.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    logging.info(\"--- Starting Ablation Study: CP Completion Only ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Set up and run the global masked CP decomposition ---\n",
        "    # Get the CP rank from the configuration.\n",
        "    rank = config[\"imputation_hparams\"][\"CP_rank_R\"]\n",
        "\n",
        "    # Call the canonical ALS solver on the full training tensor.\n",
        "    # For this baseline, regularization is turned off (lambda_reg = 0.0).\n",
        "    logging.info(f\"Running global CP decomposition with rank R={rank}...\")\n",
        "    U, V, W = _solve_cp_als(\n",
        "        tensor=X_train,\n",
        "        mask=mask_train,\n",
        "        rank=rank,\n",
        "        lambda_reg=0.0,  # No regularization for this baseline\n",
        "        config=config\n",
        "    )\n",
        "    logging.info(\"Step 1/3: Global CP decomposition complete.\")\n",
        "\n",
        "    # --- Step 2: Reconstruct the completed tensor ---\n",
        "    # Reconstruct the full, dense tensor from the converged factor matrices.\n",
        "    # Formula: X_hat = [[U, V, W]]\n",
        "    X_hat_cp_only = np.einsum('tr,nr,lr->tnl', U, V, W)\n",
        "\n",
        "    # Save the imputed tensor as a specific artifact for this ablation study.\n",
        "    output_path = os.path.join(output_dir, \"X_hat_Ablation_CP_Only.npz\")\n",
        "    np.savez_compressed(output_path, X_hat=X_hat_cp_only)\n",
        "    logging.info(f\"Step 2/3: Reconstructed tensor saved to {output_path}.\")\n",
        "\n",
        "    # --- Step 3: Evaluate and log ablation metrics ---\n",
        "    # Call the canonical evaluation function to compute accuracy metrics.\n",
        "    # This ensures a fair comparison with all other methods.\n",
        "    metrics = evaluate_imputation_accuracy(\n",
        "        tensor_true=tensor_true,\n",
        "        tensor_imputed=X_hat_cp_only,\n",
        "        mask_eval=mask_eval\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Step 3/3: Evaluation complete. Time elapsed: {end_time - start_time:.2f} seconds.\")\n",
        "    logging.info(f\"Results for CP-Only Ablation: {metrics}\")\n",
        "\n",
        "    # Return the computed metrics.\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "PmofixnXDftL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24: Ablation Study—CP Completion with Clustering (No Smoothing)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Ablation Study—CP Completion with Clustering (No Smoothing)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_ablation_cp_with_clustering(\n",
        "    X_train: np.ndarray,\n",
        "    mask_train: np.ndarray,\n",
        "    tensor_true: np.ndarray,\n",
        "    mask_eval: np.ndarray,\n",
        "    firm_to_idx: Dict[int, int],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Runs the \"CP Completion with Clustering\" ablation study.\n",
        "\n",
        "    This function evaluates the performance of the core cluster-based imputation\n",
        "    module of ACT-Tensor, deliberately omitting the final temporal smoothing step.\n",
        "    It allows for the direct measurement of the value added by modeling firm\n",
        "    heterogeneity through clustering.\n",
        "\n",
        "    The process is a sequential execution of previously defined pipeline stages:\n",
        "    1. Clusters firms based on their data profiles (`cluster_firms_by_profile`).\n",
        "    2. Computes cluster densities and partitions them (`compute_densities_and_partition`).\n",
        "    3. Imputes dense clusters using unpenalized CP-ALS (`complete_dense_clusters`).\n",
        "    4. Imputes sparse clusters using the augmented CP-ALS method (`complete_sparse_clusters`).\n",
        "    5. Assembles the completed sub-tensors into a single global tensor (`assemble_completed_tensor`).\n",
        "    6. Evaluates the accuracy of this unsmoothed tensor against the ground truth.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L) with NaNs.\n",
        "        mask_train: The boolean training mask of shape (T, N, L).\n",
        "        tensor_true: The original, ground-truth 3D tensor.\n",
        "        mask_eval: The boolean evaluation mask for the test set.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save intermediate and final artifacts.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the four computed accuracy metrics for this\n",
        "        ablation condition.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure all tensor/mask inputs are valid NumPy arrays with matching shapes.\n",
        "    if not all(isinstance(arr, np.ndarray) and arr.ndim == 3 for arr in [X_train, mask_train, tensor_true, mask_eval]):\n",
        "        raise TypeError(\"All tensor/mask inputs must be 3D NumPy arrays.\")\n",
        "    if not (X_train.shape == mask_train.shape == tensor_true.shape == mask_eval.shape):\n",
        "        raise ValueError(\"All input arrays must have the same shape.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    logging.info(\"--- Starting Ablation Study: CP Completion with Clustering ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Reuse cluster assignments and perform cluster-wise CP completion ---\n",
        "    # This sequence reuses the modular functions from the main pipeline.\n",
        "\n",
        "    # 1a. Cluster firms.\n",
        "    clusters = cluster_firms_by_profile(X_train, firm_to_idx, config, output_dir)\n",
        "\n",
        "    # 1b. Partition clusters into dense and sparse.\n",
        "    dense_ids, sparse_ids = compute_densities_and_partition(\n",
        "        X_train, mask_train, clusters, config, output_dir\n",
        "    )\n",
        "\n",
        "    # 1c. Impute dense clusters.\n",
        "    complete_dense_clusters(dense_ids, config, output_dir)\n",
        "\n",
        "    # 1d. Impute sparse clusters.\n",
        "    complete_sparse_clusters(\n",
        "        sparse_ids, dense_ids, clusters, X_train, mask_train, config, output_dir\n",
        "    )\n",
        "    logging.info(\"Step 1/3: Cluster-wise CP completion finished.\")\n",
        "\n",
        "    # --- Step 2: Assemble the completed (unsmoothed) tensor ---\n",
        "    # Reuse the canonical assembly function.\n",
        "    X_hat_clustered = assemble_completed_tensor(\n",
        "        clusters=clusters,\n",
        "        tensor_shape=X_train.shape,\n",
        "        mask_train=mask_train,\n",
        "        X_train=X_train,\n",
        "        config=config,\n",
        "        data_dir=output_dir\n",
        "    )\n",
        "\n",
        "    # Save the assembled tensor as a specific artifact for this ablation study.\n",
        "    output_path = os.path.join(output_dir, \"X_hat_Ablation_CP_Clustering.npz\")\n",
        "    np.savez_compressed(output_path, X_hat=X_hat_clustered)\n",
        "    logging.info(f\"Step 2/3: Assembled (unsmoothed) tensor saved to {output_path}.\")\n",
        "\n",
        "    # --- Step 3: Evaluate and log ablation metrics ---\n",
        "    # Call the canonical evaluation function on the unsmoothed, clustered result.\n",
        "    metrics = evaluate_imputation_accuracy(\n",
        "        tensor_true=tensor_true,\n",
        "        tensor_imputed=X_hat_clustered,\n",
        "        mask_eval=mask_eval\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Step 3/3: Evaluation complete. Time elapsed: {end_time - start_time:.2f} seconds.\")\n",
        "    logging.info(f\"Results for CP-with-Clustering Ablation: {metrics}\")\n",
        "\n",
        "    # Return the computed metrics.\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "UqxypcP3EAg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25: Ablation Study—CP Completion with Smoothing (No Clustering)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Ablation Study—CP Completion with Smoothing (No Clustering)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 25, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_ablation_cp_with_smoothing(\n",
        "    X_train: np.ndarray,\n",
        "    mask_train: np.ndarray,\n",
        "    tensor_true: np.ndarray,\n",
        "    mask_eval: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Runs the \"CP Completion with Smoothing\" ablation study.\n",
        "\n",
        "    This function evaluates the performance of a simplified model that consists of\n",
        "    a global CP decomposition followed by temporal smoothing, but without any\n",
        "    firm clustering. Its purpose is to isolate the incremental performance gain\n",
        "    attributable to the temporal smoothing module alone.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. Runs (or loads the pre-computed result of) the \"CP-only\" ablation to get\n",
        "       the globally imputed, unsmoothed tensor.\n",
        "    2. Applies the canonical Centered Moving Average (CMA) smoother to this tensor.\n",
        "    3. Saves the resulting smoothed tensor as a distinct artifact.\n",
        "    4. Evaluates the accuracy of this smoothed tensor against the ground truth\n",
        "       using the canonical evaluation function.\n",
        "\n",
        "    Args:\n",
        "        X_train: The 3D training tensor of shape (T, N, L) with NaNs.\n",
        "        mask_train: The boolean training mask of shape (T, N, L).\n",
        "        tensor_true: The original, ground-truth 3D tensor.\n",
        "        mask_eval: The boolean evaluation mask for the test set.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the imputed tensor from this ablation run.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the four computed accuracy metrics for this\n",
        "        ablation condition.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure all tensor/mask inputs are valid NumPy arrays with matching shapes.\n",
        "    if not all(isinstance(arr, np.ndarray) and arr.ndim == 3 for arr in [X_train, mask_train, tensor_true, mask_eval]):\n",
        "        raise TypeError(\"All tensor/mask inputs must be 3D NumPy arrays.\")\n",
        "    if not (X_train.shape == mask_train.shape == tensor_true.shape == mask_eval.shape):\n",
        "        raise ValueError(\"All input arrays must have the same shape.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    logging.info(\"--- Starting Ablation Study: CP Completion with Smoothing ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Step 1: Obtain the globally imputed (unsmoothed) tensor ---\n",
        "    # For efficiency, we first check if the result from the \"CP-only\" ablation\n",
        "    # already exists. If not, we run it. This avoids re-computation.\n",
        "    cp_only_path = os.path.join(output_dir, \"X_hat_Ablation_CP_Only.npz\")\n",
        "    if not os.path.exists(cp_only_path):\n",
        "        logging.info(\"CP-Only artifact not found. Running 'run_ablation_cp_only' first...\")\n",
        "        # This assumes the function from Task 23 is available and works correctly.\n",
        "        run_ablation_cp_only(\n",
        "            X_train, mask_train, tensor_true, mask_eval, config, output_dir\n",
        "        )\n",
        "\n",
        "    # Load the pre-computed globally imputed tensor.\n",
        "    X_hat_cp_only = np.load(cp_only_path)['X_hat']\n",
        "    logging.info(f\"Step 1/3: Loaded globally imputed tensor from {cp_only_path}.\")\n",
        "\n",
        "    # --- Step 2: Apply CMA smoothing ---\n",
        "    # Pass the globally imputed tensor to the canonical CMA smoothing function.\n",
        "    # This reuses the exact same smoothing logic as the full ACT-Tensor model.\n",
        "    X_tilde_cp_cma = apply_cma_smoothing(\n",
        "        X_hat_assembled=X_hat_cp_only,\n",
        "        config=config,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    # Save the result under a new name specific to this ablation study.\n",
        "    output_path = os.path.join(output_dir, \"X_hat_Ablation_CP_CMA.npz\")\n",
        "    np.savez_compressed(output_path, X_hat=X_tilde_cp_cma)\n",
        "    logging.info(f\"Step 2/3: Smoothed tensor saved to {output_path}.\")\n",
        "\n",
        "    # --- Step 3: Evaluate and log ablation metrics ---\n",
        "    # Call the canonical evaluation function on the smoothed, unclustered result.\n",
        "    metrics = evaluate_imputation_accuracy(\n",
        "        tensor_true=tensor_true,\n",
        "        tensor_imputed=X_tilde_cp_cma,\n",
        "        mask_eval=mask_eval\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Step 3/3: Evaluation complete. Time elapsed: {end_time - start_time:.2f} seconds.\")\n",
        "    logging.info(f\"Results for CP-with-Smoothing Ablation: {metrics}\")\n",
        "\n",
        "    # Return the computed metrics.\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "mqIQTpr5EoiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26: Compile Ablation Results Table (Panel C)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Compile Ablation Results Table (Panel C)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 26, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compile_ablation_results(\n",
        "    ablation_results: List[Dict[str, Any]],\n",
        "    output_dir: str = \"results\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compiles, verifies, and saves the results of the ablation studies.\n",
        "\n",
        "    This function takes the metric dictionaries from the various ablation runs\n",
        "    (CP-Only, CP+Clustering, CP+Smoothing) and the full ACT-Tensor model, and\n",
        "    aggregates them into a single, publication-ready DataFrame. It performs\n",
        "    sanity checks to verify the expected performance improvements from each\n",
        "    module and saves the final table in multiple formats.\n",
        "\n",
        "    Args:\n",
        "        ablation_results: A list of dictionaries, where each dictionary contains\n",
        "                          the results for one model configuration. Each dict\n",
        "                          should have keys like 'Method', 'Regime', 'RMSE_imp', etc.\n",
        "        output_dir: The directory to save the final results tables.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the compiled and formatted ablation results.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input list is empty or malformed.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the input is a non-empty list of dictionaries.\n",
        "    if not isinstance(ablation_results, list) or not ablation_results:\n",
        "        raise ValueError(\"Input 'ablation_results' must be a non-empty list of dictionaries.\")\n",
        "    if not all(isinstance(item, dict) for item in ablation_results):\n",
        "        raise TypeError(\"All items in 'ablation_results' must be dictionaries.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    logging.info(\"--- Compiling Ablation Study Results ---\")\n",
        "\n",
        "    # --- Step 1: Aggregate ablation metrics into a DataFrame ---\n",
        "    # Convert the list of result dictionaries directly into a pandas DataFrame.\n",
        "    results_df = pd.DataFrame(ablation_results)\n",
        "\n",
        "    # Define the expected methods for the ablation study panel.\n",
        "    expected_methods = [\n",
        "        \"CP Completion\",\n",
        "        \"CP Completion w/ Clustering\",\n",
        "        \"CP Completion w/ CMA\",\n",
        "        \"ACT-Tensor w/ CMA\"\n",
        "    ]\n",
        "\n",
        "    # Set 'Method' and 'Regime' as a MultiIndex for easier slicing and analysis.\n",
        "    results_df.set_index(['Method', 'Regime'], inplace=True)\n",
        "    results_df.sort_index(inplace=True)\n",
        "\n",
        "    logging.info(\"Step 1/3: Aggregated ablation metrics into a DataFrame.\")\n",
        "\n",
        "    # --- Step 2: Verify incremental improvements ---\n",
        "    # For each regime, check if adding modules improves performance (lower RMSE).\n",
        "    for regime in results_df.index.get_level_values('Regime').unique():\n",
        "        try:\n",
        "            # Get RMSE for each model in the current regime.\n",
        "            rmse_cp_only = results_df.loc[(\"CP Completion\", regime), 'RMSE_imp']\n",
        "            rmse_cp_clust = results_df.loc[(\"CP Completion w/ Clustering\", regime), 'RMSE_imp']\n",
        "            rmse_cp_cma = results_df.loc[(\"CP Completion w/ CMA\", regime), 'RMSE_imp']\n",
        "            rmse_act_tensor = results_df.loc[(\"ACT-Tensor w/ CMA\", regime), 'RMSE_imp']\n",
        "\n",
        "            # Verify that the full model is the best.\n",
        "            # A small tolerance is added for floating point inconsistencies.\n",
        "            assert rmse_act_tensor <= (rmse_cp_clust + 1e-9)\n",
        "            assert rmse_act_tensor <= (rmse_cp_cma + 1e-9)\n",
        "\n",
        "            # Verify that adding either module is better than CP-only.\n",
        "            assert rmse_cp_clust <= (rmse_cp_only + 1e-9)\n",
        "            assert rmse_cp_cma <= (rmse_cp_only + 1e-9)\n",
        "\n",
        "            logging.info(f\"Verification successful for regime '{regime}': Performance improves with added modules.\")\n",
        "\n",
        "        except (KeyError, AssertionError) as e:\n",
        "            # If the expected ordering does not hold, log a warning. This is a\n",
        "            # critical finding that deviates from the paper's results.\n",
        "            logging.warning(\n",
        "                f\"Verification failed for regime '{regime}'. Expected performance \"\n",
        "                f\"ordering does not hold. Error: {e}\"\n",
        "            )\n",
        "            # Note: For rigorous academic work, statistical significance tests\n",
        "            # (e.g., bootstrapping Diebold-Mariano) would be applied here to\n",
        "            # confirm if these differences are statistically meaningful.\n",
        "\n",
        "    logging.info(\"Step 2/3: Verification of incremental improvements complete.\")\n",
        "\n",
        "    # --- Step 3: Save the ablation table in multiple formats ---\n",
        "    # Create a display-friendly version of the table, formatted like Panel C.\n",
        "    # Unstack the 'Regime' level to create columns for each metric under each regime.\n",
        "    panel_c_df = results_df.unstack(level='Regime')\n",
        "\n",
        "    # Reorder columns to group by regime, then by metric.\n",
        "    panel_c_df = panel_c_df.swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
        "\n",
        "    # Define the desired column order to match the paper's table.\n",
        "    metric_order = ['RMSE_imp', 'MAE_imp', 'MAPE_imp', 'R2_imp']\n",
        "    regime_order = results_df.index.get_level_values('Regime').unique()\n",
        "    final_column_order = [(reg, met) for reg in regime_order for met in metric_order]\n",
        "    panel_c_df = panel_c_df[final_column_order]\n",
        "\n",
        "    # Save the raw, stacked data to CSV for easy programmatic access.\n",
        "    raw_csv_path = os.path.join(output_dir, \"results_ablation_raw.csv\")\n",
        "    results_df.to_csv(raw_csv_path)\n",
        "    logging.info(f\"Raw ablation results saved to {raw_csv_path}\")\n",
        "\n",
        "    # Save the formatted, publication-style table to a separate CSV.\n",
        "    formatted_csv_path = os.path.join(output_dir, \"results_ablation_panel_C.csv\")\n",
        "    panel_c_df.round(4).to_csv(formatted_csv_path)\n",
        "    logging.info(f\"Formatted Panel C results saved to {formatted_csv_path}\")\n",
        "\n",
        "    # Save to LaTeX format for easy inclusion in a research paper.\n",
        "    latex_path = os.path.join(output_dir, \"results_ablation_panel_C.tex\")\n",
        "    panel_c_df.round(4).to_latex(latex_path, multirow=True, multicolumn=True)\n",
        "    logging.info(f\"Formatted Panel C results saved to {latex_path}\")\n",
        "\n",
        "    logging.info(\"Step 3/3: Ablation results tables compiled and saved.\")\n",
        "\n",
        "    return panel_c_df\n"
      ],
      "metadata": {
        "id": "Fiwp2hiEFFGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27: Regularization-Free Stability Test (Vary $\\lambda$ for Sparse Clusters)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Regularization-Free Stability Test\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 27, Step 3: Plotting and Saving Results (Helper Function)\n",
        "# This helper is retained as its logic was sound.\n",
        "# ------------------------------------------------------------------------------\n",
        "def _plot_and_save_stability_results(\n",
        "    results_df: pd.DataFrame,\n",
        "    regime: str,\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plots the RMSE vs. lambda results, verifies flatness, and saves artifacts.\n",
        "\n",
        "    Args:\n",
        "        results_df: DataFrame with 'lambda' and 'RMSE_imp' columns.\n",
        "        regime: The missingness regime being tested.\n",
        "        output_dir: Directory to save the plot and numerical results.\n",
        "    \"\"\"\n",
        "    # --- Save Numerical Results ---\n",
        "    # Save the detailed numerical results to a CSV file for auditing and analysis.\n",
        "    results_path = os.path.join(output_dir, f\"stability_results_{regime}.csv\")\n",
        "    results_df.to_csv(results_path, index=False)\n",
        "    logging.info(f\"Numerical stability results saved to {results_path}\")\n",
        "\n",
        "    # --- Plotting ---\n",
        "    # Set a professional plot style for publication quality.\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Create the line plot of RMSE against lambda.\n",
        "    ax = sns.lineplot(data=results_df, x='lambda', y='RMSE_imp', marker='o', color='b')\n",
        "\n",
        "    # Set the x-axis to a logarithmic scale to visualize the wide range of lambda values.\n",
        "    ax.set_xscale('log')\n",
        "    # Set plot titles and labels for clarity.\n",
        "    ax.set_title(f'RMSE Sensitivity to Regularization (λ) under {regime} Missingness', fontsize=16)\n",
        "    ax.set_xlabel('Regularization Strength (λ)', fontsize=12)\n",
        "    ax.set_ylabel('Out-of-Sample RMSE', fontsize=12)\n",
        "    ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # Save the plot to a high-resolution PDF file.\n",
        "    plot_path = os.path.join(output_dir, f\"stability_plot_{regime}.pdf\")\n",
        "    plt.savefig(plot_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    logging.info(f\"Stability plot for regime '{regime}' saved to {plot_path}\")\n",
        "\n",
        "    # --- Programmatic Verification of Flatness ---\n",
        "    # Calculate the absolute range of RMSE values observed across the lambda grid.\n",
        "    rmse_range = results_df['RMSE_imp'].max() - results_df['RMSE_imp'].min()\n",
        "\n",
        "    # Define a strict tolerance for what constitutes \"flat\".\n",
        "    flatness_tolerance = 1e-4\n",
        "\n",
        "    # Check if the variation is negligible, confirming the paper's claim.\n",
        "    if rmse_range < flatness_tolerance:\n",
        "        logging.info(\n",
        "            f\"VERIFICATION SUCCESSFUL: RMSE is stable for regime '{regime}'. \"\n",
        "            f\"Total variation ({rmse_range:.6f}) is less than tolerance ({flatness_tolerance}).\"\n",
        "        )\n",
        "    else:\n",
        "        logging.warning(\n",
        "            f\"VERIFICATION FAILED: RMSE shows non-negligible variation for regime '{regime}'. \"\n",
        "            f\"Total variation = {rmse_range:.6f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 27, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_regularization_stability_test(\n",
        "    regime: str,\n",
        "    X_train: np.ndarray,\n",
        "    mask_train: np.ndarray,\n",
        "    tensor_true: np.ndarray,\n",
        "    mask_eval: np.ndarray,\n",
        "    clusters: Dict[int, List[int]],\n",
        "    dense_cluster_ids: List[int],\n",
        "    sparse_cluster_ids: List[int],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"results\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs the regularization stability test for a given missingness regime.\n",
        "\n",
        "    This function systematically varies the regularization parameter `lambda` for\n",
        "    the sparse cluster completion step and measures the impact on the final\n",
        "    out-of-sample imputation RMSE. It is designed to empirically verify the paper's\n",
        "    claim that the ACT-Tensor framework is intrinsically stable. This is a complete,\n",
        "    production-grade implementation that avoids re-computing unnecessary steps.\n",
        "\n",
        "    Args:\n",
        "        regime: The missingness regime to test (e.g., 'Block', 'Logit').\n",
        "        X_train, mask_train, tensor_true, mask_eval: The necessary data tensors.\n",
        "        clusters, dense_cluster_ids, sparse_cluster_ids: The clustering results.\n",
        "        config: The main configuration dictionary.\n",
        "        output_dir: Directory to save plots and result data.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the (lambda, RMSE) results for the regime.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    # Verify that the specified regime is one designated for this test.\n",
        "    if regime not in config[\"stability_test\"][\"regimes\"]:\n",
        "        logging.warning(f\"Regime '{regime}' is not in the stability test list defined in config. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Create a dedicated subdirectory for this test's artifacts to avoid conflicts.\n",
        "    test_output_dir = os.path.join(output_dir, f\"stability_test_{regime}\")\n",
        "    os.makedirs(test_output_dir, exist_ok=True)\n",
        "\n",
        "    logging.info(f\"--- Starting Regularization Stability Test for Regime: {regime} ---\")\n",
        "\n",
        "    # --- Step 1: One-Time Computation of Dense Clusters ---\n",
        "    # The dense cluster completion is unpenalized and thus invariant to lambda.\n",
        "    # We run it once and its results will be loaded by the assembly step in the loop.\n",
        "    logging.info(\"Performing one-time completion of dense clusters...\")\n",
        "    complete_dense_clusters(\n",
        "        dense_cluster_ids=dense_cluster_ids,\n",
        "        config=config,\n",
        "        data_dir=test_output_dir  # Use the dedicated directory\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Iterate over Lambda Grid for Sparse Clusters ---\n",
        "    # Extract the grid of lambda values to test from the configuration.\n",
        "    lambda_grid = config[\"stability_test\"][\"lambda_grid\"]\n",
        "\n",
        "    # Initialize a list to store the results from each iteration.\n",
        "    results = []\n",
        "\n",
        "    # Loop through each value of lambda in the specified grid.\n",
        "    for lambda_val in lambda_grid:\n",
        "        iteration_start_time = time.time()\n",
        "        logging.info(f\"--- Testing lambda = {lambda_val:.1e} ---\")\n",
        "\n",
        "        # Create a deep copy of the config to safely modify it for this specific run.\n",
        "        temp_config = copy.deepcopy(config)\n",
        "        # Override the lambda value for the sparse cluster completion step.\n",
        "        temp_config[\"imputation_hparams\"][\"ridge_lambda\"] = lambda_val\n",
        "\n",
        "        # 2a. Re-run sparse cluster completion with the current lambda.\n",
        "        complete_sparse_clusters(\n",
        "            sparse_cluster_ids=sparse_cluster_ids,\n",
        "            dense_cluster_ids=dense_cluster_ids,\n",
        "            clusters=clusters,\n",
        "            X_train=X_train,\n",
        "            mask_train=mask_train,\n",
        "            config=temp_config,\n",
        "            data_dir=test_output_dir\n",
        "        )\n",
        "\n",
        "        # 2b. Assemble the global tensor. This will load the newly computed sparse\n",
        "        #     tensors and the static dense tensors from the test directory.\n",
        "        X_hat_assembled = assemble_completed_tensor(\n",
        "            clusters=clusters,\n",
        "            tensor_shape=X_train.shape,\n",
        "            mask_train=mask_train,\n",
        "            X_train=X_train,\n",
        "            config=temp_config,\n",
        "            data_dir=test_output_dir\n",
        "        )\n",
        "\n",
        "        # 2c. Apply temporal smoothing (CMA is the default).\n",
        "        X_tilde = apply_cma_smoothing(\n",
        "            X_hat_assembled=X_hat_assembled,\n",
        "            config=temp_config,\n",
        "            output_dir=test_output_dir\n",
        "        )\n",
        "\n",
        "        # 2d. Evaluate the final out-of-sample RMSE.\n",
        "        metrics = evaluate_imputation_accuracy(\n",
        "            tensor_true=tensor_true,\n",
        "            tensor_imputed=X_tilde,\n",
        "            mask_eval=mask_eval\n",
        "        )\n",
        "\n",
        "        # Store the result for this lambda value.\n",
        "        results.append({'lambda': lambda_val, 'RMSE_imp': metrics['RMSE_imp']})\n",
        "        iteration_end_time = time.time()\n",
        "        logging.info(\n",
        "            f\"Result for lambda = {lambda_val:.1e}: RMSE = {metrics['RMSE_imp']:.6f} \"\n",
        "            f\"(took {iteration_end_time - iteration_start_time:.2f}s)\"\n",
        "        )\n",
        "\n",
        "    # Convert the list of results into a pandas DataFrame for analysis and plotting.\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # --- Step 3: Plot RMSE vs. lambda and Save ---\n",
        "    # Call the helper function to generate the plot, save numerical results, and verify flatness.\n",
        "    _plot_and_save_stability_results(results_df, regime, test_output_dir)\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "dAfs9v_vFuf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28: Construct Double-Sorted Value-Weighted Portfolios and Excess-Return Tensor $\\mathcal{R}$\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 28: Construct Portfolios and Return Tensor\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Step 1: Prepare Data for Sorting (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _prepare_sorting_dataframe(\n",
        "    X_tilde: np.ndarray,\n",
        "    df_clean: pd.DataFrame,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int],\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares a master DataFrame for portfolio sorting by merging market data and imputed characteristics.\n",
        "\n",
        "    This function is the critical link between the imputation pipeline and the asset\n",
        "    pricing evaluation. It takes the final imputed tensor, \"un-tensors\" it back into\n",
        "    a panel DataFrame, and merges it with the necessary market data (returns, market\n",
        "    capitalization) to create a single, unified dataset ready for sorting.\n",
        "\n",
        "    Args:\n",
        "        X_tilde: The final smoothed and imputed 3D tensor of shape (T, N, L).\n",
        "        df_clean: The cleansed DataFrame from Task 3, containing returns and price/shares data.\n",
        "        time_to_idx: Mapping from timestamp to integer index.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "        config: The study's configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame indexed by (date, permno) ready for sorting, containing\n",
        "        market equity, excess returns, and all L imputed characteristics.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(X_tilde, np.ndarray) or X_tilde.ndim != 3:\n",
        "        raise TypeError(\"Input 'X_tilde' must be a 3D NumPy array.\")\n",
        "    if not isinstance(df_clean, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df_clean' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Prepare Market Data from df_clean ---\n",
        "    # Select only the necessary columns to minimize memory footprint.\n",
        "    market_data = df_clean[['prc', 'shrout', 'ret', 'RF']].copy()\n",
        "\n",
        "    # Calculate Market Equity (ME) in millions of USD for weighting and sorting.\n",
        "    # Formula: ME = |Price| * Shares Outstanding (in thousands) / 1000\n",
        "    market_data['market_equity'] = market_data['prc'].abs() * market_data['shrout'] / 1000.0\n",
        "\n",
        "    # Calculate monthly excess returns for portfolio value calculation.\n",
        "    # The risk-free rate 'RF' is in percent format and must be converted to decimal.\n",
        "    market_data['ret_excess'] = market_data['ret'] - (market_data['RF'] / 100.0)\n",
        "\n",
        "    # --- \"Un-tensor\" the Imputed Characteristics ---\n",
        "    # Get tensor dimensions.\n",
        "    T, N, L = X_tilde.shape\n",
        "\n",
        "    # Create a DataFrame from the reshaped tensor. This flattens the tensor into a 2D array.\n",
        "    char_df = pd.DataFrame(\n",
        "        X_tilde.reshape(T * N, L),\n",
        "        # Programmatically reconstruct the (date, permno) MultiIndex using the provided mappings.\n",
        "        # This step is critical for ensuring perfect alignment.\n",
        "        index=pd.MultiIndex.from_product(\n",
        "            [time_to_idx.keys(), firm_to_idx.keys()], names=['date', 'permno']\n",
        "        ),\n",
        "        # Assign column names to the characteristics.\n",
        "        columns=[f'char_{i}' for i in range(L)]\n",
        "    )\n",
        "\n",
        "    # --- Merge Market Data and Characteristics ---\n",
        "    # Join the market data and characteristic DataFrames on their common index.\n",
        "    # An inner join ensures we only keep firm-months with all necessary data.\n",
        "    df_master = market_data.join(char_df, how='inner')\n",
        "\n",
        "    # Drop any remaining rows with missing market equity or returns, as these\n",
        "    # are essential for sorting and weighting.\n",
        "    df_master.dropna(subset=['market_equity', 'ret_excess'], inplace=True)\n",
        "\n",
        "    # Log the size of the final prepared DataFrame.\n",
        "    logging.info(f\"Prepared master sorting DataFrame with {len(df_master)} observations.\")\n",
        "\n",
        "    # Return the unified DataFrame.\n",
        "    return df_master\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_portfolio_return_tensor(\n",
        "    X_tilde: np.ndarray,\n",
        "    df_clean: pd.DataFrame,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs a 4D tensor of double-sorted, value-weighted portfolio returns.\n",
        "\n",
        "    This function is the core of the asset pricing evaluation setup. It translates\n",
        "    firm-level imputed characteristics into tradable portfolio returns, creating the\n",
        "    test assets for subsequent factor analysis. The process involves a classic\n",
        "    Fama-French style double sort, performed cross-sectionally each month.\n",
        "\n",
        "    Args:\n",
        "        X_tilde: The final smoothed and imputed 3D tensor from the imputation pipeline.\n",
        "        df_clean: The cleansed DataFrame from Task 3.\n",
        "        time_to_idx: Mapping from timestamp to integer index.\n",
        "        firm_to_idx: Mapping from permno to integer index.\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the final return tensor and audit files.\n",
        "\n",
        "    Returns:\n",
        "        A 4D NumPy array of shape (P, Q, L-1, T) containing the value-weighted\n",
        "        excess returns for each portfolio over time.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logging.info(\"--- Starting Portfolio Return Tensor Construction ---\")\n",
        "\n",
        "    # --- Step 1: Prepare the master DataFrame for sorting ---\n",
        "    # This helper function merges imputed characteristics with market data.\n",
        "    df_sort = _prepare_sorting_dataframe(X_tilde, df_clean, time_to_idx, firm_to_idx, config)\n",
        "\n",
        "    # --- Step 2: Perform Double Sorts ---\n",
        "    # Get portfolio construction parameters from the configuration.\n",
        "    P = config[\"portfolio_settings\"][\"P_size_buckets\"]\n",
        "    Q = config[\"portfolio_settings\"][\"Q_char_buckets\"]\n",
        "    L = config[\"tensor_policies\"][\"L_characteristics\"]\n",
        "\n",
        "    # Assume 'char_0' is the size characteristic. All other characteristics will be used for the second sort.\n",
        "    # Note: A more robust implementation might get the size characteristic name from the config.\n",
        "    size_char_name = 'char_0'\n",
        "    other_char_names = [f'char_{i}' for i in range(1, L)]\n",
        "\n",
        "    # Define a function to perform the double sort on a single month's data.\n",
        "    def perform_double_sort(group: pd.DataFrame) -> pd.DataFrame:\n",
        "        # First sort: Assign firms to P quantiles based on market equity.\n",
        "        # `pd.qcut` with `labels=False` creates integer bucket labels.\n",
        "        # `duplicates='drop'` handles cases with many firms of the same size.\n",
        "        group['size_bucket'] = pd.qcut(group['market_equity'], P, labels=False, duplicates='drop')\n",
        "\n",
        "        # Second sort: Within each size bucket, sort firms into Q quantiles for each characteristic.\n",
        "        # We use `groupby().transform()` for a vectorized implementation of this nested sort.\n",
        "        for char_name in other_char_names:\n",
        "            bucket_col_name = f'{char_name}_bucket'\n",
        "            group[bucket_col_name] = group.groupby('size_bucket')[char_name].transform(\n",
        "                lambda x: pd.qcut(x, Q, labels=False, duplicates='drop')\n",
        "            )\n",
        "        return group\n",
        "\n",
        "    logging.info(f\"Performing {P}x{Q} double sorts for each of {L-1} characteristics...\")\n",
        "    # Apply the sorting function to each monthly group of data.\n",
        "    # `group_keys=False` prevents the group key from being added to the index.\n",
        "    df_sorted = df_sort.groupby(level='date', group_keys=False).apply(perform_double_sort)\n",
        "\n",
        "    # --- Step 3: Compute Value-Weighted Returns and Assemble Tensor ---\n",
        "    # \"Melt\" the DataFrame from a wide format (many bucket columns) to a long format.\n",
        "    # This is a key step for enabling efficient grouped aggregation.\n",
        "    bucket_cols = [f'{name}_bucket' for name in other_char_names]\n",
        "    df_long = pd.melt(\n",
        "        df_sorted.reset_index(),\n",
        "        id_vars=['date', 'permno', 'ret_excess', 'market_equity', 'size_bucket'],\n",
        "        value_vars=bucket_cols,\n",
        "        var_name='characteristic',\n",
        "        value_name='char_bucket'\n",
        "    )\n",
        "    # Clean the characteristic name (e.g., 'char_1_bucket' -> 'char_1').\n",
        "    df_long['characteristic'] = df_long['characteristic'].str.replace('_bucket', '')\n",
        "    # Drop rows where a firm could not be assigned to a bucket (e.g., due to NaNs).\n",
        "    df_long.dropna(subset=['size_bucket', 'char_bucket'], inplace=True)\n",
        "\n",
        "    # Define the value-weighting aggregation function.\n",
        "    def value_weighted_mean(group: pd.DataFrame) -> float:\n",
        "        # Formula: R_p = sum(w_i * R_i) / sum(w_i), where w_i is market equity.\n",
        "        return np.average(group['ret_excess'], weights=group['market_equity'])\n",
        "\n",
        "    # Group by all portfolio dimensions (date, characteristic, size, char_bucket).\n",
        "    # Then apply the value-weighting function to each group.\n",
        "    portfolio_returns = df_long.groupby(\n",
        "        ['date', 'characteristic', 'size_bucket', 'char_bucket']\n",
        "    ).apply(value_weighted_mean, include_groups=False)\n",
        "\n",
        "    # Convert the resulting Series into a multi-dimensional DataFrame via unstacking.\n",
        "    portfolio_returns_df = portfolio_returns.unstack(level=['characteristic', 'size_bucket', 'char_bucket'])\n",
        "\n",
        "    # Reindex to create a complete grid of all possible portfolios, filling any\n",
        "    # portfolios that were empty in a given month with a return of 0.0.\n",
        "    full_portfolio_index = pd.MultiIndex.from_product([\n",
        "        other_char_names, range(P), range(Q)\n",
        "    ], names=['characteristic', 'size_bucket', 'char_bucket'])\n",
        "    portfolio_returns_df = portfolio_returns_df.reindex(columns=full_portfolio_index).fillna(0.0)\n",
        "\n",
        "    # Convert the DataFrame of returns to a NumPy array.\n",
        "    # The shape will be (T, L-1, P, Q).\n",
        "    return_array = portfolio_returns_df.to_numpy().reshape(T, L - 1, P, Q)\n",
        "\n",
        "    # Transpose the array to the final desired shape: (P, Q, L-1, T).\n",
        "    return_tensor = return_array.transpose(2, 3, 1, 0)\n",
        "\n",
        "    logging.info(f\"Constructed portfolio return tensor with shape {return_tensor.shape}.\")\n",
        "\n",
        "    # Save the final tensor artifact.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    tensor_path = os.path.join(output_dir, \"tensor_R.npz\")\n",
        "    np.savez_compressed(tensor_path, R=return_tensor)\n",
        "    logging.info(f\"Portfolio return tensor saved to {tensor_path}\")\n",
        "\n",
        "    # Return the final tensor.\n",
        "    return return_tensor\n"
      ],
      "metadata": {
        "id": "Ho78HJE4Hdzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 29: Extract Latent Factors via Partial Tucker Decomposition (HOSVD)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 29: Extract Latent Factors via HOSVD\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 29, Step 2: HOSVD to compute loading matrices (Helper)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_hosvd_loadings(\n",
        "    tensor: np.ndarray,\n",
        "    modes_to_decompose: List[int],\n",
        "    mode_ranks: List[int]\n",
        ") -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes the loading matrices for a partial Tucker decomposition via HOSVD.\n",
        "\n",
        "    This function implements the core of the Higher-Order SVD algorithm. For each\n",
        "    specified mode, it unfolds the tensor into a matrix, computes the SVD of that\n",
        "    matrix, and extracts the top `k` left singular vectors to form the orthogonal\n",
        "    loading matrix (basis) for that mode.\n",
        "\n",
        "    Args:\n",
        "        tensor: The N-dimensional NumPy array to decompose.\n",
        "        modes_to_decompose: A list of integer mode indices to be decomposed.\n",
        "        mode_ranks: A list of integers specifying the target rank for each mode\n",
        "                    in `modes_to_decompose`.\n",
        "\n",
        "    Returns:\n",
        "        A list of loading matrices (NumPy arrays), one for each factored mode.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if len(modes_to_decompose) != len(mode_ranks):\n",
        "        raise ValueError(\"Length of 'modes_to_decompose' and 'mode_ranks' must be equal.\")\n",
        "\n",
        "    # Initialize a list to store the computed loading matrices.\n",
        "    loading_matrices = []\n",
        "\n",
        "    # Iterate through the modes to be decomposed.\n",
        "    for mode, rank in zip(modes_to_decompose, mode_ranks):\n",
        "        # --- Unfolding (Matricization) ---\n",
        "        # Unfold the tensor along the current mode using the tensorly library.\n",
        "        # This is a robust and standard way to perform matricization.\n",
        "        unfolded_matrix = tl.unfold(tensor, mode)\n",
        "\n",
        "        # --- SVD Computation ---\n",
        "        # Compute the singular value decomposition of the unfolded matrix.\n",
        "        # `full_matrices=False` is more memory and computationally efficient.\n",
        "        try:\n",
        "            U, _, _ = np.linalg.svd(unfolded_matrix, full_matrices=False)\n",
        "        except np.linalg.LinAlgError as e:\n",
        "            logging.error(f\"SVD computation failed for mode {mode}: {e}\")\n",
        "            raise\n",
        "\n",
        "        # --- Extraction ---\n",
        "        # The loading matrix is the first `rank` columns of the left singular vectors.\n",
        "        # This forms the orthogonal basis for the compressed mode.\n",
        "        loading_matrix = U[:, :rank]\n",
        "        loading_matrices.append(loading_matrix)\n",
        "\n",
        "        logging.info(f\"Computed loading matrix for mode {mode} with shape {loading_matrix.shape}.\")\n",
        "\n",
        "    return loading_matrices\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 29, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def extract_latent_factors(\n",
        "    R_tensor: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extracts latent factors from the portfolio return tensor via HOSVD.\n",
        "\n",
        "    This function implements the dimensionality reduction step of the asset pricing\n",
        "    pipeline. It takes the high-dimensional tensor of portfolio returns and applies\n",
        "    a partial Tucker decomposition to find a low-dimensional core tensor. The\n",
        "    time series of this core tensor represent the latent factors driving returns.\n",
        "\n",
        "    This implementation uses the `tensorly` library for robust multilinear algebra.\n",
        "\n",
        "    Args:\n",
        "        R_tensor: The 4D tensor of portfolio returns with shape (P, Q, L-1, T).\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the factor matrix and loading matrices.\n",
        "\n",
        "    Returns:\n",
        "        A 2D NumPy array of shape (T, k), where k is the total number of latent\n",
        "        factors, representing the time series of factor realizations.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(R_tensor, np.ndarray) or R_tensor.ndim != 4:\n",
        "        raise TypeError(\"Input 'R_tensor' must be a 4D NumPy array.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    logging.info(\"--- Starting Latent Factor Extraction via HOSVD ---\")\n",
        "\n",
        "    # --- Step 1: Set up the Partial Tucker Decomposition ---\n",
        "    # Get the target ranks for the portfolio dimensions from the config.\n",
        "    ranks_config = config[\"factor_extraction\"][\"mode_ranks\"]\n",
        "    # The modes to decompose are the first three (0, 1, 2). Mode 3 (time) is preserved.\n",
        "    modes_to_decompose = [0, 1, 2]\n",
        "    mode_ranks = [ranks_config[\"k_p\"], ranks_config[\"k_q\"], ranks_config[\"k_ell\"]]\n",
        "\n",
        "    # --- Step 2: Compute HOSVD to get the loading matrices ---\n",
        "    # Call the helper function to get the orthogonal bases for the portfolio modes.\n",
        "    U, V, W = _compute_hosvd_loadings(R_tensor, modes_to_decompose, mode_ranks)\n",
        "\n",
        "    # Save the loading matrices for diagnostics and potential reconstruction.\n",
        "    loadings_path = os.path.join(output_dir, \"tucker_loadings.npz\")\n",
        "    np.savez_compressed(loadings_path, U=U, V=V, W=W)\n",
        "    logging.info(f\"Step 2/3: Loading matrices computed and saved to {loadings_path}.\")\n",
        "\n",
        "    # --- Step 3: Project back to obtain the core tensor and factor matrix ---\n",
        "    # The core tensor is found by projecting the original tensor onto the new bases.\n",
        "    # We use `tensorly.tenalg.multi_mode_dot` for a robust and efficient implementation.\n",
        "    # Formula: F = R x_1 U^T x_2 V^T x_3 W^T\n",
        "    core_tensor = multi_mode_dot(R_tensor, [U.T, V.T, W.T], modes=modes_to_decompose)\n",
        "\n",
        "    # The resulting core tensor has shape (k_p, k_q, k_ell, T).\n",
        "    logging.info(f\"Computed core tensor with shape {core_tensor.shape}.\")\n",
        "\n",
        "    # The time series of latent factors is the vectorized core tensor at each time t.\n",
        "    # Transpose to (T, k_p, k_q, k_ell) to bring the time dimension to the front.\n",
        "    factors_over_time = core_tensor.transpose(3, 0, 1, 2)\n",
        "\n",
        "    # Reshape into the final 2D factor matrix of shape (T, k_p*k_q*k_ell).\n",
        "    T = R_tensor.shape[3]\n",
        "    total_factors = np.prod(mode_ranks)\n",
        "    factor_matrix = factors_over_time.reshape(T, total_factors)\n",
        "\n",
        "    logging.info(f\"Step 3/3: Reshaped core tensor into factor matrix of shape {factor_matrix.shape}.\")\n",
        "\n",
        "    # Save the final factor matrix, which is the primary output of this task.\n",
        "    factors_path = os.path.join(output_dir, \"factors_F.npz\")\n",
        "    np.savez_compressed(factors_path, F=factor_matrix)\n",
        "    logging.info(f\"Factor matrix saved to {factors_path}.\")\n",
        "\n",
        "    # Return the factor matrix for use in the next pipeline step.\n",
        "    return factor_matrix\n"
      ],
      "metadata": {
        "id": "25VMrGt4LqI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 30: Forward Stepwise Factor Selection and Time-Series Regressions\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 30: Forward Stepwise Factor Selection and Time-Series Regressions\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 30, Step 2: Pseudo Cross-Sectional R-squared (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _calculate_pseudo_r2(\n",
        "    Y: np.ndarray,\n",
        "    X_candidate: np.ndarray,\n",
        "    denom_r2: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the pseudo cross-sectional R-squared for a given set of factors.\n",
        "\n",
        "    Args:\n",
        "        Y: Matrix of portfolio returns (N_port, T-1).\n",
        "        X_candidate: Matrix of candidate factors including intercept (T-1, k_factors).\n",
        "        denom_r2: The pre-computed denominator for the R-squared calculation.\n",
        "\n",
        "    Returns:\n",
        "        The pseudo cross-sectional R-squared value.\n",
        "    \"\"\"\n",
        "    # Solve for the regression coefficients (alphas and betas) for all portfolios\n",
        "    # simultaneously using a single matrix operation.\n",
        "    # B = (X'X)^-1 * (X'Y')\n",
        "    try:\n",
        "        coeffs = np.linalg.solve(X_candidate.T @ X_candidate, X_candidate.T @ Y.T)\n",
        "    except np.linalg.LinAlgError:\n",
        "        # If the matrix is singular, this factor set is invalid.\n",
        "        return -np.inf\n",
        "\n",
        "    # The alphas (pricing errors) are the coefficients of the intercept term (first row).\n",
        "    alphas = coeffs[0, :]\n",
        "\n",
        "    # The numerator of the R-squared formula is the mean squared alpha.\n",
        "    numerator_r2 = np.mean(alphas**2)\n",
        "\n",
        "    # Calculate the pseudo cross-sectional R-squared.\n",
        "    # Formula: R2_xs = 1 - (mean(alpha^2) / Var_xs(mean_returns))\n",
        "    r2_xs = 1 - (numerator_r2 / denom_r2)\n",
        "\n",
        "    return r2_xs\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 30, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def select_factors_and_predict(\n",
        "    R_tensor: np.ndarray,\n",
        "    factor_matrix: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Selects predictive factors via forward stepwise regression and computes forecasts.\n",
        "\n",
        "    This function implements a forward stepwise selection algorithm to choose a\n",
        "    parsimonious set of latent factors that best explain the cross-section of\n",
        "    portfolio returns. The selection criterion is the maximization of the pseudo\n",
        "    cross-sectional R-squared. After selection, it runs the final regressions\n",
        "    to obtain pricing errors (alphas), factor loadings (betas), and return forecasts.\n",
        "\n",
        "    Args:\n",
        "        R_tensor: The 4D tensor of portfolio returns (P, Q, L-1, T).\n",
        "        factor_matrix: The 2D matrix of all potential latent factors (T, k_total).\n",
        "        config: The study's configuration dictionary.\n",
        "        output_dir: Directory to save the selected factors and regression outputs.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - alphas (np.ndarray): Vector of pricing errors for each portfolio (N_port,).\n",
        "        - betas (np.ndarray): Matrix of factor loadings for each portfolio (N_port, M).\n",
        "        - R_hat (np.ndarray): Matrix of predicted returns (N_port, T-1).\n",
        "    \"\"\"\n",
        "    logging.info(\"--- Starting Factor Selection and Prediction ---\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Prepare Data for Regression ---\n",
        "    # Reshape the 4D return tensor into a 2D matrix (N_port, T).\n",
        "    P, Q, L_minus_1, T = R_tensor.shape\n",
        "    N_port = P * Q * L_minus_1\n",
        "    portfolio_returns = R_tensor.reshape(N_port, T)\n",
        "\n",
        "    # Align dependent and independent variables for predictive regression r_t+1 ~ f_t.\n",
        "    Y = portfolio_returns[:, 1:]  # Returns from t=1 to T-1\n",
        "    X_all_factors = factor_matrix[:-1, :] # Factors from t=0 to T-2\n",
        "\n",
        "    # Add an intercept term to the factor matrix for OLS.\n",
        "    X_with_intercept = np.hstack([np.ones((T - 1, 1)), X_all_factors])\n",
        "\n",
        "    logging.info(f\"Step 1/3: Prepared data for regression. Y shape: {Y.shape}, X shape: {X_with_intercept.shape}.\")\n",
        "\n",
        "    # --- Step 2: Forward Stepwise Factor Selection ---\n",
        "    # Pre-compute the constant denominator for the R-squared calculation.\n",
        "    # This is the cross-sectional variance of the time-series mean portfolio returns.\n",
        "    mean_returns_xs = np.mean(Y, axis=1)\n",
        "    denom_r2 = np.var(mean_returns_xs)\n",
        "\n",
        "    # Get selection parameters from config.\n",
        "    target_num_factors = config[\"factor_extraction\"][\"target_num_factors\"]\n",
        "\n",
        "    # Initialize the set of selected factor indices (plus intercept).\n",
        "    selected_indices = [0] # Start with the intercept\n",
        "    # Initialize the set of candidate factor indices.\n",
        "    candidate_indices = list(range(1, X_with_intercept.shape[1]))\n",
        "\n",
        "    logging.info(f\"Starting forward stepwise selection to find {target_num_factors} factors...\")\n",
        "    for i in range(target_num_factors):\n",
        "        best_r2 = -np.inf\n",
        "        best_candidate = -1\n",
        "\n",
        "        # Iterate through all remaining candidate factors.\n",
        "        for cand_idx in candidate_indices:\n",
        "            # Form a temporary set of factors to test.\n",
        "            temp_indices = selected_indices + [cand_idx]\n",
        "            X_candidate = X_with_intercept[:, temp_indices]\n",
        "\n",
        "            # Calculate the R-squared for this set of factors.\n",
        "            r2 = _calculate_pseudo_r2(Y, X_candidate, denom_r2)\n",
        "\n",
        "            # If this is the best factor so far, store it.\n",
        "            if r2 > best_r2:\n",
        "                best_r2 = r2\n",
        "                best_candidate = cand_idx\n",
        "\n",
        "        # Add the best factor to the selected set and remove it from candidates.\n",
        "        if best_candidate != -1:\n",
        "            selected_indices.append(best_candidate)\n",
        "            candidate_indices.remove(best_candidate)\n",
        "            logging.info(\n",
        "                f\"Step {i+1}/{target_num_factors}: Selected factor index {best_candidate-1} \"\n",
        "                f\"(Column {best_candidate}). New R2_xs = {best_r2:.4f}\"\n",
        "            )\n",
        "        else:\n",
        "            logging.warning(\"Could not find a factor to improve R-squared. Stopping selection.\")\n",
        "            break\n",
        "\n",
        "    # The final selected factor indices (excluding the intercept).\n",
        "    final_factor_indices = [i - 1 for i in selected_indices if i > 0]\n",
        "    logging.info(f\"Step 2/3: Final selected factor indices: {final_factor_indices}\")\n",
        "\n",
        "    # --- Step 3: Run Final Regressions and Compute Predictions ---\n",
        "    # Get the final matrix of selected factors (including intercept).\n",
        "    X_final = X_with_intercept[:, selected_indices]\n",
        "\n",
        "    # Run the final multivariate OLS regression for all portfolios simultaneously.\n",
        "    # Coeffs = (X'X)^-1 * (X'Y')\n",
        "    final_coeffs = np.linalg.solve(X_final.T @ X_final, X_final.T @ Y.T)\n",
        "\n",
        "    # The alphas are the first row of the coefficient matrix (intercept).\n",
        "    alphas = final_coeffs[0, :]\n",
        "    # The betas are the other rows, transposed to be (N_port, M).\n",
        "    betas = final_coeffs[1:, :].T\n",
        "\n",
        "    # Compute the matrix of predicted returns.\n",
        "    # R_hat = X_final * Coeffs\n",
        "    R_hat = (X_final @ final_coeffs).T # Shape (N_port, T-1)\n",
        "\n",
        "    logging.info(\"Step 3/3: Final regressions complete. Alphas, betas, and predictions computed.\")\n",
        "\n",
        "    # --- Save Artifacts ---\n",
        "    # Save the indices of the selected factors.\n",
        "    np.savez_compressed(os.path.join(output_dir, \"selected_factors.npz\"), indices=np.array(final_factor_indices))\n",
        "    # Save the estimated alphas (pricing errors).\n",
        "    np.savez_compressed(os.path.join(output_dir, \"alphas.npz\"), alphas=alphas)\n",
        "    # Save the estimated betas (factor loadings).\n",
        "    np.savez_compressed(os.path.join(output_dir, \"betas.npz\"), betas=betas)\n",
        "    # Save the predicted returns.\n",
        "    np.savez_compressed(os.path.join(output_dir, \"R_hat.npz\"), R_hat=R_hat)\n",
        "\n",
        "    logging.info(\"All factor selection and prediction artifacts have been saved.\")\n",
        "\n",
        "    return alphas, betas, R_hat\n"
      ],
      "metadata": {
        "id": "PYh8EtdhPaJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 31: Compute Asset-Pricing Evaluation Metrics (RMSE_α, MAE_α, MAE-Rank, IC)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 31: Compute Asset-Pricing Evaluation Metrics\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 31, Step 1: Pricing Error Metrics (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_pricing_error_metrics(alphas: np.ndarray) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Computes the root mean squared and mean absolute pricing errors.\n",
        "\n",
        "    Args:\n",
        "        alphas: A 1D NumPy array of regression intercepts (pricing errors).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing (RMSE_alpha, MAE_alpha).\n",
        "    \"\"\"\n",
        "    # Formula: RMSE_alpha = sqrt(mean(alpha^2))\n",
        "    rmse_alpha = np.sqrt(np.mean(alphas**2))\n",
        "\n",
        "    # Formula: MAE_alpha = mean(|alpha|)\n",
        "    mae_alpha = np.mean(np.abs(alphas))\n",
        "\n",
        "    return rmse_alpha, mae_alpha\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 31, Step 2: Predictive Ranking Accuracy (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_mae_rank(Y_realized: np.ndarray, Y_predicted: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Computes the Mean Absolute Rank Error between realized and predicted returns.\n",
        "\n",
        "    Args:\n",
        "        Y_realized: A 2D (N_port, T-1) array of realized portfolio returns.\n",
        "        Y_predicted: A 2D (N_port, T-1) array of predicted portfolio returns.\n",
        "\n",
        "    Returns:\n",
        "        The time-series average of the cross-sectional MAE-Rank.\n",
        "    \"\"\"\n",
        "    # Get the number of time periods for averaging.\n",
        "    num_periods = Y_realized.shape[1]\n",
        "\n",
        "    # Initialize a list to store the MAE-Rank for each time period.\n",
        "    monthly_mae_ranks = []\n",
        "\n",
        "    # Iterate through each time period (column).\n",
        "    for t in range(num_periods):\n",
        "        # Extract the cross-section of realized and predicted returns for time t.\n",
        "        realized_t = Y_realized[:, t]\n",
        "        predicted_t = Y_predicted[:, t]\n",
        "\n",
        "        # Compute ordinal ranks for both vectors. 'ordinal' assigns unique ranks.\n",
        "        rank_realized = rankdata(realized_t, method='ordinal')\n",
        "        rank_predicted = rankdata(predicted_t, method='ordinal')\n",
        "\n",
        "        # Calculate the Mean Absolute Rank Error for the current time period.\n",
        "        mae_rank_t = np.mean(np.abs(rank_realized - rank_predicted))\n",
        "        monthly_mae_ranks.append(mae_rank_t)\n",
        "\n",
        "    # The final metric is the time-series average of the monthly MAE-Ranks.\n",
        "    # Formula: MAE-Rank = (1/T) * sum(MAE-Rank_t)\n",
        "    mean_absolute_rank_error = np.mean(monthly_mae_ranks)\n",
        "\n",
        "    return mean_absolute_rank_error\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 31, Step 3: Information Coefficient (Helper Function)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_information_coefficient(Y_realized: np.ndarray, Y_predicted: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Computes the Information Coefficient (IC).\n",
        "\n",
        "    The IC is the time-series average of the cross-sectional Pearson correlation\n",
        "    between predicted and realized returns.\n",
        "\n",
        "    Args:\n",
        "        Y_realized: A 2D (N_port, T-1) array of realized portfolio returns.\n",
        "        Y_predicted: A 2D (N_port, T-1) array of predicted portfolio returns.\n",
        "\n",
        "    Returns:\n",
        "        The Information Coefficient.\n",
        "    \"\"\"\n",
        "    # Get the number of time periods.\n",
        "    num_periods = Y_realized.shape[1]\n",
        "\n",
        "    # Initialize a list to store the IC for each time period.\n",
        "    monthly_ics = []\n",
        "\n",
        "    # Iterate through each time period.\n",
        "    for t in range(num_periods):\n",
        "        # Extract the cross-section of returns for time t.\n",
        "        realized_t = Y_realized[:, t]\n",
        "        predicted_t = Y_predicted[:, t]\n",
        "\n",
        "        # Handle the edge case where a cross-section has zero variance.\n",
        "        if np.std(realized_t) < 1e-9 or np.std(predicted_t) < 1e-9:\n",
        "            # Correlation is undefined, append NaN and handle it during averaging.\n",
        "            monthly_ics.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        # Compute the Pearson correlation for the current time period.\n",
        "        # Formula: IC_t = corr(r_t, r_hat_t)\n",
        "        ic_t = np.corrcoef(realized_t, predicted_t)[0, 1]\n",
        "        monthly_ics.append(ic_t)\n",
        "\n",
        "    # The final IC is the time-series average of the monthly correlations.\n",
        "    # `np.nanmean` safely ignores any periods where correlation was undefined.\n",
        "    # Formula: IC = (1/T) * sum(IC_t)\n",
        "    information_coefficient = np.nanmean(monthly_ics)\n",
        "\n",
        "    return information_coefficient\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 31, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_asset_pricing_metrics(\n",
        "    alphas: np.ndarray,\n",
        "    R_hat: np.ndarray,\n",
        "    R_tensor: np.ndarray\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes a suite of standard asset pricing evaluation metrics.\n",
        "\n",
        "    This function quantifies the performance of the factor model by calculating:\n",
        "    1. Pricing Accuracy: How well the factors explain average returns, measured\n",
        "       by the magnitude of the pricing errors (alphas).\n",
        "    2. Predictive Power: How well the model's forecasts align with realized\n",
        "       outcomes, measured by rank correlation (MAE-Rank) and linear correlation (IC).\n",
        "\n",
        "    Args:\n",
        "        alphas: 1D array of pricing errors from the time-series regressions.\n",
        "        R_hat: 2D array (N_port, T-1) of predicted portfolio returns.\n",
        "        R_tensor: The original 4D tensor (P, Q, L-1, T) of realized returns.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the computed metrics: 'RMSE_alpha', 'MAE_alpha',\n",
        "        'MAE_Rank', and 'IC'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not all(isinstance(arr, np.ndarray) for arr in [alphas, R_hat, R_tensor]):\n",
        "        raise TypeError(\"All inputs must be NumPy arrays.\")\n",
        "\n",
        "    logging.info(\"--- Computing Asset Pricing Evaluation Metrics ---\")\n",
        "\n",
        "    # --- Prepare Data ---\n",
        "    # Reshape the realized return tensor to match the (N_port, T) format.\n",
        "    P, Q, L_minus_1, T = R_tensor.shape\n",
        "    N_port = P * Q * L_minus_1\n",
        "    portfolio_returns = R_tensor.reshape(N_port, T)\n",
        "\n",
        "    # Align realized returns with the prediction period.\n",
        "    Y_realized = portfolio_returns[:, 1:]\n",
        "\n",
        "    # Validate shapes.\n",
        "    if R_hat.shape != Y_realized.shape:\n",
        "        raise ValueError(\"Shape mismatch between predicted and realized returns.\")\n",
        "    if alphas.shape[0] != N_port:\n",
        "        raise ValueError(\"Length of alphas does not match number of portfolios.\")\n",
        "\n",
        "    # --- Step 1: Compute Pricing Error Metrics ---\n",
        "    # Call the helper to compute RMSE and MAE of the pricing errors.\n",
        "    rmse_alpha, mae_alpha = _compute_pricing_error_metrics(alphas)\n",
        "    logging.info(f\"Pricing Accuracy: RMSE_alpha = {rmse_alpha:.4f}, MAE_alpha = {mae_alpha:.4f}\")\n",
        "\n",
        "    # --- Step 2: Compute Predictive Ranking Accuracy ---\n",
        "    # Call the helper to compute the Mean Absolute Rank Error.\n",
        "    mae_rank = _compute_mae_rank(Y_realized, R_hat)\n",
        "    logging.info(f\"Predictive Power (Rank): MAE-Rank = {mae_rank:.2f}\")\n",
        "\n",
        "    # --- Step 3: Compute Information Coefficient ---\n",
        "    # Call the helper to compute the Information Coefficient.\n",
        "    ic = _compute_information_coefficient(Y_realized, R_hat)\n",
        "    logging.info(f\"Predictive Power (Linear): IC = {ic:.4f}\")\n",
        "\n",
        "    # --- Compile and Return Results ---\n",
        "    # Aggregate all computed metrics into a single dictionary.\n",
        "    metrics = {\n",
        "        \"RMSE_alpha\": rmse_alpha,\n",
        "        \"MAE_alpha\": mae_alpha,\n",
        "        \"MAE_Rank\": mae_rank,\n",
        "        \"IC\": ic\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "8Ldv5IJfQ6J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 32: Construct the Top-Minus-Bottom (T–B) Long-Short Portfolio\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 32: Construct the Top-Minus-Bottom (T–B) Long-Short Portfolio\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 32, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_tb_portfolio(\n",
        "    R_hat: np.ndarray,\n",
        "    R_tensor: np.ndarray,\n",
        "    output_dir: str = \"data_processed\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the time series of returns for a Top-minus-Bottom (T-B) strategy.\n",
        "\n",
        "    This function simulates an investable long-short portfolio strategy based on the\n",
        "    model's return predictions. Each month, it identifies the top and bottom deciles\n",
        "    of test portfolios based on their predicted returns. It then calculates the\n",
        "    realized return of a strategy that goes long the top decile and short the\n",
        "    bottom decile, assuming equal weighting within each leg.\n",
        "\n",
        "    Args:\n",
        "        R_hat: 2D array (N_port, T-1) of predicted portfolio returns from Task 30.\n",
        "        R_tensor: The original 4D tensor (P, Q, L-1, T) of realized returns.\n",
        "        output_dir: Directory to save the T-B return series.\n",
        "\n",
        "    Returns:\n",
        "        A 1D NumPy array of length (T-1) containing the time series of the\n",
        "        T-B portfolio's excess returns.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If input shapes are inconsistent.\n",
        "        TypeError: If inputs are not NumPy arrays.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that inputs are NumPy arrays.\n",
        "    if not all(isinstance(arr, np.ndarray) for arr in [R_hat, R_tensor]):\n",
        "        raise TypeError(\"All inputs must be NumPy arrays.\")\n",
        "\n",
        "    logging.info(\"--- Constructing Top-minus-Bottom (T-B) Portfolio ---\")\n",
        "\n",
        "    # --- Prepare Data ---\n",
        "    # Reshape the realized return tensor to match the (N_port, T) format.\n",
        "    P, Q, L_minus_1, T = R_tensor.shape\n",
        "    N_port = P * Q * L_minus_1\n",
        "    portfolio_returns = R_tensor.reshape(N_port, T)\n",
        "\n",
        "    # Align realized returns with the prediction period (t=1 to T-1).\n",
        "    Y_realized = portfolio_returns[:, 1:]\n",
        "\n",
        "    # Validate that the shapes of predicted and realized returns match.\n",
        "    if R_hat.shape != Y_realized.shape:\n",
        "        raise ValueError(\n",
        "            f\"Shape mismatch: R_hat shape {R_hat.shape} does not match \"\n",
        "            f\"realized returns shape {Y_realized.shape}.\"\n",
        "        )\n",
        "\n",
        "    # Get the number of time periods for the strategy.\n",
        "    num_periods = R_hat.shape[1]\n",
        "\n",
        "    # Initialize a list to store the monthly T-B returns.\n",
        "    tb_returns = []\n",
        "\n",
        "    # --- Main Loop: Iterate Through Time to Form Portfolios ---\n",
        "    # Loop from t=0 to T-2 to form portfolios for month t+1.\n",
        "    for t in range(num_periods):\n",
        "        # --- Step 1: Identify Top and Bottom Deciles ---\n",
        "        # Extract the cross-section of predicted returns for the current period.\n",
        "        predictions_t = R_hat[:, t]\n",
        "\n",
        "        # Calculate the 10th and 90th percentile breakpoints for this month's predictions.\n",
        "        bottom_decile_breakpoint = np.percentile(predictions_t, 10)\n",
        "        top_decile_breakpoint = np.percentile(predictions_t, 90)\n",
        "\n",
        "        # Create boolean masks to identify portfolios in the top and bottom deciles.\n",
        "        in_bottom_decile = predictions_t <= bottom_decile_breakpoint\n",
        "        in_top_decile = predictions_t >= top_decile_breakpoint\n",
        "\n",
        "        # --- Step 2: Compute the T-B Portfolio Return ---\n",
        "        # Extract the realized returns for the next period (t+1).\n",
        "        realized_returns_t_plus_1 = Y_realized[:, t]\n",
        "\n",
        "        # Calculate the equally-weighted average realized return of the top decile portfolios.\n",
        "        top_leg_return = np.mean(realized_returns_t_plus_1[in_top_decile])\n",
        "\n",
        "        # Calculate the equally-weighted average realized return of the bottom decile portfolios.\n",
        "        bottom_leg_return = np.mean(realized_returns_t_plus_1[in_bottom_decile])\n",
        "\n",
        "        # The T-B return is the difference between the long (top) and short (bottom) legs.\n",
        "        # Formula: r_TB = r_top - r_bottom\n",
        "        monthly_tb_return = top_leg_return - bottom_leg_return\n",
        "\n",
        "        # Append the calculated return to our time series.\n",
        "        tb_returns.append(monthly_tb_return)\n",
        "\n",
        "    # Convert the list of monthly returns to a NumPy array.\n",
        "    tb_returns_series = np.array(tb_returns)\n",
        "    logging.info(\"Step 1 & 2: Monthly T-B returns calculated for all periods.\")\n",
        "\n",
        "    # --- Step 3: Verify and Save the T-B Return Series ---\n",
        "    # Perform a basic sanity check on the computed returns.\n",
        "    if np.isnan(tb_returns_series).any():\n",
        "        logging.warning(\"T-B return series contains NaN values.\")\n",
        "\n",
        "    # Log summary statistics of the strategy returns.\n",
        "    logging.info(\n",
        "        f\"T-B Strategy Summary: Mean Monthly Return = {np.mean(tb_returns_series):.4f}, \"\n",
        "        f\"Std Dev = {np.std(tb_returns_series):.4f}\"\n",
        "    )\n",
        "\n",
        "    # Save the T-B return series to a compressed archive.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = os.path.join(output_dir, \"TB_returns.npz\")\n",
        "    np.savez_compressed(output_path, r_TB=tb_returns_series)\n",
        "    logging.info(f\"Step 3/3: T-B return series saved to {output_path}.\")\n",
        "\n",
        "    # Return the final time series of strategy returns.\n",
        "    return tb_returns_series\n"
      ],
      "metadata": {
        "id": "S4MsyXU0RsmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 33: Compute Annualized Sharpe Ratio for the T–B Strategy\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 33: Compute Annualized Sharpe Ratio for the T–B Strategy\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 33, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_annualized_sharpe_ratio(\n",
        "    r_tb_series: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the annualized Sharpe Ratio for a time series of monthly returns.\n",
        "\n",
        "    This function calculates the most common metric of risk-adjusted return. It\n",
        "    takes a series of monthly excess returns (assumed to be from a zero-investment,\n",
        "    e.g., long-short, portfolio), computes its mean and standard deviation, and\n",
        "    annualizes the resulting ratio.\n",
        "\n",
        "    The statistical significance of the Sharpe Ratio is not computed here but is\n",
        "    an important consideration for a full academic analysis (e.g., via bootstrapping\n",
        "    or the methods of Ledoit & Wolf).\n",
        "\n",
        "    Args:\n",
        "        r_tb_series: A 1D NumPy array of monthly excess returns for the T-B strategy.\n",
        "        config: The study's configuration dictionary, from which the annualization\n",
        "                factor is sourced.\n",
        "\n",
        "    Returns:\n",
        "        The annualized Sharpe Ratio as a float. Returns np.nan if the ratio\n",
        "        cannot be computed (e.g., zero volatility).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input series is too short to be meaningful.\n",
        "        TypeError: If the input is not a 1D NumPy array.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the input is a 1D NumPy array.\n",
        "    if not isinstance(r_tb_series, np.ndarray) or r_tb_series.ndim != 1:\n",
        "        raise TypeError(\"Input 'r_tb_series' must be a 1D NumPy array.\")\n",
        "\n",
        "    # Verify that the series is long enough for a meaningful standard deviation calculation.\n",
        "    if len(r_tb_series) < 2:\n",
        "        logging.warning(\"Return series is too short (length < 2) to compute Sharpe Ratio. Returning NaN.\")\n",
        "        return np.nan\n",
        "\n",
        "    logging.info(\"--- Computing Annualized Sharpe Ratio for T-B Strategy ---\")\n",
        "\n",
        "    # --- Step 1: Compute the Annualized Sharpe Ratio ---\n",
        "    # Calculate the arithmetic mean of the monthly returns.\n",
        "    mean_monthly_return = np.mean(r_tb_series)\n",
        "\n",
        "    # Calculate the sample standard deviation of the monthly returns.\n",
        "    # `ddof=1` is critical for an unbiased estimate of the population standard deviation.\n",
        "    std_monthly_return = np.std(r_tb_series, ddof=1)\n",
        "\n",
        "    # Edge Case Handling: If volatility is zero or near-zero, the Sharpe Ratio is undefined.\n",
        "    if std_monthly_return < 1e-9:\n",
        "        logging.warning(\"Standard deviation of returns is near zero. Sharpe Ratio is undefined. Returning NaN.\")\n",
        "        return np.nan\n",
        "\n",
        "    # Get the annualization factor from the configuration (typically sqrt(12) for monthly data).\n",
        "    annualization_factor = np.sqrt(config[\"evaluation_metrics\"][\"sharpe_annualization_factor\"])\n",
        "\n",
        "    # Calculate the annualized Sharpe Ratio.\n",
        "    # Formula: Sharpe_annual = sqrt(12) * (mean(r) / std(r))\n",
        "    annualized_sharpe = annualization_factor * (mean_monthly_return / std_monthly_return)\n",
        "\n",
        "    # --- Step 2: Verify Plausibility and Log ---\n",
        "    # Log the components of the calculation for auditability.\n",
        "    logging.info(f\"Mean Monthly Return: {mean_monthly_return:.6f}\")\n",
        "    logging.info(f\"Std Dev Monthly Return: {std_monthly_return:.6f}\")\n",
        "    logging.info(f\"Annualization Factor: {annualization_factor:.4f}\")\n",
        "    logging.info(f\"Final Annualized Sharpe Ratio: {annualized_sharpe:.4f}\")\n",
        "\n",
        "    # A sanity check for an unusually high Sharpe Ratio.\n",
        "    if abs(annualized_sharpe) > 5.0:\n",
        "        logging.warning(f\"Calculated Sharpe Ratio ({annualized_sharpe:.2f}) is unusually high.\")\n",
        "\n",
        "    # Return the final scalar value.\n",
        "    return annualized_sharpe\n"
      ],
      "metadata": {
        "id": "RN3JXIXySKXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 34: Compile the Asset-Pricing Results Table\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 34: Compile the Asset-Pricing Results Table\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 34, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compile_asset_pricing_results(\n",
        "    results_list: List[Dict[str, Any]],\n",
        "    output_dir: str = \"results\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compiles all asset-pricing evaluation metrics into a final results table.\n",
        "\n",
        "    This function aggregates the performance metrics (pricing errors, predictive\n",
        "    power, and Sharpe ratio) from various model runs (e.g., ACT-Tensor variants,\n",
        "    baselines) into a single, structured pandas DataFrame. It then formats this\n",
        "    DataFrame to be publication-ready and saves it in multiple formats (CSV, JSON,\n",
        "    LaTeX) for analysis and reporting.\n",
        "\n",
        "    Args:\n",
        "        results_list: A list of dictionaries, where each dictionary contains the\n",
        "                      full set of asset pricing results for a single model run.\n",
        "                      Required keys: 'Method', 'Regime', 'RMSE_alpha', 'MAE_alpha',\n",
        "                      'MAE_Rank', 'IC', 'TB_Sharpe'.\n",
        "        output_dir: The directory to save the final results tables.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the compiled and formatted asset pricing\n",
        "        results, indexed by ('Regime', 'Method').\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input list is empty or if dictionaries are missing\n",
        "                    required keys.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the input is a non-empty list of dictionaries.\n",
        "    if not isinstance(results_list, list) or not results_list:\n",
        "        raise ValueError(\"Input 'results_list' must be a non-empty list of dictionaries.\")\n",
        "    if not all(isinstance(item, dict) for item in results_list):\n",
        "        raise TypeError(\"All items in 'results_list' must be dictionaries.\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    logging.info(\"--- Compiling Asset-Pricing Results Table ---\")\n",
        "\n",
        "    # --- Step 1: Aggregate Metrics into a DataFrame ---\n",
        "    # Convert the list of result dictionaries directly into a pandas DataFrame.\n",
        "    try:\n",
        "        results_df = pd.DataFrame(results_list)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to create DataFrame from results list. Error: {e}\")\n",
        "\n",
        "    # Define the expected columns for validation.\n",
        "    expected_cols = ['Method', 'Regime', 'RMSE_alpha', 'MAE_alpha', 'MAE_Rank', 'IC', 'TB_Sharpe']\n",
        "    if not all(col in results_df.columns for col in expected_cols):\n",
        "        missing = set(expected_cols) - set(results_df.columns)\n",
        "        raise ValueError(f\"Input dictionaries are missing required keys: {missing}\")\n",
        "\n",
        "    # Set a MultiIndex on 'Regime' and 'Method' for clear, hierarchical structuring.\n",
        "    results_df.set_index(['Regime', 'Method'], inplace=True)\n",
        "    # Sort the index to ensure a consistent and logical order.\n",
        "    results_df.sort_index(inplace=True)\n",
        "\n",
        "    logging.info(\"Step 1/3: Aggregated asset pricing metrics into a DataFrame.\")\n",
        "\n",
        "    # --- Step 2: Format the Table ---\n",
        "    # Round all numerical metric columns to 4 decimal places for publication-quality precision.\n",
        "    formatted_df = results_df.round(4)\n",
        "\n",
        "    # Log a summary of the best performing method for key metrics in each regime.\n",
        "    for regime in formatted_df.index.get_level_values('Regime').unique():\n",
        "        regime_slice = formatted_df.loc[regime]\n",
        "        best_sharpe_method = regime_slice['TB_Sharpe'].idxmax()\n",
        "        best_ic_method = regime_slice['IC'].idxmax()\n",
        "        best_rmse_method = regime_slice['RMSE_alpha'].idxmin()\n",
        "        logging.info(f\"Summary for Regime '{regime}':\")\n",
        "        logging.info(f\"  - Best T-B Sharpe: {regime_slice.loc[best_sharpe_method, 'TB_Sharpe']:.4f} (Method: {best_sharpe_method})\")\n",
        "        logging.info(f\"  - Best IC: {regime_slice.loc[best_ic_method, 'IC']:.4f} (Method: {best_ic_method})\")\n",
        "        logging.info(f\"  - Best RMSE_alpha: {regime_slice.loc[best_rmse_method, 'RMSE_alpha']:.4f} (Method: {best_rmse_method})\")\n",
        "\n",
        "    logging.info(\"Step 2/3: Formatted results and logged performance summary.\")\n",
        "\n",
        "    # --- Step 3: Save the Results Table in Multiple Formats ---\n",
        "    # Save the formatted, indexed data to CSV for easy viewing and analysis.\n",
        "    csv_path = os.path.join(output_dir, \"results_asset_pricing.csv\")\n",
        "    formatted_df.to_csv(csv_path)\n",
        "    logging.info(f\"Formatted asset pricing results saved to {csv_path}\")\n",
        "\n",
        "    # Save to JSON format with an index-oriented structure.\n",
        "    json_path = os.path.join(output_dir, \"results_asset_pricing.json\")\n",
        "    formatted_df.to_json(json_path, orient='index', indent=4)\n",
        "    logging.info(f\"Formatted asset pricing results saved to {json_path}\")\n",
        "\n",
        "    # Save to LaTeX format for direct inclusion in academic papers.\n",
        "    latex_path = os.path.join(output_dir, \"results_asset_pricing.tex\")\n",
        "    # The `to_latex` method creates a clean, publication-ready table.\n",
        "    formatted_df.to_latex(latex_path, multirow=True)\n",
        "    logging.info(f\"Formatted asset pricing results saved to {latex_path}\")\n",
        "\n",
        "    logging.info(\"Step 3/3: All asset pricing results tables have been saved.\")\n",
        "\n",
        "    # Return the final formatted DataFrame.\n",
        "    return formatted_df\n"
      ],
      "metadata": {
        "id": "XCLG6TsuSqhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 35: Create an End-to-End Orchestrator Function for the Full Pipeline\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 35: End-to-End Orchestrator Function for the Full Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 35, Helper Function for Logging Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "def _setup_logger(name: str, log_file: str, level: int = logging.INFO) -> logging.Logger:\n",
        "    \"\"\"\n",
        "    Sets up a dedicated logger for a pipeline run to file and console.\n",
        "\n",
        "    This helper function creates a logger instance that is isolated to a specific\n",
        "    pipeline run. It configures it to write detailed logs to a specified file\n",
        "    and simultaneously print informational messages to the console. This ensures\n",
        "    a complete audit trail for each experiment while providing real-time feedback.\n",
        "\n",
        "    Args:\n",
        "        name: A unique name for the logger instance.\n",
        "        log_file: The full path to the file where logs will be saved.\n",
        "        level: The logging level (e.g., logging.INFO, logging.DEBUG).\n",
        "\n",
        "    Returns:\n",
        "        A configured logging.Logger instance.\n",
        "    \"\"\"\n",
        "    # Get a logger instance with a unique name for this run.\n",
        "    logger = logging.getLogger(name)\n",
        "\n",
        "    # If handlers already exist (e.g., from a previous run in the same session), clear them.\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    # Set the logger's reporting level.\n",
        "    logger.setLevel(level)\n",
        "\n",
        "    # Create a file handler to write detailed logs to a file.\n",
        "    # The mode 'w' ensures that the log file is overwritten for each new run.\n",
        "    file_handler = logging.FileHandler(log_file, mode='w')\n",
        "    # Define a detailed format for the file log.\n",
        "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "\n",
        "    # Create a stream handler to print concise logs to the console.\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    # Define a simpler format for console output.\n",
        "    stream_formatter = logging.Formatter('%(message)s')\n",
        "    stream_handler.setFormatter(stream_formatter)\n",
        "\n",
        "    # Add both handlers to the logger.\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(stream_handler)\n",
        "\n",
        "    # Prevent the logger from propagating messages to the root logger.\n",
        "    logger.propagate = False\n",
        "\n",
        "    return logger\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 35, Helper for Downstream Asset Pricing Pipeline\n",
        "# ------------------------------------------------------------------------------\n",
        "def _run_downstream_pipeline(\n",
        "    X_tilde: np.ndarray,\n",
        "    df_clean: pd.DataFrame,\n",
        "    time_to_idx: Dict[pd.Timestamp, int],\n",
        "    firm_to_idx: Dict[int, int],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str,\n",
        "    model_name: str\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Executes the complete downstream asset pricing evaluation pipeline.\n",
        "\n",
        "    This helper function encapsulates the entire workflow from portfolio\n",
        "    construction to the final Sharpe Ratio calculation (Tasks 28-33). It takes\n",
        "    a fully imputed tensor as input and returns a dictionary of all key asset\n",
        "    pricing performance metrics. This modular design ensures that all models\n",
        "    (e.g., ACT-Tensor and various baselines) are evaluated using the exact same,\n",
        "    consistent methodology, which is critical for a fair comparison.\n",
        "\n",
        "    Args:\n",
        "        X_tilde: A fully imputed and smoothed 3D tensor of shape (T, N, L).\n",
        "        df_clean: The cleansed DataFrame from Task 3, containing necessary\n",
        "                  market data like prices, shares, and returns.\n",
        "        time_to_idx: A dictionary mapping from pd.Timestamp to integer index\n",
        "                     for the time dimension.\n",
        "        firm_to_idx: A dictionary mapping from permno to integer index for the\n",
        "                     firm dimension.\n",
        "        config: The main configuration dictionary for the study.\n",
        "        output_dir: The base directory to save intermediate artifacts generated\n",
        "                    during the asset pricing pipeline.\n",
        "        model_name: A string identifier for the model being evaluated (e.g.,\n",
        "                    'ACT-Tensor_CMA', 'Baseline_Median'), used for logging and\n",
        "                    creating unique subdirectories.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing all computed asset pricing metrics for the given\n",
        "        model, including RMSE_alpha, MAE_alpha, MAE_Rank, IC, and TB_Sharpe.\n",
        "    \"\"\"\n",
        "    # Log the start of this specific downstream evaluation.\n",
        "    logging.info(f\"--- Starting Downstream Asset Pricing Pipeline for: {model_name} ---\")\n",
        "\n",
        "    # Create a dedicated subdirectory for this model's asset pricing artifacts\n",
        "    # to prevent file name collisions between different models in the same run.\n",
        "    ap_output_dir = os.path.join(output_dir, f\"ap_{model_name}\")\n",
        "    os.makedirs(ap_output_dir, exist_ok=True)\n",
        "\n",
        "    # Task 28: Construct a 4D tensor of double-sorted, value-weighted portfolio returns\n",
        "    # using the provided imputed characteristics tensor `X_tilde`.\n",
        "    R_tensor = construct_portfolio_return_tensor(\n",
        "        X_tilde, df_clean, time_to_idx, firm_to_idx, config, ap_output_dir\n",
        "    )\n",
        "\n",
        "    # Task 29: Extract a parsimonious set of latent factors from the portfolio return tensor\n",
        "    # using a partial Tucker decomposition (HOSVD).\n",
        "    factor_matrix = extract_latent_factors(R_tensor, config, ap_output_dir)\n",
        "\n",
        "    # Task 30: Use forward stepwise selection to identify the most predictive factors\n",
        "    # and run final time-series regressions to get alphas, betas, and return forecasts.\n",
        "    alphas, betas, R_hat = select_factors_and_predict(\n",
        "        R_tensor, factor_matrix, config, ap_output_dir\n",
        "    )\n",
        "\n",
        "    # Task 31: Compute the core asset pricing metrics that measure pricing accuracy\n",
        "    # (RMSE_alpha, MAE_alpha) and predictive skill (MAE_Rank, IC).\n",
        "    ap_metrics = compute_asset_pricing_metrics(alphas, R_hat, R_tensor)\n",
        "\n",
        "    # Task 32: Construct the time series of returns for a Top-minus-Bottom (T-B)\n",
        "    # long-short strategy based on the model's return forecasts.\n",
        "    r_tb = construct_tb_portfolio(R_hat, R_tensor, ap_output_dir)\n",
        "\n",
        "    # Task 33: Compute the final, and most important, risk-adjusted performance metric:\n",
        "    # the annualized Sharpe Ratio of the T-B strategy.\n",
        "    sharpe = compute_annualized_sharpe_ratio(r_tb, config)\n",
        "    # Add the Sharpe Ratio to the metrics dictionary.\n",
        "    ap_metrics['TB_Sharpe'] = sharpe\n",
        "\n",
        "    # Log the completion of this downstream evaluation.\n",
        "    logging.info(f\"--- Downstream Asset Pricing Pipeline for {model_name} COMPLETE ---\")\n",
        "\n",
        "    # Return the comprehensive dictionary of asset pricing metrics.\n",
        "    return ap_metrics\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 35: End-to-End Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_act_tensor_pipeline(\n",
        "    df_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    regime: str,\n",
        "    smoother: str,\n",
        "    run_baselines: bool = True,\n",
        "    run_ablations: bool = True,\n",
        "    run_stability_test: bool = False,\n",
        "    base_output_dir: str = \"experiment_results\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end ACT-Tensor research pipeline for a single experimental condition.\n",
        "\n",
        "    This master orchestrator manages the entire workflow from raw data ingestion\n",
        "    to the generation of final performance tables. This amended version includes\n",
        "    a full, parallel asset pricing evaluation for baseline models, ensuring a\n",
        "    complete and fair comparison, and removes all placeholders.\n",
        "\n",
        "    Args:\n",
        "        df_raw: The raw, unprocessed pandas DataFrame from data loading.\n",
        "        config: The main `act_tensor_config` dictionary.\n",
        "        regime: The evaluation regime to run, one of {\"MAR\", \"Block\", \"Logit\"}.\n",
        "        smoother: The temporal smoothing method to use, one of {\"CMA\", \"EMA\", \"KF\"}.\n",
        "        run_baselines: If True, also run and evaluate all baseline models, including\n",
        "                       their downstream asset pricing performance.\n",
        "        run_ablations: If True, also run and evaluate all ablation models.\n",
        "        run_stability_test: If True, run the lambda-sensitivity test.\n",
        "        base_output_dir: The root directory where all outputs for this run will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing a comprehensive summary of all evaluation metrics\n",
        "        and metadata for the completed run.\n",
        "    \"\"\"\n",
        "    # --- 0. Setup: Directory, Logging, and Timestamp ---\n",
        "    # Create a unique, timestamped directory name for this specific experimental run\n",
        "    # to ensure that artifacts from different runs are isolated.\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_name = f\"{regime}_{smoother}_{timestamp}\"\n",
        "    # Define the full path for this run's output directory.\n",
        "    output_dir = os.path.join(base_output_dir, run_name)\n",
        "    # Create the directory. `exist_ok=True` prevents an error if it already exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Set up a dedicated logger for this run, directing output to a unique log file.\n",
        "    logger = _setup_logger(run_name, os.path.join(output_dir, f\"pipeline_log_{run_name}.log\"))\n",
        "\n",
        "    # Log the start of the pipeline run with its unique identifier.\n",
        "    logger.info(f\"STARTING PIPELINE RUN: {run_name}\")\n",
        "    # Record the start time to calculate the total execution duration.\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize the final results dictionary with metadata about the run.\n",
        "    final_results = {\n",
        "        \"run_info\": {\"name\": run_name, \"regime\": regime, \"smoother\": smoother, \"status\": \"IN_PROGRESS\"},\n",
        "        \"artifacts\": {\"output_directory\": output_dir}\n",
        "    }\n",
        "\n",
        "    # Use a top-level try...except...finally block for robust error handling and cleanup.\n",
        "    try:\n",
        "        # --- 1. Data Preparation (Tasks 1-6) ---\n",
        "        logger.info(\"--- STAGE 1: DATA PREPARATION ---\")\n",
        "        # Validate the configuration dictionary.\n",
        "        config = validate_config(config, output_dir)\n",
        "        # Validate and enforce the schema of the raw DataFrame.\n",
        "        df_validated = validate_and_enforce_schema(df_raw, output_dir)\n",
        "        # Apply universe filters and core data cleansing.\n",
        "        df_clean = cleanse_and_filter_universe(df_validated, config, output_dir)\n",
        "        # Construct the raw characteristic signals.\n",
        "        df_chars_raw = construct_characteristics(df_clean, config, output_dir)\n",
        "        # Perform cross-sectional rank normalization.\n",
        "        df_normalized = rank_normalize_characteristics(df_chars_raw, config, output_dir)\n",
        "        # Form the final 3D tensor and observed set mask.\n",
        "        tensor_X, mask_observed, time_to_idx, firm_to_idx = form_tensor_and_observed_set(\n",
        "            df_normalized, config, output_dir, output_dir\n",
        "        )\n",
        "\n",
        "        # --- 2. Experimental Setup (Tasks 7-10) ---\n",
        "        logger.info(\"--- STAGE 2: EXPERIMENTAL SETUP ---\")\n",
        "        # Generate all three evaluation masks to ensure they are disjoint.\n",
        "        mask_mar = create_mar_mask(mask_observed, config, output_dir)\n",
        "        mask_block = create_block_mask(mask_observed, config, output_dir, existing_masks=[mask_mar])\n",
        "        mask_logit = create_logit_mask(\n",
        "            mask_observed, df_clean, df_chars_raw, time_to_idx, firm_to_idx,\n",
        "            config, output_dir, existing_masks=[mask_mar, mask_block]\n",
        "        )\n",
        "\n",
        "        # Select the specific evaluation mask for the current experimental regime.\n",
        "        eval_masks = {\"MAR\": mask_mar, \"Block\": mask_block, \"Logit\": mask_logit}\n",
        "        mask_eval = eval_masks[regime]\n",
        "\n",
        "        # Create the training tensor and mask by removing the evaluation set.\n",
        "        create_training_tensors(tensor_X, mask_observed, [regime], output_dir)\n",
        "        X_train = np.load(os.path.join(output_dir, f\"X_train_{regime}.npz\"))['X_train']\n",
        "        mask_train = np.load(os.path.join(output_dir, f\"mask_train_{regime}.npz\"))['mask']\n",
        "\n",
        "        # --- 3. ACT-Tensor Imputation & Smoothing (Tasks 11-18) ---\n",
        "        logger.info(\"--- STAGE 3: ACT-TENSOR IMPUTATION & SMOOTHING ---\")\n",
        "        clusters = cluster_firms_by_profile(X_train, firm_to_idx, config, output_dir)\n",
        "        dense_ids, sparse_ids = compute_densities_and_partition(\n",
        "            X_train, mask_train, clusters, config, output_dir\n",
        "        )\n",
        "        complete_dense_clusters(dense_ids, config, output_dir)\n",
        "        complete_sparse_clusters(sparse_ids, dense_ids, clusters, X_train, mask_train, config, output_dir)\n",
        "        X_hat = assemble_completed_tensor(clusters, X_train.shape, mask_train, X_train, config, output_dir)\n",
        "        smoother_func = {\"CMA\": apply_cma_smoothing, \"EMA\": apply_ema_smoothing, \"KF\": apply_kf_smoothing}[smoother]\n",
        "        X_tilde_act_tensor = smoother_func(X_hat, config, output_dir)\n",
        "\n",
        "        # --- 4. Imputation Accuracy Evaluation (Tasks 19-20) ---\n",
        "        logger.info(\"--- STAGE 4: IMPUTATION ACCURACY EVALUATION (ACT-TENSOR) ---\")\n",
        "        final_results[\"imputation_accuracy_main\"] = evaluate_imputation_accuracy(tensor_X, X_tilde_act_tensor, mask_eval)\n",
        "        final_results[\"imputation_accuracy_sparse\"] = evaluate_sparse_cluster_accuracy(tensor_X, X_tilde_act_tensor, mask_eval, clusters, sparse_ids)\n",
        "\n",
        "        # --- 5. Downstream Asset Pricing Evaluation (Tasks 28-34) for ACT-Tensor ---\n",
        "        logger.info(\"--- STAGE 5: ASSET PRICING EVALUATION (ACT-TENSOR) ---\")\n",
        "        final_results[\"asset_pricing_performance\"] = _run_downstream_pipeline(\n",
        "            X_tilde_act_tensor, df_clean, time_to_idx, firm_to_idx, config, output_dir, f\"ACT-Tensor_{smoother}\"\n",
        "        )\n",
        "\n",
        "        # --- Optional Analyses (Full Implementation) ---\n",
        "        if run_baselines:\n",
        "            logger.info(\"--- STAGE 6: BASELINE EVALUATION (IMPUTATION & ASSET PRICING) ---\")\n",
        "            # Run all baseline imputation methods.\n",
        "            baseline_tensors = run_baseline_imputations(X_train, time_to_idx, firm_to_idx, output_dir)\n",
        "\n",
        "            # Evaluate the imputation accuracy of each baseline.\n",
        "            baseline_imputation_results = evaluate_baseline_methods(tensor_X, regime, list(baseline_tensors.keys()), output_dir)\n",
        "            final_results[\"baseline_imputation_performance\"] = baseline_imputation_results\n",
        "\n",
        "            # Run the full downstream asset pricing pipeline for each baseline.\n",
        "            baseline_ap_results = []\n",
        "            for method_name, X_tilde_baseline in baseline_tensors.items():\n",
        "                ap_metrics = _run_downstream_pipeline(\n",
        "                    X_tilde_baseline, df_clean, time_to_idx, firm_to_idx, config, output_dir, f\"Baseline_{method_name}\"\n",
        "                )\n",
        "                baseline_ap_results.append({\"Method\": method_name, \"Regime\": regime, **ap_metrics})\n",
        "            final_results[\"baseline_asset_pricing_performance\"] = baseline_ap_results\n",
        "\n",
        "        if run_ablations:\n",
        "            logger.info(\"--- STAGE 7: ABLATION STUDIES ---\")\n",
        "            ablation_results = []\n",
        "            metrics_cp_only = run_ablation_cp_only(X_train, mask_train, tensor_X, mask_eval, config, output_dir)\n",
        "            ablation_results.append({\"Method\": \"CP Completion\", \"Regime\": regime, **metrics_cp_only})\n",
        "            metrics_cp_clust = run_ablation_cp_with_clustering(X_train, mask_train, tensor_X, mask_eval, firm_to_idx, config, output_dir)\n",
        "            ablation_results.append({\"Method\": \"CP Completion w/ Clustering\", \"Regime\": regime, **metrics_cp_clust})\n",
        "            metrics_cp_smooth = run_ablation_cp_with_smoothing(X_train, mask_train, tensor_X, mask_eval, config, output_dir)\n",
        "            ablation_results.append({\"Method\": \"CP Completion w/ CMA\", \"Regime\": regime, **metrics_cp_smooth})\n",
        "            final_results[\"ablation_studies\"] = ablation_results\n",
        "\n",
        "        if run_stability_test:\n",
        "            logger.info(\"--- STAGE 8: STABILITY TEST ---\")\n",
        "            stability_df = run_regularization_stability_test(\n",
        "                regime, X_train, mask_train, tensor_X, mask_eval, clusters,\n",
        "                dense_ids, sparse_ids, config, output_dir\n",
        "            )\n",
        "            final_results[\"stability_test_results\"] = stability_df.to_dict('records')\n",
        "\n",
        "        # Mark the run as successful in the final summary.\n",
        "        final_results[\"run_info\"][\"status\"] = \"SUCCESS\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any part of the pipeline fails, log the full error and traceback.\n",
        "        logger.error(f\"PIPELINE FAILED: An unhandled exception occurred.\", exc_info=True)\n",
        "        # Update the status in the final summary.\n",
        "        final_results[\"run_info\"][\"status\"] = \"FAILED\"\n",
        "        final_results[\"run_info\"][\"error_message\"] = str(e)\n",
        "\n",
        "    finally:\n",
        "        # --- Finalization ---\n",
        "        # This block executes regardless of success or failure.\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        final_results[\"run_info\"][\"execution_time_seconds\"] = duration\n",
        "        logger.info(f\"PIPELINE FINISHED. Status: {final_results['run_info']['status']}. Total duration: {duration:.2f} seconds.\")\n",
        "\n",
        "        # Save the final, comprehensive summary dictionary to a JSON file.\n",
        "        summary_path = os.path.join(output_dir, \"run_summary.json\")\n",
        "        with open(summary_path, 'w') as f:\n",
        "            # Define a custom JSON encoder to handle NumPy data types for serialization.\n",
        "            class NpEncoder(json.JSONEncoder):\n",
        "                def default(self, obj):\n",
        "                    if isinstance(obj, (np.integer, np.floating)): return float(obj)\n",
        "                    if isinstance(obj, np.ndarray): return obj.tolist()\n",
        "                    return super(NpEncoder, self).default(obj)\n",
        "            # Write the dictionary to the file with pretty printing.\n",
        "            json.dump(final_results, f, indent=4, cls=NpEncoder)\n",
        "\n",
        "        # Clean up logging handlers to prevent interference with subsequent runs in the same session.\n",
        "        logging.getLogger(run_name).handlers.clear()\n",
        "\n",
        "    # Return the final results dictionary.\n",
        "    return final_results\n",
        "\n"
      ],
      "metadata": {
        "id": "J1jdixlsTWqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 36: Execute the Full Pipeline for All Regimes and Smoothers\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 36: Execute the Full Pipeline for All Regimes and Smoothers\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 36, Helper for Compiling Imputation Tables\n",
        "# ------------------------------------------------------------------------------\n",
        "def compile_imputation_results(\n",
        "    df_imputation: pd.DataFrame,\n",
        "    df_ablation: pd.DataFrame,\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates and saves all final, formatted imputation accuracy tables (Panels A, B, C).\n",
        "\n",
        "    This function takes the parsed DataFrames of imputation and ablation results,\n",
        "    formats them into the three distinct panels as seen in the paper, and saves\n",
        "    them as both CSV and styled LaTeX files.\n",
        "\n",
        "    Args:\n",
        "        df_imputation: A DataFrame containing the main and baseline imputation\n",
        "                       results for both 'Overall' and 'Sparse' scopes.\n",
        "        df_ablation: A DataFrame containing the results of the ablation studies.\n",
        "        output_dir: The directory in which to save the final table artifacts.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_imputation, pd.DataFrame) or not isinstance(df_ablation, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs must be pandas DataFrames.\")\n",
        "\n",
        "    # --- Define Table Structure ---\n",
        "    # Define the canonical order of methods for table rows to ensure consistency.\n",
        "    method_order_main = [\n",
        "        \"Cross-Sectional Median\", \"Global BF-XS\", \"Local B-XS\",\n",
        "        \"ACT-Tensor w/ EMA\", \"ACT-Tensor w/ KF\", \"ACT-Tensor w/ CMA\"\n",
        "    ]\n",
        "    method_order_ablation = [\n",
        "        \"CP Completion\", \"CP Completion w/ Clustering\",\n",
        "        \"CP Completion w/ CMA\", \"ACT-Tensor w/ CMA\"\n",
        "    ]\n",
        "    # Define the order of metrics for table columns.\n",
        "    metric_cols = ['RMSE_imp', 'MAE_imp', 'MAPE_imp', 'R2_imp']\n",
        "    # Specify which metrics are better when higher (for styling purposes).\n",
        "    higher_is_better_imp = ['R2_imp']\n",
        "\n",
        "    # --- Generate Each Panel ---\n",
        "    # Use a loop to generate Panels A, B, and C with their respective configurations.\n",
        "    for panel, scope, df, order in [\n",
        "        (\"A\", \"Overall\", df_imputation, method_order_main),\n",
        "        (\"B\", \"Sparse\", df_imputation, method_order_main),\n",
        "        (\"C\", \"Overall\", df_ablation, method_order_ablation)\n",
        "    ]:\n",
        "        # Filter the data for the current panel based on its scope.\n",
        "        df_panel_data = df[df['Scope'] == scope] if 'Scope' in df.columns else df\n",
        "\n",
        "        # If no data exists for this panel, log a warning and skip.\n",
        "        if df_panel_data.empty:\n",
        "            logging.warning(f\"No data found for Imputation Panel {panel}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Pivot the table to the desired layout: Methods as index, Regime/Metrics as columns.\n",
        "        df_panel = df_panel_data.pivot(index='Method', columns='Regime', values=metric_cols)\n",
        "\n",
        "        # Enforce the canonical row order and remove any all-NaN rows that might result.\n",
        "        if order:\n",
        "            df_panel = df_panel.reindex(order).dropna(how='all')\n",
        "\n",
        "        # Save the unstyled, rounded data to a CSV file for easy analysis.\n",
        "        df_panel.round(4).to_csv(os.path.join(output_dir, f\"table_imputation_panel_{panel}.csv\"))\n",
        "\n",
        "        # Generate and save the styled LaTeX table with color-coding.\n",
        "        _style_and_save_table(df_panel, higher_is_better_imp, f\"table_imputation_panel_{panel}_styled\", output_dir)\n",
        "\n",
        "        # Log the successful generation of the panel.\n",
        "        logging.info(f\"Generated and saved Imputation Results Panel {panel}.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 36, Helper for Final Aggregation and Reporting\n",
        "# ------------------------------------------------------------------------------\n",
        "def _aggregate_and_generate_final_tables(\n",
        "    summary_paths: List[str],\n",
        "    base_output_dir: str\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Parses all run summaries and generates all final publication artifacts.\n",
        "\n",
        "    This is the master reporting function. It ingests the raw JSON outputs from\n",
        "    all experimental runs, parses them into structured DataFrames, and then calls\n",
        "    dedicated compiler functions to produce all the final tables and figures\n",
        "    required for the paper's replication.\n",
        "\n",
        "    Args:\n",
        "        summary_paths: A list of file paths to the `run_summary.json` files.\n",
        "        base_output_dir: The root directory to save the final artifacts.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the paths to the key output directories.\n",
        "    \"\"\"\n",
        "    # Log the start of the final aggregation stage.\n",
        "    logging.info(f\"\\n{'='*80}\\nAGGREGATING FINAL RESULTS FROM {len(summary_paths)} RUNS.\\n{'='*80}\")\n",
        "\n",
        "    # --- Setup: Create output directories ---\n",
        "    # Create dedicated subdirectories for the final aggregated artifacts.\n",
        "    tables_dir = os.path.join(base_output_dir, \"final_tables\")\n",
        "    figures_dir = os.path.join(base_output_dir, \"final_figures\")\n",
        "    os.makedirs(tables_dir, exist_ok=True)\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 1: Parse all raw summary files ---\n",
        "    # Call the parsing helper to convert all JSONs into a dictionary of DataFrames.\n",
        "    try:\n",
        "        parsed_data = _parse_run_summaries(summary_paths)\n",
        "    except ValueError as e:\n",
        "        logging.error(f\"Failed to parse summary files: {e}. Aborting artifact generation.\")\n",
        "        return {}\n",
        "\n",
        "    # --- Step 2: Generate All Final Tables ---\n",
        "    # Generate the three imputation tables (Panels A, B, C) by calling the new compiler.\n",
        "    compile_imputation_results(\n",
        "        df_imputation=parsed_data[\"imputation\"],\n",
        "        df_ablation=parsed_data[\"ablation\"],\n",
        "        output_dir=tables_dir\n",
        "    )\n",
        "\n",
        "    # Generate the final asset pricing table if data is available.\n",
        "    if not parsed_data[\"asset_pricing\"].empty:\n",
        "        # This call now includes results from both ACT-Tensor and baselines.\n",
        "        compile_asset_pricing_results(\n",
        "            results_list=parsed_data[\"asset_pricing\"].to_dict('records'),\n",
        "            output_dir=tables_dir\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Generate All Final Figures ---\n",
        "    # Generate stability plots if data is available.\n",
        "    df_stability = parsed_data[\"stability\"]\n",
        "    if not df_stability.empty:\n",
        "        # This assumes a simple plotting helper `_generate_stability_plots` exists.\n",
        "        _generate_stability_plots(df_stability, figures_dir)\n",
        "\n",
        "    # Log the completion of the artifact generation process.\n",
        "    logging.info(\"All final publication artifacts have been successfully generated.\")\n",
        "\n",
        "    # Return a dictionary of paths to the key output directories for verification.\n",
        "    return {\n",
        "        \"imputation_tables_dir\": tables_dir,\n",
        "        \"asset_pricing_tables_dir\": tables_dir,\n",
        "        \"figures_dir\": figures_dir\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 36, Main Execution Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def execute_full_experimental_suite(\n",
        "    data_path: str,\n",
        "    config_path: str,\n",
        "    base_output_dir: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the full suite of ACT-Tensor experiments and aggregates all results.\n",
        "\n",
        "    This master function is the single entry point for a complete replication.\n",
        "    It systematically iterates through all experimental conditions, calls the main\n",
        "    pipeline orchestrator for each, and then calls a final aggregation function\n",
        "    to produce all publication-ready tables and figures.\n",
        "\n",
        "    Args:\n",
        "        data_path: The file path to the raw input DataFrame (e.g., Parquet file).\n",
        "        config_path: The file path to the main JSON configuration file.\n",
        "        base_output_dir: The root directory where all experimental results will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the paths to the final aggregated result artifacts.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Load Global Inputs and Set Seed ---\n",
        "    # Create the base output directory if it doesn't exist.\n",
        "    os.makedirs(base_output_dir, exist_ok=True)\n",
        "    # Log the start of the entire process.\n",
        "    logging.info(\"--- MASTER SCRIPT: STARTING FULL EXPERIMENTAL SUITE ---\")\n",
        "\n",
        "    # Load the raw financial data panel, with robust error handling.\n",
        "    try:\n",
        "        df_raw = pd.read_parquet(data_path)\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"FATAL: Could not load raw data file at '{data_path}'. Error: {e}. Exiting.\")\n",
        "        raise\n",
        "\n",
        "    # Load the main configuration file, with robust error handling.\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = json.load(f)\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"FATAL: Could not load config file at '{config_path}'. Error: {e}. Exiting.\")\n",
        "        raise\n",
        "\n",
        "    # Set the global random seeds from the configuration for top-level reproducibility.\n",
        "    np.random.seed(config[\"reproducibility\"][\"seed_global\"])\n",
        "    random.seed(config[\"reproducibility\"][\"seed_global\"])\n",
        "\n",
        "    # --- Step 2: Execute the Orchestrator for All Combinations ---\n",
        "    # Define the full factorial experimental design.\n",
        "    regimes_to_run: List[str] = [\"MAR\", \"Block\", \"Logit\"]\n",
        "    smoothers_to_run: List[str] = [\"CMA\", \"EMA\", \"KF\"]\n",
        "    # This list will collect the paths to the summary file from each successful run.\n",
        "    summary_file_paths: List[str] = []\n",
        "\n",
        "    # Loop through every combination of regime and smoother.\n",
        "    for regime in regimes_to_run:\n",
        "        for smoother in smoothers_to_run:\n",
        "            # Define a unique prefix for this experimental condition.\n",
        "            run_name_prefix = f\"{regime}_{smoother}\"\n",
        "            logging.info(f\"\\n{'='*80}\\nPreparing experiment: {run_name_prefix}\\n{'='*80}\")\n",
        "\n",
        "            # --- Resumability Logic ---\n",
        "            # Check if a summary file already exists for a run with this prefix.\n",
        "            existing_summary_path = None\n",
        "            for run_dir in os.listdir(base_output_dir):\n",
        "                if run_dir.startswith(run_name_prefix):\n",
        "                    summary_path = os.path.join(base_output_dir, run_dir, \"run_summary.json\")\n",
        "                    if os.path.exists(summary_path):\n",
        "                        logging.warning(f\"Found existing completed run in '{run_dir}'. Loading path and skipping.\")\n",
        "                        existing_summary_path = summary_path\n",
        "                        break\n",
        "\n",
        "            # If a completed run was found, add its path and continue to the next experiment.\n",
        "            if existing_summary_path:\n",
        "                summary_file_paths.append(existing_summary_path)\n",
        "                continue\n",
        "\n",
        "            # Execute the full end-to-end pipeline for one experimental condition.\n",
        "            run_summary = run_act_tensor_pipeline(\n",
        "                df_raw=df_raw, config=config, regime=regime, smoother=smoother,\n",
        "                run_baselines=True, run_ablations=True,\n",
        "                run_stability_test=(regime in [\"Block\", \"Logit\"]),\n",
        "                base_output_dir=base_output_dir\n",
        "            )\n",
        "\n",
        "            # Collect the path to the summary file if the run was successful.\n",
        "            if run_summary.get(\"run_info\", {}).get(\"status\") == \"SUCCESS\":\n",
        "                run_output_dir = run_summary.get(\"artifacts\", {}).get(\"output_directory\")\n",
        "                if run_output_dir:\n",
        "                    summary_file_paths.append(os.path.join(run_output_dir, \"run_summary.json\"))\n",
        "\n",
        "    # --- Step 3: Aggregate All Results into Final Artifacts ---\n",
        "    # Call the dedicated aggregation helper to generate all final tables and figures.\n",
        "    final_artifact_paths = _aggregate_and_generate_final_tables(summary_file_paths, base_output_dir)\n",
        "\n",
        "    # Log the successful completion of the entire suite of experiments.\n",
        "    logging.info(\"--- MASTER SCRIPT: EXECUTION AND AGGREGATION FINISHED ---\")\n",
        "\n",
        "    # Return the dictionary of paths to the final generated artifacts.\n",
        "    return final_artifact_paths\n",
        "\n"
      ],
      "metadata": {
        "id": "AxPBgkVqX-kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 37: Generate Publication-Ready Tables and Figures\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 37: Generate Publication-Ready Tables and Figures\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 37, Helper for Parsing Raw JSON Summaries\n",
        "# ------------------------------------------------------------------------------\n",
        "def _parse_run_summaries(summary_paths: List[str]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Parses all raw run_summary.json files into clean, aggregated DataFrames.\n",
        "\n",
        "    This function reads a list of JSON summary files generated by the main\n",
        "    pipeline, extracts all relevant metrics for imputation, asset pricing,\n",
        "    ablations, and other tests, and consolidates them into separate, well-structured\n",
        "    pandas DataFrames for easy analysis and reporting.\n",
        "\n",
        "    Args:\n",
        "        summary_paths: A list of file paths to the `run_summary.json` files\n",
        "                       from all experimental runs.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of pandas DataFrames, with keys like 'imputation',\n",
        "        'ablation', 'asset_pricing', and 'stability'.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If no successful runs are found in the provided summary files.\n",
        "    \"\"\"\n",
        "    # Load all specified JSON files into a list of dictionaries.\n",
        "    all_summaries = []\n",
        "    for path in summary_paths:\n",
        "        try:\n",
        "            with open(path, 'r') as f:\n",
        "                all_summaries.append(json.load(f))\n",
        "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "            logging.error(f\"Could not load or parse summary file {path}. Skipping. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Filter for runs that completed successfully.\n",
        "    successful_runs = [s for s in all_summaries if s.get(\"run_info\", {}).get(\"status\") == \"SUCCESS\"]\n",
        "    if not successful_runs:\n",
        "        raise ValueError(\"No successful runs found to aggregate.\")\n",
        "\n",
        "    # Initialize record lists for each category of result.\n",
        "    records = {\"imputation\": [], \"ablation\": [], \"asset_pricing\": [], \"stability\": []}\n",
        "\n",
        "    # Iterate through each successful run and extract all relevant metrics.\n",
        "    for summary in successful_runs:\n",
        "        # Get metadata for the current run.\n",
        "        info = summary.get('run_info', {})\n",
        "        method = f\"ACT-Tensor w/ {info.get('smoother')}\"\n",
        "        regime = info.get('regime')\n",
        "\n",
        "        # Extract main and sparse imputation results for the primary model.\n",
        "        for scope, key in [(\"Overall\", \"imputation_accuracy_main\"), (\"Sparse\", \"imputation_accuracy_sparse\")]:\n",
        "            if key in summary:\n",
        "                records[\"imputation\"].append({\"Method\": method, \"Regime\": regime, \"Scope\": scope, **summary[key]})\n",
        "\n",
        "        # Extract imputation results for baseline models.\n",
        "        if \"baseline_imputation_performance\" in summary:\n",
        "            records[\"imputation\"].extend(summary[\"baseline_imputation_performance\"])\n",
        "\n",
        "        # Extract ablation study results.\n",
        "        if \"ablation_studies\" in summary:\n",
        "            records[\"ablation\"].extend(summary[\"ablation_studies\"])\n",
        "\n",
        "        # Extract asset pricing results for the primary model.\n",
        "        if \"asset_pricing_performance\" in summary:\n",
        "            records[\"asset_pricing\"].append({\"Method\": method, \"Regime\": regime, **summary['asset_pricing_performance']})\n",
        "\n",
        "        # Extract asset pricing results for baseline models.\n",
        "        if \"baseline_asset_pricing_performance\" in summary:\n",
        "            records[\"asset_pricing\"].extend(summary[\"baseline_asset_pricing_performance\"])\n",
        "\n",
        "        # Extract stability test results.\n",
        "        if \"stability_test_results\" in summary:\n",
        "            for res in summary[\"stability_test_results\"]:\n",
        "                records[\"stability\"].append({\"Regime\": regime, **res})\n",
        "\n",
        "    # Convert lists of records to DataFrames, dropping duplicates to ensure uniqueness.\n",
        "    return {key: pd.DataFrame(val).drop_duplicates().reset_index(drop=True) for key, val in records.items()}\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 37, Helper for Formatting and Saving Tables\n",
        "# ------------------------------------------------------------------------------\n",
        "def _style_and_save_table(\n",
        "    df: pd.DataFrame,\n",
        "    higher_is_better_cols: List[str],\n",
        "    table_name: str,\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Applies conditional color formatting to a DataFrame and saves it as a styled LaTeX table.\n",
        "\n",
        "    This function takes a DataFrame of results, ranks each column, and applies\n",
        "    a green color scale to the top 3 performers. It then exports the styled\n",
        "    table to a .tex file, complete with the necessary LaTeX package headers.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame to be styled. It should be pivoted with methods\n",
        "            as the index and metrics as columns.\n",
        "        higher_is_better_cols: A list of column names for which a higher value\n",
        "                               is better (e.g., 'IC', 'R2_imp'). For all other\n",
        "                               columns, lower is assumed to be better.\n",
        "        table_name: The base name for the output .tex file.\n",
        "        output_dir: The directory in which to save the file.\n",
        "    \"\"\"\n",
        "    # Define the color palette for highlighting (lightest to darkest green).\n",
        "    colors = ['#d9ead3', '#b6d7a8', '#93c47d']\n",
        "\n",
        "    # Create a pandas Styler object from the DataFrame.\n",
        "    styler = df.style\n",
        "\n",
        "    # Iterate through each column of the DataFrame to apply conditional styling.\n",
        "    for col in df.columns:\n",
        "        # Determine the ranking direction for this metric.\n",
        "        is_ascending = col not in higher_is_better_cols\n",
        "\n",
        "        # Rank the column values to identify the top performers. `method='min'` handles ties.\n",
        "        ranks = df[col].rank(method='min', ascending=is_ascending)\n",
        "\n",
        "        # Define a styling function that returns a CSS background-color style.\n",
        "        def color_by_rank(val: float) -> str:\n",
        "            # This inner function will be applied to each cell in the column.\n",
        "            # It uses the pre-computed ranks to determine the color.\n",
        "            rank = ranks[val] if pd.notna(val) else -1\n",
        "            if rank == 1:\n",
        "                return f'background-color: {colors[2]}'  # Darkest green for 1st place\n",
        "            elif rank == 2:\n",
        "                return f'background-color: {colors[1]}'  # Medium green for 2nd place\n",
        "            elif rank == 3:\n",
        "                return f'background-color: {colors[0]}'   # Lightest green for 3rd place\n",
        "            return '' # No style for other ranks\n",
        "\n",
        "        # Apply the styling function to the current column.\n",
        "        styler.apply(lambda s: s.map(lambda v: color_by_rank(v)), subset=[col])\n",
        "\n",
        "    # Apply a consistent floating-point format to all cells.\n",
        "    styler.format(\"{:.4f}\")\n",
        "\n",
        "    # Generate the LaTeX string from the Styler object.\n",
        "    # `convert_css=True` is essential for translating background colors to \\cellcolor.\n",
        "    latex_string = styler.to_latex(\n",
        "        hrules=True,\n",
        "        convert_css=True,\n",
        "        multicol_align='c'\n",
        "    )\n",
        "\n",
        "    # Prepend the necessary LaTeX package headers for color and multi-row/col support.\n",
        "    latex_header = \"\\\\usepackage{colortbl}\\n\\\\usepackage{multirow}\\n\"\n",
        "    # Insert the headers before the `\\begin{tabular}` environment.\n",
        "    tabular_pos = latex_string.find(\"\\\\begin{tabular}\")\n",
        "    final_latex = latex_header + latex_string[:tabular_pos] + latex_string[tabular_pos:]\n",
        "\n",
        "    # Write the final, complete LaTeX code to the output file.\n",
        "    with open(os.path.join(output_dir, f\"{table_name}.tex\"), 'w') as f:\n",
        "        f.write(final_latex)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 37, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def generate_publication_artifacts(\n",
        "    summary_paths: List[str],\n",
        "    base_output_dir: str = \"results\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates all publication-ready tables and figures from experimental results.\n",
        "\n",
        "    This function serves as the final reporting step of the pipeline. It takes the\n",
        "    raw, structured results from all experimental runs (via their summary files)\n",
        "    and transforms them into the final tables and figures presented in the\n",
        "    research paper, including formatted and color-coded LaTeX tables.\n",
        "\n",
        "    Args:\n",
        "        summary_paths: A list of file paths to the `run_summary.json` files\n",
        "                       generated by `execute_full_experimental_suite`.\n",
        "        base_output_dir: The root directory to save the final tables and figures.\n",
        "    \"\"\"\n",
        "    # --- Setup: Create output directories ---\n",
        "    # Create dedicated subdirectories for tables and figures to keep outputs organized.\n",
        "    tables_dir = os.path.join(base_output_dir, \"final_tables\")\n",
        "    figures_dir = os.path.join(base_output_dir, \"final_figures\")\n",
        "    os.makedirs(tables_dir, exist_ok=True)\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    logging.info(\"--- Generating All Publication Artifacts ---\")\n",
        "\n",
        "    # --- Step 1: Parse all raw summary files into structured DataFrames ---\n",
        "    try:\n",
        "        parsed_data = _parse_run_summaries(summary_paths)\n",
        "    except ValueError as e:\n",
        "        logging.error(f\"Failed to parse summary files: {e}. Aborting artifact generation.\")\n",
        "        return\n",
        "\n",
        "    # --- Step 2: Generate Imputation Tables (Panels A, B, C) ---\n",
        "    # Extract the relevant DataFrames from the parsed data.\n",
        "    df_imputation = parsed_data[\"imputation\"]\n",
        "    df_ablation = parsed_data[\"ablation\"]\n",
        "\n",
        "    # Define the canonical order of methods for table rows.\n",
        "    method_order_main = [\"Cross-Sectional Median\", \"Global BF-XS\", \"Local B-XS\", \"ACT-Tensor w/ EMA\", \"ACT-Tensor w/ KF\", \"ACT-Tensor w/ CMA\"]\n",
        "    method_order_ablation = [\"CP Completion\", \"CP Completion w/ Clustering\", \"CP Completion w/ CMA\", \"ACT-Tensor w/ CMA\"]\n",
        "    # Define the order of metrics for table columns.\n",
        "    metric_cols = ['RMSE_imp', 'MAE_imp', 'MAPE_imp', 'R2_imp']\n",
        "    # Specify which metrics are better when higher.\n",
        "    higher_is_better_imp = ['R2_imp']\n",
        "\n",
        "    # Generate each panel by filtering, pivoting, and styling.\n",
        "    for panel, scope, df, order in [(\"A\", \"Overall\", df_imputation, method_order_main),\n",
        "                                    (\"B\", \"Sparse\", df_imputation, method_order_main),\n",
        "                                    (\"C\", \"Overall\", df_ablation, method_order_ablation)]:\n",
        "        # Filter the data for the current panel.\n",
        "        df_panel_data = df[df['Scope'] == scope] if 'Scope' in df.columns else df\n",
        "        if df_panel_data.empty: continue\n",
        "        # Pivot the table to the desired layout (Methods as index, Regime/Metrics as columns).\n",
        "        df_panel = df_panel_data.pivot(index='Method', columns='Regime', values=metric_cols)\n",
        "        # Enforce the canonical row order.\n",
        "        if order:\n",
        "            df_panel = df_panel.reindex(order).dropna(how='all')\n",
        "        # Save the unstyled CSV.\n",
        "        df_panel.round(4).to_csv(os.path.join(tables_dir, f\"table_imputation_panel_{panel}.csv\"))\n",
        "        # Generate and save the styled LaTeX table.\n",
        "        _style_and_save_table(df_panel, higher_is_better_imp, f\"table_imputation_panel_{panel}_styled\", tables_dir)\n",
        "        logging.info(f\"Generated and saved Imputation Results Panel {panel}.\")\n",
        "\n",
        "    # --- Step 3: Generate Asset-Pricing Table ---\n",
        "    df_ap = parsed_data[\"asset_pricing\"]\n",
        "    if not df_ap.empty:\n",
        "        # Pivot the asset pricing data.\n",
        "        df_ap_pivot = df_ap.pivot(index='Method', columns='Regime', values=['RMSE_alpha', 'MAE_alpha', 'MAE_Rank', 'IC', 'TB_Sharpe'])\n",
        "        # Reorder rows to match the main method order.\n",
        "        df_ap_pivot = df_ap_pivot.reindex(method_order_main).dropna(how='all')\n",
        "        # Specify which metrics are better when higher.\n",
        "        higher_is_better_ap = ['IC', 'TB_Sharpe']\n",
        "        # Save the unstyled CSV.\n",
        "        df_ap_pivot.round(4).to_csv(os.path.join(tables_dir, \"table_asset_pricing.csv\"))\n",
        "        # Generate and save the styled LaTeX table.\n",
        "        _style_and_save_table(df_ap_pivot, higher_is_better_ap, \"table_asset_pricing_styled\", tables_dir)\n",
        "        logging.info(\"Generated and saved Asset Pricing Results Table.\")\n",
        "\n",
        "    # --- Step 4: Generate Stability Plots ---\n",
        "    df_stability = parsed_data[\"stability\"]\n",
        "    if not df_stability.empty:\n",
        "        # Generate one plot for each regime tested.\n",
        "        for regime in df_stability['Regime'].unique():\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            ax = sns.lineplot(data=df_stability[df_stability['Regime'] == regime], x='lambda', y='RMSE_imp', marker='o')\n",
        "            ax.set_xscale('log')\n",
        "            ax.set_title(f'RMSE Sensitivity to Regularization (λ) under {regime} Missingness')\n",
        "            ax.set_xlabel('Regularization Strength (λ)')\n",
        "            ax.set_ylabel('Out-of-Sample RMSE')\n",
        "            plot_path = os.path.join(figures_dir, f\"figure_stability_{regime}.pdf\")\n",
        "            plt.savefig(plot_path, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            logging.info(f\"Generated and saved stability plot for regime '{regime}'.\")\n",
        "\n",
        "    logging.info(\"All publication artifacts have been successfully generated.\")\n"
      ],
      "metadata": {
        "id": "BGMLA9D6DD56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Final Fused Task: A Singular Top-Level Pipeline Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Final Fused Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_complete_act_tensor_replication(\n",
        "    data_path: str,\n",
        "    config_path: str,\n",
        "    base_output_dir: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire ACT-Tensor replication study and generates all final artifacts.\n",
        "\n",
        "    This singular, top-level orchestrator serves as the master entry point for the\n",
        "    entire project. It is a complete, production-grade \"run-and-report\" engine that:\n",
        "    1. Manages the full experimental campaign by iterating through all specified\n",
        "       regimes and smoothers.\n",
        "    2. Calls the robust `run_act_tensor_pipeline` for each experimental condition,\n",
        "       which handles the end-to-end workflow from data prep to evaluation.\n",
        "    3. Implements resumability by skipping previously completed runs.\n",
        "    4. After all experiments are complete, automatically aggregates the results\n",
        "       from all runs.\n",
        "    5. Generates all final, publication-ready tables and figures as specified in\n",
        "       the study's design.\n",
        "\n",
        "    Args:\n",
        "        data_path: The file path to the raw input DataFrame (e.g., a Parquet file).\n",
        "        config_path: The file path to the main JSON configuration file.\n",
        "        base_output_dir: The root directory where all experimental results, logs,\n",
        "                         and final artifacts will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the paths to the final aggregated result artifacts,\n",
        "        confirming the successful completion of the entire study.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the essential `data_path` or `config_path`\n",
        "                           files are not found.\n",
        "        Exception: Propagates any fatal error from the underlying pipeline.\n",
        "    \"\"\"\n",
        "    # --- 1. Global Setup: Logging, Directory, and Input Loading ---\n",
        "    # Create the base output directory if it doesn't exist.\n",
        "    os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "    # Set up a master logger for the entire replication project.\n",
        "    master_log_path = os.path.join(base_output_dir, \"master_replication_log.log\")\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(master_log_path, mode='w'),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Log the start of the entire project.\n",
        "    logging.info(f\"{'='*80}\")\n",
        "    logging.info(\"STARTING COMPLETE ACT-TENSOR REPLICATION PIPELINE\")\n",
        "    logging.info(f\"Master log will be saved to: {master_log_path}\")\n",
        "    logging.info(f\"{'='*80}\")\n",
        "\n",
        "    # Load the raw financial data panel, with robust error handling.\n",
        "    try:\n",
        "        df_raw = pd.read_parquet(data_path)\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"FATAL: Could not load raw data file at '{data_path}'. Error: {e}. Exiting.\")\n",
        "        raise\n",
        "\n",
        "    # Load the main configuration file, with robust error handling.\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = json.load(f)\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"FATAL: Could not load config file at '{config_path}'. Error: {e}. Exiting.\")\n",
        "        raise\n",
        "\n",
        "    # Set the global random seeds from the configuration for top-level reproducibility.\n",
        "    np.random.seed(config[\"reproducibility\"][\"seed_global\"])\n",
        "    random.seed(config[\"reproducibility\"][\"seed_global\"])\n",
        "\n",
        "    # --- 2. Experimental Execution Loop (Task 36 Logic) ---\n",
        "    # Define the full factorial experimental design.\n",
        "    regimes_to_run: List[str] = [\"MAR\", \"Block\", \"Logit\"]\n",
        "    smoothers_to_run: List[str] = [\"CMA\", \"EMA\", \"KF\"]\n",
        "    # This list will collect the paths to the summary file from each successful run.\n",
        "    summary_file_paths: List[str] = []\n",
        "\n",
        "    # Loop through every combination of regime and smoother.\n",
        "    for regime in regimes_to_run:\n",
        "        for smoother in smoothers_to_run:\n",
        "            # Define a unique prefix for this experimental condition.\n",
        "            run_name_prefix = f\"{regime}_{smoother}\"\n",
        "            logging.info(f\"\\n{'='*80}\\nPreparing experiment: {run_name_prefix}\\n{'='*80}\")\n",
        "\n",
        "            # --- Resumability Logic ---\n",
        "            # Check if a summary file already exists for a run with this prefix.\n",
        "            existing_summary_path = None\n",
        "            for run_dir in os.listdir(base_output_dir):\n",
        "                if run_dir.startswith(run_name_prefix):\n",
        "                    summary_path = os.path.join(base_output_dir, run_dir, \"run_summary.json\")\n",
        "                    if os.path.exists(summary_path):\n",
        "                        logging.warning(f\"Found existing completed run in '{run_dir}'. Loading path and skipping.\")\n",
        "                        existing_summary_path = summary_path\n",
        "                        break\n",
        "\n",
        "            # If a completed run was found, add its path and continue to the next experiment.\n",
        "            if existing_summary_path:\n",
        "                summary_file_paths.append(existing_summary_path)\n",
        "                continue\n",
        "\n",
        "            # --- Single Run Execution (Task 35 Logic) ---\n",
        "            # Execute the full end-to-end pipeline for one experimental condition.\n",
        "            run_summary = run_act_tensor_pipeline(\n",
        "                df_raw=df_raw, config=config, regime=regime, smoother=smoother,\n",
        "                run_baselines=True, run_ablations=True,\n",
        "                run_stability_test=(regime in [\"Block\", \"Logit\"]),\n",
        "                base_output_dir=base_output_dir\n",
        "            )\n",
        "\n",
        "            # Collect the path to the summary file if the run was successful.\n",
        "            if run_summary.get(\"run_info\", {}).get(\"status\") == \"SUCCESS\":\n",
        "                run_output_dir = run_summary.get(\"artifacts\", {}).get(\"output_directory\")\n",
        "                if run_output_dir:\n",
        "                    summary_file_paths.append(os.path.join(run_output_dir, \"run_summary.json\"))\n",
        "\n",
        "    # --- 3. Final Aggregation and Reporting (Task 37 Logic) ---\n",
        "    # Call the dedicated aggregation function to generate all final tables and figures.\n",
        "    final_artifact_paths = generate_publication_artifacts(\n",
        "        summary_paths=summary_file_paths,\n",
        "        base_output_dir=base_output_dir\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the entire suite of experiments.\n",
        "    logging.info(f\"\\n{'='*80}\")\n",
        "    logging.info(\"MASTER SCRIPT: EXECUTION AND AGGREGATION FINISHED SUCCESSFULLY.\")\n",
        "    logging.info(f\"Final artifacts are located in: {final_artifact_paths}\")\n",
        "    logging.info(f\"{'='*80}\")\n",
        "\n",
        "    # Return the dictionary of paths to the final generated artifacts.\n",
        "    return final_artifact_paths\n"
      ],
      "metadata": {
        "id": "cZbqj-I8cBYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}